/*
 * Copyright (c) 1992-1999 by Sun Microsystems, Inc.
 * All rights reserved.
 */

#pragma ident   "@(#)ip.c	1.216	99/12/02 SMI"


#include <sys/types.h>
#include <sys/stream.h>
#include <sys/dlpi.h>
#include <sys/stropts.h>
#include <sys/sysmacros.h>
#include <sys/strsubr.h>
#include <sys/strlog.h>
#define	_SUN_TPI_VERSION 2
#include <sys/tihdr.h>
#include <sys/xti_inet.h>
#include <sys/ddi.h>
#include <sys/cmn_err.h>
#include <sys/debug.h>
#include <sys/modctl.h>
#include <sys/atomic.h>

#include <sys/systm.h>
#include <sys/param.h>
#include <sys/kmem.h>
#include <sys/callb.h>
#include <sys/socket.h>
#include <sys/vtrace.h>
#include <sys/isa_defs.h>
#include <sys/kmem.h>
#include <net/if.h>
#include <net/if_arp.h>
#include <net/route.h>
#include <sys/sockio.h>
#include <netinet/in.h>
#include <net/if_dl.h>

#include <inet/common.h>
#include <inet/mi.h>
#include <inet/mib2.h>
#include <inet/nd.h>
#include <inet/arp.h>
#include <inet/snmpcom.h>
#include <sys/strick.h>

#include <netinet/igmp_var.h>
#include <netinet/ip6.h>
#include <netinet/icmp6.h>

#include <inet/ip.h>
#include <inet/ip6.h>
#include <inet/tcp.h>
#include <inet/ip_multi.h>
#include <inet/ip_if.h>
#include <inet/ip_ire.h>
#include <inet/ip_rts.h>
#include <inet/optcom.h>
#include <inet/ip_ndp.h>
#include <netinet/igmp.h>
#include <netinet/ip_mroute.h>

#include <netinet/igmp.h>
#include <netinet/ip_mroute.h>

#include <net/pfkeyv2.h>
#include <inet/ipsec_info.h>
#include <inet/sadb.h>
#include <sys/kmem.h>
#include <inet/ipsec_conf.h>
#include <inet/tun.h>

/*
 * Cluster specific hooks. These should be NULL when booted as a non-cluster
 */

/*
 * Hook functions to enable cluster networking
 * On non-clustered systems these vectors must always be NULL.
 *
 * Hook function to Check ip specified ip address is a shared ip address
 * in the cluster
 *
 */
int (*cl_inet_isclusterwide)(uint8_t protocol,
    sa_family_t addr_family, uint8_t *laddrp) = NULL;

/*
 * Hook function to generate cluster wide ip fragment identifier
 */
uint32_t (*cl_inet_ipident)(uint8_t protocol, sa_family_t addr_family,
    uint8_t *laddrp, uint8_t *faddrp) = NULL;

/*
 * Synchronization notes:
 *
 * This modules uses a combination of a per-module STREAMS perimeter and
 * explicit locking. The perimeter is configured so that open, close, put
 * and callbacks (qtimeouts) procedures are hot i.e. can execute
 * concurrently. Thus only the service procedures and those places that
 * explicitly use qwriter (become_exclusive) will acquire exclusive access
 * to the IP perimeter.
 *
 * When the open and close procedures need exclusive access to IP (in the
 * non-performance critical cases) they qenable the write service procedure
 * and qwait until it is done.
 * When the put procedures need exclusive access to all of IP
 * we asynchronously pass a message to a procedure by invoking qwriter
 * ("become_exclusive") which will arrange to call the
 * routine only after all reader threads have exited the shared resource, and
 * the writer lock has been acquired.
 *
 * There are explicit locks in IP to handle:
 *  - the ip_g_head list maintained by mi_open_link() and friends
 *  - the reassembly data structures (one lock per hash bucket)
 *  - the hashes used to find the IP client streams (one lock per hash bucket)
 *  - the reference count in each IPC structure
 *  - the atomic increment of ire_ident
 *  - certain IGMP and multicast routing data structures
 *  - ire_lock to protect some of the fields of the ire, IRE tables
 *    (one lock per hash bucket). Refer to ip_ire.c for details.
 *  - ndp_g_lock and nce_lock for protecting NCEs.
 *
 * IPSEC notes :
 *
 * IP interacts with the IPSEC code (AH/ESP) by tagging a M_CTL message
 * in front of the actual packet. For outbound datagrams, the M_CTL
 * contains a ipsec_out_t (defined in ipsec_info.h), which has the
 * information used by the IPSEC code for applying the right level of
 * protection. The information initialized by IP in the ipsec_out_t
 * is determined by the per-socket policy or global policy in the system.
 * For inbound datagrams, the M_CTL contains a ipsec_in_t (defined in
 * ipsec_info.h) which starts out with nothing in it. It gets filled
 * with the right information if it goes through the AH/ESP code, which
 * happens if the incoming packet is secure. The information initialized
 * by AH/ESP, is later used by IP(during fanouts to ULP) to see whether
 * the policy requirements needed by per-socket polcy or global policy
 * is met or not.
 *
 * If there is both per-socket policy (set using setsockopt) and there
 * is also global policy match for the 5 tuples of the socket,
 * ipsec_override_policy() makes the decision of which one to use.
 *
 * For fully connected sockets i.e dst, src [addr, port] is known,
 * ipc_policy_cached is set indicating that policy has been cached.
 * ipc_in_enforce_policy may or may not be set depending on whether
 * there is a global policy match or per-socket policy match.
 * Policy inheriting happpens in ip_bind during the ipa_conn_t bind.
 * Once the right policy is set on the ipc_t, policy cannot change for
 * this socket. This makes life simpler for TCP (UDP ?) where
 * re-transmissions go out with the same policy. For symmetry, policy
 * is cached for fully connected UDP sockets also. Thus if policy is cached,
 * it also implies that policy is latched i.e policy cannot change
 * on these sockets. As we have the right policy on the ipc, we don't
 * have to lookup global policy for every outbound and inbound datagram
 * and thus serving as an optimization. Note that a global policy change
 * does not affect fully connected sockets if they have policy. If fully
 * connected sockets did not have any policy associated with it, global
 * policy change may affect them.
 */


/*
 * Check with IPSEC inbound policy if
 *
 * 1) per-socket policy is present - indicated by ipc_in_enforce_policy.
 * 2) Or if we have not cached policy on the ipc and the global policy is
 *    non-empty.
 */
#define	IPC_INBOUND_POLICY_PRESENT(ipc)			\
	((ipc)->ipc_in_enforce_policy ||		\
	(!(ipc)->ipc_policy_cached &&			\
	    (ipsec_policy_head[IPSEC_TYPE_INBOUND].ipsec_policy_next != NULL)))

static char *ipsec_policy_failure_msgs[] = {

	"%s: Dropping the datagram because the incoming packet "
	"is %s, but the recipient expects clear; Source %s, "
	"Destination %s.\n",

	"%s: Policy Failure for the incoming packet (%s); Source %s, "
	"Destination %s.\n",

	"%s: Authentication present while not expected in the "
	"incoming %s packet; Source %s, Destination %s.\n",

	"%s: Encryption present while not expected in the "
	"incoming %s packet; Source %s, Destination %s.\n",

	"%s: Self-Encapsulation present while not expected in the "
	"incoming %s packet; Source %s, Destination %s.\n",
};

/*
 * Have a counter for every possible policy message in the previous array.
 */
static uint32_t ipsec_policy_failure_count[IPSEC_POLICY_MAX];
/* Time since last ipsec policy failure that printed a message. */
hrtime_t ipsec_policy_failure_last = 0;

#define	IS_SIMPLE_IPH(ipha)						\
	((ipha)->ipha_version_and_hdr_length == IP_SIMPLE_HDR_VERSION)

/* RFC1122 Conformance */
#define	IP_FORWARD_DEFAULT	IP_FORWARD_NEVER

/*
 * We don't use the iph in the following definition, but we might want
 * to some day.
 */
#define	IP_OK_TO_FORWARD_THRU(ipha, ire)				\
	(WE_ARE_FORWARDING)

#ifdef	_BIG_ENDIAN
#define	IP_HDR_CSUM_TTL_ADJUST	256
#define	IP_TCP_CSUM_COMP	IPPROTO_TCP
#define	IP_UDP_CSUM_COMP	IPPROTO_UDP
#else
#define	IP_HDR_CSUM_TTL_ADJUST	1
#define	IP_TCP_CSUM_COMP	(IPPROTO_TCP << 8)
#define	IP_UDP_CSUM_COMP	(IPPROTO_UDP << 8)
#endif

#define	TCP_CHECKSUM_OFFSET		16

#define	ILL_MAX_NAMELEN			LIFNAMSIZ

/* Leave room for ip_newroute to tack on the src and target addresses */
#define	OK_RESOLVER_MP(mp)						\
	((mp) && ((mp)->b_wptr - (mp)->b_rptr) >= (2 * IP_ADDR_LEN))

uint32_t	ipsec_next_policy_index_to_try = 1;
/*
 * When the ipsec_policy_index_wrapped is B_TRUE, we need to check
 * for duplicates. Till then ipsec_next_policy_index_to_try will
 * have the right next value to be used.
 */
boolean_t	ipsec_policy_index_wrapped = B_FALSE;
ipsec_policy_t ipsec_policy_head[IPSEC_NTYPES];

/*
 * The following variables are kept because IPsec needs to be loaded only when
 * it its used.
 */
static boolean_t ipsec_failure = B_FALSE;
static kthread_t *ipsec_loader_thread;
kmutex_t ipsec_loader_lock;
static queue_t *ipsec_loader_q;
static kcondvar_t ipsec_loader_cv;	/* For loader_q conditions. */
int ipsec_loader_sig;
kcondvar_t ipsec_loader_sig_cv;		/* For loader_sig conditions. */
static kcondvar_t ipsec_loader_exit_cv;	/* For ip_ddi_destroy conditions. */

#define	IPSEC_LOADER_EXITNOW	-1
#define	IPSEC_LOADER_UNLOADED	0
#define	IPSEC_LOADER_LOADNOW	1
#define	IPSEC_LOADER_EXITED	2

/*
 * Assume '1' and '2' are not valid pointers.
 */
#define	IPSEC_LOADER_LOAD_FAILED	(kthread_t *)1
#define	IPSEC_LOADER_LOAD_OK		(kthread_t *)2


/*
 * Following are the estimates of what the maximum AH and ESP header size
 * would be. This is used to tell the upper layer the right value of MSS
 * it should use without consulting AH/ESP. If the size is something
 * different from this, ULP will learn the right one through
 * ICMP_FRAGMENTATION_NEEDED messages generated locally.
 *
 * AH : 12 bytes of constant header + 12 bytes of ICV checksum (MD5/SHA1).
 *
 * ESP : 8 bytes of constant header + 16 bytes (max) of block size +
 * 12 bytes of ICV checksum.
 */
#define		IPSEC_MAX_AH_HDR_SIZE	24
#define		IPSEC_MAX_ESP_HDR_SIZE	36

/*
 * Folowing macro is used whenever the code does not know whether there
 * is a M_CTL present in the front and it needs to examine the actual mp
 * i.e the IP header. As a M_CTL message could be in the front, this
 * extracts the packet into mp and the M_CTL mp into first_mp. If M_CTL
 * mp is not present, both first_mp and mp point to the same message.
 */
#define	EXTRACT_PKT_MP(mp, first_mp, mctl_present)	\
	(first_mp) = (mp);				\
	if ((mp)->b_datap->db_type == M_CTL) {		\
		(mp) = (mp)->b_cont;			\
		(mctl_present) = B_TRUE;		\
	} else {					\
		(mctl_present) = B_FALSE;		\
	}						\

static mblk_t	*ip_wput_attach_llhdr(mblk_t *, ire_t *);
static boolean_t ipsec_check_policy(mblk_t *, ipc_t *, boolean_t,
    boolean_t);
static boolean_t ipsec_match_policy(ipsec_policy_t *, ipsec_selector_t *,
    ipsec_conf_t *);
static int	ipsec_extra_length(mblk_t *);
static void	ipsec_out_process(queue_t *, mblk_t *);
static void	ipsec_loader(void *);
static void	ip_fanout_proto_again(queue_t *, mblk_t *);
static void	ip_fanout_sec_proto(queue_t *q, mblk_t *, int, uint_t);
static boolean_t ipsec_inherit_global_policy(ipc_t *, ipsec_req_t *,
    ipsec_selector_t *);
static void	ipsec_out_to_in(mblk_t *);
static void	ipsec_copy_policy(ipsec_conf_t *, ipsec_req_t *);
static mblk_t	*ipsec_attach_ipsec_out(mblk_t *, ipc_t *, ipsec_req_t *,
    uint8_t);
static mblk_t	*ipsec_init_ipsec_out(mblk_t *, ipc_t *, ipsec_req_t *,
    uint8_t);
static void	icmp_frag_needed(queue_t *, mblk_t *, int);
static void	icmp_inbound(queue_t *, mblk_t *, int, ill_t *, int, uint32_t,
		    boolean_t);
static boolean_t icmp_inbound_too_big(icmph_t *, ipha_t *);
static void	icmp_inbound_error_fanout(queue_t *, ill_t *, mblk_t *,
		    icmph_t *, ipha_t *, int, int, boolean_t);
static void	icmp_options_update(ipha_t *);
static void	icmp_param_problem(queue_t *, mblk_t *, uint8_t);
static void	icmp_pkt(queue_t *, mblk_t *, void *, size_t, boolean_t);
static mblk_t	*icmp_pkt_err_ok(mblk_t *);
static void	icmp_redirect(mblk_t *);
static void	icmp_send_redirect(queue_t *, mblk_t *, ipaddr_t);

static void	igmp_timeout(void);
static void	mld_timeout(void);

static void	ip_arp_news(queue_t *, mblk_t *);
static int	ip_bind_fanout_insert(ipc_t *, int, int);
static boolean_t ip_bind_insert_ire(mblk_t *, ire_t *, iulp_t *);
static void	ip_bind_v4(queue_t *, mblk_t *);
extern uint_t	bcksum(uchar_t *, int, uint32_t);
mblk_t		*ip_dlpi_alloc(size_t, t_uscalar_t);
char		*ip_dot_addr(ipaddr_t, char *);
mblk_t		*ip_carve_mp(mblk_t **, ssize_t);
int		ip_close(queue_t *);
static char	*ip_dot_saddr(uchar_t *, char *);
void		ip_fanout_destroy(void);
void		ip_fanout_init(void);
static void	ip_fanout_proto(queue_t *, mblk_t *, ill_t *, ipha_t *, uint_t,
		    boolean_t);
static void	ip_fanout_tcp(queue_t *, mblk_t *, ipha_t *, uint32_t, uint_t,
		    boolean_t);
static void	ip_fanout_udp(queue_t *, mblk_t *, ill_t *, ipha_t *, uint32_t,
		    ushort_t, uint_t, boolean_t);
static void	ip_lrput(queue_t *, mblk_t *);
static void	ip_lwput(queue_t *, mblk_t *);
ipaddr_t	ip_massage_options(ipha_t *);
ipaddr_t	ip_net_mask(ipaddr_t);
static void	ip_newroute(queue_t *, mblk_t *, ipaddr_t, ipc_t *);
static void	ip_newroute_ipif(queue_t *, mblk_t *, ipif_t *, ipaddr_t,
		    ipc_t *);

char	*ip_nv_lookup(nv_t *, int);

static int	ip_optmgmt_writer(mblk_t *);

static void	ip_optcom_req(queue_t *, mblk_t *);
int	ip_opt_default(queue_t *, int, int, uchar_t *);
int	ip_opt_get(queue_t *, int, int, uchar_t *);
int	ip_opt_set(queue_t *, uint_t, int, int, uint_t, uchar_t *, uint_t *,
	    uchar_t *);
static int	ip_param_get(queue_t *, mblk_t *, void *);
static boolean_t	ip_param_register(ipparam_t *, size_t);
static int	ip_param_set(queue_t *, mblk_t *, char *, void *);
void	ip_rput(queue_t *, mblk_t *);
static void	ip_rput_dlpi_writer(queue_t *, mblk_t *);
void	ip_rput_forward(ire_t *, ipha_t *, mblk_t *);
static int	ip_rput_forward_options(mblk_t *, ipha_t *, ire_t *);
void	ip_rput_local(queue_t *, mblk_t *, ipha_t *, ire_t *, uint_t);
static int	ip_rput_local_options(queue_t *, mblk_t *, ipha_t *, ire_t *);
static int	ip_rput_options(queue_t *, mblk_t *, ipha_t *, ipaddr_t *);
static int	ip_snmp_get(queue_t *, mblk_t *);
static mblk_t	*ip_snmp_get_mib2_ip(queue_t *, mblk_t *);
static mblk_t	*ip_snmp_get_mib2_ip6(queue_t *, mblk_t *);
static mblk_t	*ip_snmp_get_mib2_icmp(queue_t *, mblk_t *);
static mblk_t	*ip_snmp_get_mib2_icmp6(queue_t *, mblk_t *);
static mblk_t	*ip_snmp_get_mib2_igmp(queue_t *, mblk_t *);
static mblk_t	*ip_snmp_get_mib2_multi(queue_t *, mblk_t *);
static mblk_t	*ip_snmp_get_mib2_ip_addr(queue_t *, mblk_t *);
static mblk_t	*ip_snmp_get_mib2_ip6_addr(queue_t *, mblk_t *);
static mblk_t	*ip_snmp_get_mib2_ip_group_mem(queue_t *, mblk_t *);
static mblk_t	*ip_snmp_get_mib2_ip6_group_mem(queue_t *, mblk_t *);
static mblk_t	*ip_snmp_get_mib2_virt_multi(queue_t *, mblk_t *);
static mblk_t	*ip_snmp_get_mib2_multi_rtable(queue_t *, mblk_t *);
static mblk_t	*ip_snmp_get_mib2_ip_route_media(queue_t *, mblk_t *);
static mblk_t	*ip_snmp_get_mib2_ip6_route_media(queue_t *, mblk_t *);
static void	ip_snmp_get2_v4(ire_t *, mblk_t *[]);
static void	ip_snmp_get2_v6(ire_t *, mblk_t *[]);
static int	ip_snmp_set(queue_t *, int, int, uchar_t *, int);
static boolean_t	ip_source_routed(ipha_t *);
static boolean_t	ip_source_route_included(ipha_t *);

static void	ip_unbind(queue_t *, mblk_t *);
static void	ip_wput_frag(ire_t *, mblk_t *, uint_t *, uint32_t, uint32_t);
static mblk_t	*ip_wput_frag_copyhdr(uchar_t *, int, int);
static void	ip_wput_local_options(ipha_t *);
static int	ip_wput_options(queue_t *, mblk_t *, ipha_t *, boolean_t);

static boolean_t	ismod_next(queue_t *, char *);

static void	ipc_hash_remove(ipc_t *);

static void	ipc_qenable(ipc_t *, void *);
static void	ipc_walk_nontcp(pfv_t, void *);
static boolean_t	ipc_wantpacket(ipc_t *, ill_t *, ipaddr_t);
static void	ip_arp_done(queue_t *, mblk_t *);

static ipc_t	*ip_proxy_match_listener(ipaddr_t, uint16_t, boolean_t);
static int	ip_proxy_add_listener(ipc_t *);
static void	ip_proxy_remove_listener(ipc_t *);
static int	ip_proxy_addr_compare(proxy_addr_t *, proxy_addr_t *);
static int	ip_proxy_addr_report(queue_t *, mblk_t *, void *);
static boolean_t ip_source_route_more_hops(ipha_t *, uint32_t);

struct ill_s	*ill_g_head;	/* ILL List Head */
	ill_t	*ill_ire_gc;	/* ILL used for ire memory reclaim */

ill_t	*ip_timer_ill;		/* ILL for IRE expiration timer. */
timeout_id_t ip_ire_expire_id;	/* IRE expiration timer. */
timeout_id_t ip_ire_reclaim_id; /* IRE reclaim id. */
static clock_t ip_ire_arp_time_elapsed; /* Time since IRE cache last flushed */
static clock_t ip_ire_rd_time_elapsed;	/* ... redirect IREs last flushed */
static clock_t ip_ire_pmtu_time_elapsed; /* Time since path mtu increase */

ill_t	*igmp_timer_ill;	/* ILL for IGMP timer. */
mblk_t	*igmp_timer_mp;		/* IGMP timer */
int	igmp_timer_interval = IGMP_TIMEOUT_INTERVAL;
ill_t	*mld_timer_ill;		/* ILL for MLD timer. */
mblk_t	*mld_timer_mp;	/* MLD timer */

uint_t	ip_ire_default_count;	/* Number of IPv4 IRE_DEFAULT entries. */
uint_t	ip_ire_default_index;	/* Walking index used to mod in */

ipaddr_t	ip_g_all_ones = IP_HOST_MASK;
clock_t icmp_pkt_err_last = 0;	/* Time since last icmp_pkt_err */
uint_t	icmp_pkt_err_sent = 0;	/* Number of packets sent in burst */

/* How long, in seconds, we allow frags to hang around. */
#define	IP_FRAG_TIMEOUT	60

time_t	ip_g_frag_timeout = IP_FRAG_TIMEOUT;
clock_t	ip_g_frag_timo_ms = IP_FRAG_TIMEOUT * 1000;

/* Protected by ip_mi_lock */
static void	*ip_g_head;		/* Instance Data List Head */
kmutex_t	ip_mi_lock;		/* Lock for list of instances */
krwlock_t	ipsec_conf_lock;	/* Lock for IPSEC conf/policy stuff */

/* Only modified during _init and _fini thus no locking is needed. */
caddr_t		ip_g_nd;		/* Named Dispatch List Head */


static long ip_rput_pullups;
int	ip_max_mtu;			/* Used by udp/icmp */
int	dohwcksum = 1;	/* use h/w cksum if supported by the hardware */

/*
 * MIB-2 stuff for SNMP (both IP and ICMP)
 */
mib2_ip_t	ip_mib;
mib2_icmp_t	icmp_mib;

uint_t loopback_packets = 0;	/* loopback interface statistics */

uint_t ip_proxy_ib_pkt_count = 0; /* # of pkts intercepted at forwarding path */
uint_t ip_proxy_ib_frag_count = 0; /* frags intercepted at forwarding path */
static ipc_t *ip_proxy_listeners = NULL; /* list of all proxy listeners */
static krwlock_t ip_palist_lock;	/* protect ip_proxy_listeners list */
ill_t	*proxy_frag_ill = NULL;	/* ILL for reassembling frags of proxy addrs */

/*
 * XXX following really should only be in a header. Would need more
 * header and .c clean up first.
 */
extern optdb_obj_t	ip_opt_obj;

/*
 * Named Dispatch Parameter Table.
 * All of these are alterable, within the min/max values given, at run time.
 */
static ipparam_t	lcl_param_arr[] = {
	/* min	max	value	name */
	{  0,	1,	0,	"ip_respond_to_address_mask_broadcast"},
	{  0,	1,	1,	"ip_respond_to_echo_broadcast"},
	{  0,	1,	1,	"ip_respond_to_timestamp"},
	{  0,	1,	1,	"ip_respond_to_timestamp_broadcast"},
	{  0,	1,	1,	"ip_send_redirects"},
	{  0,	1,	1,	"ip_forward_directed_broadcasts"},
	{  0,	10,	0,	"ip_debug"},
	{  0,	10,	0,	"ip_mrtdebug"},
	{  5000, 999999999,	60000, "ip_ire_timer_interval" },
	{  60000, 999999999,	1200000, "ip_ire_arp_interval" },
	{  60000, 999999999,	60000, "ip_ire_redirect_interval" },
	{  1,	255,	255,	"ip_def_ttl" },
	{  0,	1,	1,	"ip_forward_src_routed"},
	{  0,	256,	32,	"ip_wroff_extra" },
	{  5000, 999999999, 600000, "ip_ire_pathmtu_interval" },
	{  8,	65536,  64,	"ip_icmp_return_data_bytes" },
	{  0,	1,	1,	"ip_path_mtu_discovery" },
	{  0,	240,	30,	"ip_ignore_delete_time" },
	{  0,	1,	0,	"ip_ignore_redirect" },
	{  0,	1,	1,	"ip_output_queue" },
	{  1,	254,	1,	"ip_broadcast_ttl" },
	{  0,	99999,	100,	"ip_icmp_err_interval" },
	{  1,	99999,	10,	"ip_icmp_err_burst" },
	{  0,	999999999,	1000000, "ip_reass_queue_bytes" },
	{  0,	1,	0,	"ip_strict_dst_multihoming" },
	{  1,	8192,	256,	"ip_addrs_per_if"},
	{  0,	1,	0,	"ipsec_override_persocket_policy" },
	{  0,	1,	1,	"icmp_accept_clear_messages" },
	{  0,	1,	1,	"igmp_accept_clear_messages" },
	{  2,	999999999, ND_DELAY_FIRST_PROBE_TIME,
				"ip_ndp_delay_first_probe_time"},
	{  1,	999999999, ND_MAX_UNICAST_SOLICIT,
				"ip_ndp_max_unicast_solicit"},
	{  1,	255,	IPV6_MAX_HOPS,	"ip6_def_hops" },
	{  8,	IPV6_MIN_MTU,	IPV6_MIN_MTU, "ip6_icmp_return_data_bytes" },
	{  0,	1,	0,	"ip6_forwarding"},
	{  0,	1,	1,	"ip6_forward_src_routed"},
	{  0,	1,	1,	"ip6_respond_to_echo_multicast"},
	{  0,	1,	1,	"ip6_send_redirects"},
	{  0,	1,	0,	"ip6_ignore_redirect" },
	{  0,	1,	0,	"ip6_strict_dst_multihoming" },

	{  1,	8,	3,	"ip_ire_reclaim_fraction" },

	{  0,	999999,	1000,	"ipsec_policy_log_interval" },

	{  0,	1,	1,	"pim_accept_clear_messages" },

#ifdef DEBUG
	{  0,	1,	0,	"ip6_drop_inbound_icmpv6" },
#endif
};
	ipparam_t	*ip_param_arr = lcl_param_arr;

/*
 * ip_g_forward controls IP forwarding.  It takes two values:
 *	0: IP_FORWARD_NEVER	Don't forward packets ever.
 *	1: IP_FORWARD_ALWAYS	Forward packets for elsewhere.
 *
 * RFC1122 says there must be a configuration switch to control forwarding,
 * but that the default MUST be to not forward packets ever.  Implicit
 * control based on configuration of multiple interfaces MUST NOT be
 * implemented (Section 3.1).  SunOS 4.1 did provide the "automatic" capability
 * and, in fact, it was the default.  That capability is now provided in the
 * /etc/rc2.d/S69inet script.
 */
int ip_g_forward = IP_FORWARD_DEFAULT;

/* Following line is external, and in ip.h.  Normally marked with * *. */
#define	ip_respond_to_address_mask_broadcast ip_param_arr[0].ip_param_value
#define	ip_g_resp_to_echo_bcast		ip_param_arr[1].ip_param_value
#define	ip_g_resp_to_timestamp		ip_param_arr[2].ip_param_value
#define	ip_g_resp_to_timestamp_bcast	ip_param_arr[3].ip_param_value
#define	ip_g_send_redirects		ip_param_arr[4].ip_param_value
#define	ip_g_forward_directed_bcast	ip_param_arr[5].ip_param_value
#define	ip_debug			ip_param_arr[6].ip_param_value	/* */
#define	ip_mrtdebug			ip_param_arr[7].ip_param_value	/* */
#define	ip_timer_interval		ip_param_arr[8].ip_param_value	/* */
#define	ip_ire_arp_interval		ip_param_arr[9].ip_param_value /* */
#define	ip_ire_redir_interval		ip_param_arr[10].ip_param_value
#define	ip_def_ttl			ip_param_arr[11].ip_param_value
#define	ip_forward_src_routed		ip_param_arr[12].ip_param_value
#define	ip_wroff_extra			ip_param_arr[13].ip_param_value
#define	ip_ire_pathmtu_interval		ip_param_arr[14].ip_param_value
#define	ip_icmp_return			ip_param_arr[15].ip_param_value
#define	ip_path_mtu_discovery		ip_param_arr[16].ip_param_value /* */
#define	ip_ignore_delete_time		ip_param_arr[17].ip_param_value /* */
#define	ip_ignore_redirect		ip_param_arr[18].ip_param_value
#define	ip_output_queue			ip_param_arr[19].ip_param_value
#define	ip_broadcast_ttl		ip_param_arr[20].ip_param_value
#define	ip_icmp_err_interval		ip_param_arr[21].ip_param_value
#define	ip_icmp_err_burst		ip_param_arr[22].ip_param_value
#define	ip_reass_queue_bytes		ip_param_arr[23].ip_param_value
#define	ip_strict_dst_multihoming	ip_param_arr[24].ip_param_value
#define	ip_addrs_per_if			ip_param_arr[25].ip_param_value
#define	ipsec_override_persocket_policy	ip_param_arr[26].ip_param_value
#define	icmp_accept_clear_messages	ip_param_arr[27].ip_param_value
#define	igmp_accept_clear_messages	ip_param_arr[28].ip_param_value

/* IPv6 configuration knobs */
#define	delay_first_probe_time		ip_param_arr[29].ip_param_value
#define	max_unicast_solicit		ip_param_arr[30].ip_param_value
#define	ipv6_def_hops			ip_param_arr[31].ip_param_value
#define	ipv6_icmp_return		ip_param_arr[32].ip_param_value
#define	ipv6_forward			ip_param_arr[33].ip_param_value
#define	ipv6_forward_src_routed		ip_param_arr[34].ip_param_value
#define	ipv6_resp_echo_mcast		ip_param_arr[35].ip_param_value
#define	ipv6_send_redirects		ip_param_arr[36].ip_param_value
#define	ipv6_ignore_redirect		ip_param_arr[37].ip_param_value
#define	ipv6_strict_dst_multihoming	ip_param_arr[38].ip_param_value
#define	ip_ire_reclaim_fraction		ip_param_arr[39].ip_param_value
#define	ipsec_policy_log_interval	ip_param_arr[40].ip_param_value
#define	pim_accept_clear_messages	ip_param_arr[41].ip_param_value
#ifdef DEBUG
#define	ipv6_drop_inbound_icmpv6	ip_param_arr[42].ip_param_value
#else
#define	ipv6_drop_inbound_icmpv6	0
#endif

/*
 * ip_enable_group_ifs controls if logical interfaces should be
 * grouped together if they are on the same subnet.  By grouping
 * logical interfaces on the same subnet, the IP module can make an
 * attempt to balance traffic that leaves via a subnet across multiple
 * interfaces that are attached to it.  This allows a machine to
 * exploit switched Ethernet technology to allow 10n Mbit/sec, where n
 * is the number of physical taps on the Ethernet switch.
 *
 * If this variable is changed with NDD, the appropriate grouping/ungrouping
 * of interfaces will occur.
 */
int ip_enable_group_ifs = 0;

/* TCP fanout hash list and size. */
static	icf_t *ipc_tcp_conn_fanout;
uint_t	ipc_tcp_conn_hash_size = IP_TCP_CONN_HASH_SIZE;

static	icf_t ipc_tcp_listen_fanout[256];
static	icf_t ipc_udp_fanout[256];	/* UDP fanout hash list. */
static	icf_t ipc_proto_fanout[IPPROTO_MAX+1];	/* Misc. fanout hash list. */
	icf_t rts_clients;		/* Routing sockets */

uint_t	ipif_g_count;			/* Count of IPIFs "up". */

static nv_t	ire_nv_arr[] = {
	{ IRE_BROADCAST, "BROADCAST" },
	{ IRE_LOCAL, "LOCAL" },
	{ IRE_LOOPBACK, "LOOPBACK" },
	{ IRE_CACHE, "CACHE" },
	{ IRE_DEFAULT, "DEFAULT" },
	{ IRE_PREFIX, "PREFIX" },
	{ IRE_IF_NORESOLVER, "IF_NORESOL" },
	{ IRE_IF_RESOLVER, "IF_RESOLV" },
	{ IRE_HOST, "HOST" },
	{ IRE_HOST_REDIRECT, "HOST_REDIRECT" },
	{ 0 }
};

nv_t	*ire_nv_tbl = ire_nv_arr;

/* Simple ICMP IP Header Template */
static ipha_t icmp_ipha = {
	IP_SIMPLE_HDR_VERSION, 0, 0, 0, 0, 0, IPPROTO_ICMP
};

struct module_info ip_mod_info = {
	5701, "ip", 1, INFPSZ, 65536, 1024
};

/* XXX will need a ipv6_mod_info when ip no longer is D_MTPERMOD */

static struct qinit rinit = {
	(pfi_t)ip_rput, (pfi_t)ip_rsrv, ip_open, ip_close, NULL,
	&ip_mod_info
};

static struct qinit winit = {
	(pfi_t)ip_wput, (pfi_t)ip_wsrv, ip_open, ip_close, NULL,
	&ip_mod_info
};

static struct qinit lrinit = {
	(pfi_t)ip_lrput, NULL, ip_open, ip_close, NULL,
	&ip_mod_info
};

static struct qinit lwinit = {
	(pfi_t)ip_lwput, NULL, ip_open, ip_close, NULL,
	&ip_mod_info
};

struct streamtab ipinfo = {
	&rinit, &winit, &lrinit, &lwinit
};

/* Generate an ICMP fragmentation needed message. */
static void
icmp_frag_needed(queue_t *q, mblk_t *mp, int mtu)
{
	icmph_t	icmph;
	mblk_t *first_mp;
	boolean_t mctl_present;

	EXTRACT_PKT_MP(mp, first_mp, mctl_present);

	if (!(mp = icmp_pkt_err_ok(mp))) {
		if (mctl_present)
			freeb(first_mp);
		return;
	}

	bzero(&icmph, sizeof (icmph_t));
	icmph.icmph_type = ICMP_DEST_UNREACHABLE;
	icmph.icmph_code = ICMP_FRAGMENTATION_NEEDED;
	icmph.icmph_du_mtu = htons((uint16_t)mtu);
	BUMP_MIB(icmp_mib.icmpOutFragNeeded);
	BUMP_MIB(icmp_mib.icmpOutDestUnreachs);
	icmp_pkt(q, first_mp, &icmph, sizeof (icmph_t), mctl_present);
}

/*
 * icmp_inbound deals with ICMP messages in the following ways.
 *
 * 1) It needs to send a reply back and possibly delivering it
 *    to the "interested" upper clients.
 * 2) It needs to send it to the upper clients only.
 * 3) It needs to change some values in IP only.
 * 4) It needs to change some values in IP and upper layers e.g TCP.
 *
 * We need to accomodate icmp messages coming in clear until we get
 * everything secure from the wire. If icmp_accept_clear_messages
 * is zero we check with the global policy and act accordingly. If
 * it is non-zero, we accept the message without any checks. But
 * *this does not mean* that this will be delivered to the upper
 * clients. By accepting we might send replies back, change our MTU
 * value etc. but delivery to the ULP/clients depends on their policy
 * dispositions.
 *
 * We handle the above 4 cases in the context of IPSEC in the
 * following way :
 *
 * 1) Send the reply back in the same way as the request came in.
 *    If it came in encrypted, it goes out encrypted. If it came in
 *    clear, it goes out in clear. Thus, this will prevent chosen
 *    plain text attack.
 * 2) The client may or may not expect things to come in secure.
 *    If it comes in secure, the policy constraints are checked
 *    before delivering it to the upper layers. If it comes in
 *    clear, ipsec_inbound_accept_clear will decide whether to
 *    accept this in clear or not. In both the cases, if the returned
 *    message (IP header + 8 bytes) that caused the icmp message has
 *    AH/ESP headers, it is sent up to AH/ESP for validation before
 *    sending up. If there are only 8 bytes of returned message, then
 *    upper client will not be notified.
 * 3) Check with global policy to see whether it matches the constaints.
 *    But this will be done only if icmp_accept_messages_in_clear is
 *    zero.
 * 4) If we need to change both in IP and ULP, then the decision taken
 *    while affecting the values in IP and while delivering up to TCP
 *    should be the same.
 *
 * 	There are two cases.
 *
 * 	a) If we reject data at the IP layer (ipsec_check_global_policy()
 *	   failed), we will not deliver it to the ULP, even though they
 *	   are *willing* to accept in *clear*. This is fine as our global
 *	   disposition to icmp messages asks us reject the datagram.
 *
 *	b) If we accept data at the IP layer (ipsec_check_global_policy()
 *	   succeeded or icmp_accept_messages_in_clear is 1), and not able
 *	   to deliver it to ULP (policy failed), it can lead to
 *	   consistency problems. The cases known at this time are
 *	   ICMP_DESTINATION_UNREACHABLE  messages with following code
 *	   values :
 *
 *	   - ICMP_FRAGMENTATION_NEEDED : IP adapts to the new value
 *	     and Upper layer rejects. Then the communication will
 *	     come to a stop. This is solved by making similar decisions
 *	     at both levels. Currently, when we are unable to deliver
 *	     to the Upper Layer (due to policy failures) while IP has
 *	     adjusted ire_max_frag, the next outbound datagram would
 *	     generate a local ICMP_FRAGMENTATION_NEEDED message - which
 *	     will be with the right level of protection. Thus the right
 *	     value will be communicated even if we are not able to
 *	     communicate when we get from the wire initially. But this
 *	     assumes there would be at least one outbound datagram after
 *	     IP has adjusted its ire_max_frag value. To make things
 *	     simpler, we accept in clear after the validation of
 *	     AH/ESP headers.
 *
 *	   - Other ICMP ERRORS : We may not be able to deliver it to the
 *	     upper layer depending on the level of protection the upper
 *	     layer expects and the dispostion in ipsec_inbound_accept_clear().
 *	     ipsec_inbound_accept_clear() decides whether a given ICMP error
 *	     should be accepted in clear when the Upper layer expects secure.
 *	     Thus the communication may get aborted by some bad ICMP
 *	     packets.
 */
static void
icmp_inbound(queue_t *q, mblk_t *mp, int ire_type, ill_t *ill,
    int sum_valid, uint32_t sum, boolean_t mctl_present)
{
	icmph_t	*icmph;
	ipha_t	*ipha;
	int	iph_hdr_length;
	int	hdr_length;
	boolean_t	interested;
	uint32_t	ts;
	uchar_t	*wptr;
	ipif_t	*ipif;
	mblk_t *first_mp;
	ipsec_in_t *ii;
	boolean_t secure;

	ASSERT(ill != NULL);

	first_mp = mp;
	if (mctl_present) {
		mp = first_mp->b_cont;
		secure = ipsec_in_is_secure(first_mp);
		ASSERT(mp != NULL);
	} else {
		secure = B_FALSE;
	}

	ipha = (ipha_t *)mp->b_rptr;
	if (icmp_accept_clear_messages == 0) {
		if (!ipsec_check_global_policy(first_mp, NULL)) {
			ipsec_log_policy_failure(q, IPSEC_POLICY_MISMATCH,
			    "icmp_inbound", ipha, secure);
			freemsg(first_mp);
			return;
		}
	}
	/*
	 * We have accepted the ICMP message. It means that we will
	 * respond to the packet if needed. It may not be delivered
	 * to the upper client depending on the policy constraints
	 * and the disposition in ipsec_inbound_accept_clear.
	 */

	ASSERT(ill != NULL);

	BUMP_MIB(icmp_mib.icmpInMsgs);
	iph_hdr_length = IPH_HDR_LENGTH(ipha);
	if ((mp->b_wptr - mp->b_rptr) < (iph_hdr_length + ICMPH_SIZE)) {
		/* Last chance to get real. */
		if (!pullupmsg(mp, iph_hdr_length + ICMPH_SIZE)) {
			BUMP_MIB(icmp_mib.icmpInErrors);
			freemsg(first_mp);
			return;
		}
		/* Refresh iph following the pullup. */
		ipha = (ipha_t *)mp->b_rptr;
	}
	/* ICMP header checksum, including checksum field, should be zero. */
	if (sum_valid ? (sum != 0 && sum != 0xFFFF) :
	    IP_CSUM(mp, iph_hdr_length, 0)) {
		BUMP_MIB(icmp_mib.icmpInCksumErrs);
		freemsg(first_mp);
		return;
	}
	/* The IP header will always be a multiple of four bytes */
	icmph = (icmph_t *)&mp->b_rptr[iph_hdr_length];
	wptr = (uchar_t *)icmph + ICMPH_SIZE;
	/* We will set "interested" to "true" if we want a copy */
	interested = B_FALSE;
	switch (icmph->icmph_type) {
	case ICMP_ECHO_REPLY:
		BUMP_MIB(icmp_mib.icmpInEchoReps);
		break;
	case ICMP_DEST_UNREACHABLE:
		if (icmph->icmph_code == ICMP_FRAGMENTATION_NEEDED)
			BUMP_MIB(icmp_mib.icmpInFragNeeded);
		interested = B_TRUE;	/* Pass up to transport */
		BUMP_MIB(icmp_mib.icmpInDestUnreachs);
		break;
	case ICMP_SOURCE_QUENCH:
		interested = B_TRUE;	/* Pass up to transport */
		BUMP_MIB(icmp_mib.icmpInSrcQuenchs);
		break;
	case ICMP_REDIRECT:
		if (!ip_ignore_redirect)
			interested = B_TRUE;
		BUMP_MIB(icmp_mib.icmpInRedirects);
		break;
	case ICMP_ECHO_REQUEST:
		/*
		 * Whether to respond to echo requests that come in as IP
		 * broadcasts is subject to debate (what isn't?).  We aim
		 * to please, you pick it.  Default is do it.
		 */
		if (ip_g_resp_to_echo_bcast || ire_type != IRE_BROADCAST)
			interested = B_TRUE;
		BUMP_MIB(icmp_mib.icmpInEchos);
		break;
	case ICMP_ROUTER_ADVERTISEMENT:
	case ICMP_ROUTER_SOLICITATION:
		break;
	case ICMP_TIME_EXCEEDED:
		interested = B_TRUE;	/* Pass up to transport */
		BUMP_MIB(icmp_mib.icmpInTimeExcds);
		break;
	case ICMP_PARAM_PROBLEM:
		interested = B_TRUE;	/* Pass up to transport */
		BUMP_MIB(icmp_mib.icmpInParmProbs);
		break;
	case ICMP_TIME_STAMP_REQUEST:
		/* Response to Time Stamp Requests is local policy. */
		if (ip_g_resp_to_timestamp &&
		    /* So is whether to respond if it was an IP broadcast. */
		    (ire_type != IRE_BROADCAST ||
			ip_g_resp_to_timestamp_bcast)) {
			int tstamp_len = 3 * sizeof (uint32_t);

			if (wptr +  tstamp_len > mp->b_wptr) {
				if (!pullupmsg(mp, wptr + tstamp_len -
				    mp->b_rptr)) {
					BUMP_MIB(ip_mib.ipInDiscards);
					freemsg(first_mp);
					return;
				}
				/* Refresh ipha following the pullup. */
				ipha = (ipha_t *)mp->b_rptr;
				icmph = (icmph_t *)&mp->b_rptr[iph_hdr_length];
				wptr = (uchar_t *)icmph + ICMPH_SIZE;
			}
			interested = B_TRUE;
		}
		BUMP_MIB(icmp_mib.icmpInTimestamps);
		break;
	case ICMP_TIME_STAMP_REPLY:
		BUMP_MIB(icmp_mib.icmpInTimestampReps);
		break;
	case ICMP_INFO_REQUEST:
		/* Per RFC 1122 3.2.2.7, ignore this. */
	case ICMP_INFO_REPLY:
		break;
	case ICMP_ADDRESS_MASK_REQUEST:
		if ((ip_respond_to_address_mask_broadcast ||
		    ire_type != IRE_BROADCAST) &&
		    /* TODO m_pullup of complete header? */
		    (mp->b_datap->db_lim - wptr) >= IP_ADDR_LEN)
			interested = B_TRUE;
		BUMP_MIB(icmp_mib.icmpInAddrMasks);
		break;
	case ICMP_ADDRESS_MASK_REPLY:
		BUMP_MIB(icmp_mib.icmpInAddrMaskReps);
		break;
	default:
		interested = B_TRUE;	/* Pass up to transport */
		BUMP_MIB(icmp_mib.icmpInUnknowns);
		break;
	}
	/* See if there is an ICMP client. */
	if (ipc_proto_fanout[IPPROTO_ICMP].icf_ipc != NULL) {
		/* If there is an ICMP client and we want one too, copy it. */
		mblk_t *first_mp1;

		if (!interested) {
			ip_fanout_proto(q, first_mp, ill, ipha, IP_FF_SEND_ICMP,
			    mctl_present);
			return;
		}
		first_mp1 = copymsg(first_mp);
		if (first_mp1 != NULL) {
			ip_fanout_proto(q, first_mp1, ill, ipha,
			    IP_FF_SEND_ICMP, mctl_present);
		}
	} else if (!interested) {
		freemsg(first_mp);
		return;
	}
	/* We want to do something with it. */
	/* Check db_ref to make sure we can modify the packet. */
	if (mp->b_datap->db_ref > 1) {
		mblk_t	*first_mp1;

		first_mp1 = copymsg(first_mp);
		freemsg(first_mp);
		if (!first_mp1) {
			BUMP_MIB(icmp_mib.icmpOutDrops);
			return;
		}
		first_mp = first_mp1;
		if (mctl_present) {
			mp = first_mp->b_cont;
			ASSERT(mp != NULL);
		} else {
			mp = first_mp;
		}
		ipha = (ipha_t *)mp->b_rptr;
		icmph = (icmph_t *)&mp->b_rptr[iph_hdr_length];
		wptr = (uchar_t *)icmph + ICMPH_SIZE;
	}
	switch (icmph->icmph_type) {
	case ICMP_ADDRESS_MASK_REQUEST:
		/* Set ipif for the non-broadcast case. */
		if (ire_type != IRE_BROADCAST) {
			if (ill->ill_ipif_up_count == 1) {
				ipif = ill->ill_ipif;
			} else {
				ipif = ipif_lookup_remote(ill, ipha->ipha_src);
				if (ipif == NULL) {
					freemsg(first_mp);
					return;
				}
			}
		}
		/*
		 * outging interface must be IPv4
		 */
		ASSERT(ipif != NULL && !ipif->ipif_isv6);
		icmph->icmph_type = ICMP_ADDRESS_MASK_REPLY;
		bcopy(&ipif->ipif_net_mask, wptr, IP_ADDR_LEN);
		BUMP_MIB(icmp_mib.icmpOutAddrMaskReps);
		break;
	case ICMP_ECHO_REQUEST:
		icmph->icmph_type = ICMP_ECHO_REPLY;
		BUMP_MIB(icmp_mib.icmpOutEchoReps);
		break;
	case ICMP_TIME_STAMP_REQUEST: {
		uint32_t *tsp;

		icmph->icmph_type = ICMP_TIME_STAMP_REPLY;
		tsp = (uint32_t *)wptr;
		tsp++;		/* Skip past 'originate time' */
		/* Compute # of milliseconds since midnight */
		ts = (hrestime.tv_sec % (24 * 60 * 60)) * 1000 +
			hrestime.tv_nsec / (NANOSEC / MILLISEC);
		*tsp++ = htonl(ts);	/* Lay in 'receive time' */
		*tsp++ = htonl(ts);	/* Lay in 'send time' */
		BUMP_MIB(icmp_mib.icmpOutTimestampReps);
		break;
	}
	default:
		ipha = (ipha_t *)&icmph[1];
		if ((uchar_t *)&ipha[1] > mp->b_wptr) {
			if (!pullupmsg(mp, (uchar_t *)&ipha[1] - mp->b_rptr)) {
				BUMP_MIB(ip_mib.ipInDiscards);
				freemsg(first_mp);
				return;
			}
			icmph = (icmph_t *)&mp->b_rptr[iph_hdr_length];
			ipha = (ipha_t *)&icmph[1];
		}
		hdr_length = IPH_HDR_LENGTH(ipha);
		if ((uchar_t *)ipha + hdr_length > mp->b_wptr) {
			if (!pullupmsg(mp,
			    (uchar_t *)ipha + hdr_length - mp->b_rptr)) {
				BUMP_MIB(ip_mib.ipInDiscards);
				freemsg(first_mp);
				return;
			}
			icmph = (icmph_t *)&mp->b_rptr[iph_hdr_length];
			ipha = (ipha_t *)&icmph[1];
		}
		switch (icmph->icmph_type) {
		case ICMP_REDIRECT:
			/*
			 * As there is no upper client to deliver, we don't
			 * need the first_mp any more.
			 */
			if (mctl_present) {
				freeb(first_mp);
			}
			icmp_redirect(mp);
			return;
		case ICMP_DEST_UNREACHABLE:
			if (icmph->icmph_code == ICMP_FRAGMENTATION_NEEDED) {
				if (!icmp_inbound_too_big(icmph, ipha)) {
					freemsg(first_mp);
					return;
				}
			}
			/* FALLTHRU */
		default :
			icmp_inbound_error_fanout(q, ill, first_mp, icmph,
			    ipha, iph_hdr_length, hdr_length, mctl_present);
		}
		return;
	}
	/* Send out an ICMP packet */
	icmph->icmph_checksum = 0;
	icmph->icmph_checksum = IP_CSUM(mp, iph_hdr_length, 0);
	if (ire_type == IRE_BROADCAST) {
		/*
		 * Make it look like it was directed to us, so we don't look
		 * like a fool with a broadcast source address.
		 */
		ipif = ipif_lookup_remote(ill, ipha->ipha_src);
		if (ipif == NULL) {
			ip0dbg(("icmp_inbound: "
			    "No source for broadcast/multicast:\n"
			    "\tsrc 0x%x dst 0x%x ill %p "
			    "ipif_lcl_addr 0x%x\n",
			    ntohl(ipha->ipha_src), ntohl(ipha->ipha_dst),
			    (void *)ill,
			    ill->ill_ipif->ipif_lcl_addr));
			freemsg(first_mp);
			return;
		}
		ASSERT(ipif != NULL && !ipif->ipif_isv6);
		ipha->ipha_dst = ipif->ipif_src_addr;
	}
	/* Reset time to live. */
	ipha->ipha_ttl = ip_def_ttl;
	{
		/* Swap source and destination addresses */
		ipaddr_t tmp;

		tmp = ipha->ipha_src;
		ipha->ipha_src = ipha->ipha_dst;
		ipha->ipha_dst = tmp;
	}
	ipha->ipha_ident = 0;
	if (!IS_SIMPLE_IPH(ipha))
		icmp_options_update(ipha);

	if (!mctl_present) {
		/*
		 * This packet should go out the same way as it
		 * came in i.e in clear. To make sure that global
		 * policy will not be applied to this in ip_wput,
		 * we attach a IPSEC_IN mp and clear ipsec_in_secure.
		 */
		ASSERT(first_mp == mp);
		if ((first_mp = ipsec_in_alloc()) == NULL) {
			BUMP_MIB(ip_mib.ipInDiscards);
			freemsg(mp);
			return;
		}
		ii = (ipsec_in_t *)first_mp->b_rptr;

		/* This is not a secure packet */
		ii->ipsec_in_secure = B_FALSE;
		first_mp->b_cont = mp;
	}
	(void) ipsec_in_to_out(first_mp);
	BUMP_MIB(icmp_mib.icmpOutMsgs);
	put(WR(q), first_mp);
}

/* Table from RFC 1191 */
static int icmp_frag_size_table[] =
{ 32000, 17914, 8166, 4352, 2002, 1496, 1006, 508, 296, 68 };

/*
 * Process received ICMP Packet too big.
 * After updating any IRE it does the fanout to any matching transport streams.
 * Assumes the message has been pulled up till the IP header that caused
 * the error.
 *
 * Returns B_FALSE on failure and B_TRUE on success.
 */
static boolean_t
icmp_inbound_too_big(icmph_t *icmph, ipha_t *ipha)
{
	ire_t	*ire;
	int	mtu;
	int	hdr_length;

	ASSERT(icmph->icmph_type == ICMP_DEST_UNREACHABLE &&
	    icmph->icmph_code == ICMP_FRAGMENTATION_NEEDED);

	hdr_length = IPH_HDR_LENGTH(ipha);

	ire = ire_ctable_lookup(ipha->ipha_dst, 0, IRE_CACHE, NULL,
	    NULL, MATCH_IRE_TYPE);

	if (!ire) {
		ip1dbg(("icmp_inbound_too_big: no route for 0x%x\n",
		    ntohl(ipha->ipha_dst)));
		return (B_FALSE);
	}
	/* Drop if the original packet contained a source route */
	if (ip_source_route_included(ipha)) {
		ire_refrele(ire);
		return (B_FALSE);
	}
	/* Check for MTU discovery advice as described in RFC 1191 */
	mtu = ntohs(icmph->icmph_du_mtu);
	mutex_enter(&ire->ire_lock);
	if (icmph->icmph_du_zero == 0 && mtu > 68) {
		/* Reduce the IRE max frag value as advised. */
		ire->ire_max_frag = MIN(ire->ire_max_frag, mtu);
		ip1dbg(("Received mtu from router: %d\n", mtu));
	} else {
		uint32_t length;
		int	i;

		/*
		 * Use the table from RFC 1191 to figure out
		 * the next "plateau" based on the length in
		 * the original IP packet.
		 */
		length = ntohs(ipha->ipha_length);
		if (ire->ire_max_frag <= length &&
		    ire->ire_max_frag >= length - hdr_length) {
			/*
			 * Handle broken BSD 4.2 systems that
			 * return the wrong iph_length in ICMP
			 * errors.
			 */
			ip1dbg(("Wrong mtu: sent %d, ire %d\n",
			    length, ire->ire_max_frag));
			length -= hdr_length;
		}
		for (i = 0; i < A_CNT(icmp_frag_size_table); i++) {
			if (length > icmp_frag_size_table[i])
				break;
		}
		if (i == A_CNT(icmp_frag_size_table)) {
			/* Smaller than 68! */
			ip1dbg(("Too big for packet size %d\n", length));
			ire->ire_max_frag = MIN(ire->ire_max_frag, 576);
			ire->ire_frag_flag = 0;
		} else {
			mtu = icmp_frag_size_table[i];
			ip1dbg(("Calculated mtu %d, packet size %d, before %d",
			    mtu, length, ire->ire_max_frag));
			ire->ire_max_frag = MIN(ire->ire_max_frag, mtu);
			ip1dbg((", after %d\n", ire->ire_max_frag));
		}
		/* Record the new max frag size for the ULP. */
		icmph->icmph_du_zero = 0;
		icmph->icmph_du_mtu = htons((uint16_t)ire->ire_max_frag);
	}
	mutex_exit(&ire->ire_lock);
	ire_refrele(ire);
	return (B_TRUE);
}

/*
 * If the packet in error is Self-Encapsulated, icmp_inbound_error_fanout
 * calls this function.
 */
static mblk_t *
icmp_inbound_self_encap_error(mblk_t *mp, int iph_hdr_length, int hdr_length)
{
	ipha_t *ipha;
	icmph_t *icmph;
	ipha_t *in_ipha;
	int length;

	ASSERT(mp->b_datap->db_type == M_DATA);

	/*
	 * For Self-encapsulated packets, we added an extra IP header
	 * without the options. Inner IP header is the one from which
	 * the outer IP header was formed. Thus, we need to remove the
	 * outer IP header. To do this, we pullup the whole message
	 * and overlay whatever follows the outer IP header over the
	 * outer IP header.
	 */

	if (!pullupmsg(mp, -1)) {
		BUMP_MIB(ip_mib.ipInDiscards);
		return (NULL);
	}

	icmph = (icmph_t *)&mp->b_rptr[iph_hdr_length];
	ipha = (ipha_t *)&icmph[1];
	in_ipha = (ipha_t *)((uchar_t *)ipha + hdr_length);

	/*
	 * The length that we want to overlay is following the inner
	 * IP header. Subtracting the IP header + icmp header + outer
	 * IP header's length should give us the length that we want to
	 * overlay.
	 */
	length = msgdsize(mp) - iph_hdr_length - sizeof (icmph_t) -
	    hdr_length;
	/*
	 * Overlay whatever follows the inner header over the
	 * outer header.
	 */
	bcopy((uchar_t *)in_ipha, (uchar_t *)ipha, length);

	/* Set the wptr to account for the outer header */
	mp->b_wptr -= hdr_length;
	return (mp);
}

/*
 * Try to pass the ICMP message upstream in case the ULP cares.
 *
 * If the packet that caused the ICMP error is secure, we send
 * it to AH/ESP to make sure that the attached packet has a
 * valid association. ipha in the code below points to the
 * IP header of the packet that caused the error.
 *
 * We handle ICMP_FRAGMENTATION_NEEDED(IFN) message differently
 * in the context of IPSEC. Normally we tell the upper layer
 * whenever we send the ire (including ip_bind), the IPSEC header
 * length in ire_ipsec_options_size. TCP can deduce the MSS as it
 * has both the MTU (ire_max_frag) and the ire_ipsec_options_size.
 * Similarly, we pass the new MTU icmph_du_mtu and TCP does the
 * same thing. As TCP has the IPSEC options size that needs to be
 * adjusted, we just pass the MTU unchanged.
 *
 * IFN could have been generated locally or by some router.
 *
 * LOCAL : *ip_wput_ire -> icmp_frag_needed could have generated this.
 *	    This happens because IP adjusted its value of MTU on an
 *	    earlier IFN message and could not tell the upper layer,
 *	    the new adjusted value of MTU e.g. Packet was encrypted
 *	    or there was not enough information to fanout to upper
 *	    layers. Thus on the next outbound datagram, ip_wput_ire
 *	    generates the IFN, where IPSEC processing has *not* been
 *	    done.
 *
 *	   *ip_wput_ire_fragmentit -> ip_wput_frag -> icmp_frag_needed
 *	    could have generated this. This happens because ire_max_frag
 *	    value in IP was set to a new value, while the IPSEC processing
 *	    was being done and after we made the fragmentation check in
 *	    ip_wput_ire. Thus on return from IPSEC processing,
 *	    ip_wput_ipsec_out finds that the new length is > ire_max_frag
 *	    and generates the IFN. As IPSEC processing is over, we fanout
 *	    to AH/ESP to remove the header.
 *
 *	    In both these cases, ipsec_in_loopback will be set indicating
 *	    that IFN was generated locally.
 *
 * ROUTER : IFN could be secure or non-secure.
 *
 *	    * SECURE : We use the IPSEC_IN to fanout to AH/ESP if the
 *	      packet in error has AH/ESP headers to validate the AH/ESP
 *	      headers. AH/ESP will verify whether there is a valid SA or
 *	      not and send it back. We will fanout again if we have more
 *	      data in the packet.
 *
 *	      If the packet in error does not have AH/ESP, we handle it
 *	      like any other case.
 *
 *	    * NON_SECURE : If the packet in error has AH/ESP headers,
 *	      we attach a dummy ipsec_in and send it up to AH/ESP
 *	      for validation. AH/ESP will verify whether there is a
 *	      valid SA or not and send it back. We will fanout again if
 *	      we have more data in the packet.
 *
 *	      If the packet in error does not have AH/ESP, we handle it
 *	      like any other case.
 */
static void
icmp_inbound_error_fanout(queue_t *q, ill_t *ill, mblk_t *mp,
    icmph_t *icmph, ipha_t *ipha, int iph_hdr_length, int hdr_length,
    boolean_t mctl_present)
{
	uint16_t *up;	/* Pointer to ports in ULP header */
	uint32_t ports;	/* reversed ports for fanout */
	ipha_t ripha;	/* With reversed addresses */
	mblk_t *first_mp;
	ipsec_in_t *ii;

	first_mp = mp;
	if (mctl_present) {
		mp = first_mp->b_cont;
		ASSERT(mp != NULL);

		ii = (ipsec_in_t *)first_mp->b_rptr;
		ASSERT(ii->ipsec_in_type == IPSEC_IN);
	} else {
		ii = NULL;
	}

	switch (ipha->ipha_protocol) {
	case IPPROTO_UDP:
		/* Verify that we have at least 8 bytes of the UDP header */
		if ((uchar_t *)ipha + hdr_length + 8 > mp->b_wptr) {
			if (!pullupmsg(mp, (uchar_t *)ipha + hdr_length + 8 -
			    mp->b_rptr)) {
				BUMP_MIB(ip_mib.ipInDiscards);
				goto drop_pkt;
			}
			icmph = (icmph_t *)&mp->b_rptr[iph_hdr_length];
			ipha = (ipha_t *)&icmph[1];
		}
		up = (uint16_t *)((uchar_t *)ipha + hdr_length);

		/*
		 * Attempt to find a client stream based on port.
		 * Note that we do a reverse lookup since the header is
		 * in the form we sent it out.
		 * The ripha header is only used for the IP_UDP_MATCH and we
		 * only set the src and dst addresses and protocol.
		 */
		ripha.ipha_src = ipha->ipha_dst;
		ripha.ipha_dst = ipha->ipha_src;
		ripha.ipha_protocol = ipha->ipha_protocol;
		((uint16_t *)&ports)[0] = up[1];
		((uint16_t *)&ports)[1] = up[0];

		/* Have to change db_type after any pullupmsg */
		mp->b_datap->db_type = M_CTL;

		ip_fanout_udp(q, first_mp, ill, &ripha, ports, IRE_LOCAL, 0,
		    mctl_present);
		return;

	case IPPROTO_TCP:
		/* Verify that we have at least 8 bytes of the TCP header */
		if ((uchar_t *)ipha + hdr_length + 8 > mp->b_wptr) {
			if (!pullupmsg(mp, (uchar_t *)ipha + hdr_length + 8 -
			    mp->b_rptr)) {
				BUMP_MIB(ip_mib.ipInDiscards);
				goto drop_pkt;
			}
			icmph = (icmph_t *)&mp->b_rptr[iph_hdr_length];
			ipha = (ipha_t *)&icmph[1];
		}
		up = (uint16_t *)((uchar_t *)ipha + hdr_length);
		/*
		 * Find a TCP client stream for this packet.
		 * Note that we do a reverse lookup since the header is
		 * in the form we sent it out.
		 * The ripha header is only used for the IP_TCP_*_MATCH and we
		 * only set the src and dst addresses and protocol.
		 */
		ripha.ipha_src = ipha->ipha_dst;
		ripha.ipha_dst = ipha->ipha_src;
		ripha.ipha_protocol = ipha->ipha_protocol;
		((uint16_t *)&ports)[0] = up[1];
		((uint16_t *)&ports)[1] = up[0];

		/* Have to change db_type after any pullupmsg */
		mp->b_datap->db_type = M_CTL;
		ip_fanout_tcp(q, first_mp, &ripha, ports, 0, mctl_present);
		return;

	case IPPROTO_ESP:
	case IPPROTO_AH:
		/*
		 * We need a IPSEC_IN in the front to fanout to AH/ESP.
		 * We will re-use the IPSEC_IN if it is already present as
		 * AH/ESP will not affect any fields in the IPSEC_IN for
		 * ICMP errors. If there is no IPSEC_IN, allocate a new
		 * one and attach it in the front.
		 */
		if (ii != NULL) {
			/*
			 * ip_fanout_proto_again converts the ICMP errors
			 * that come back from AH/ESP to M_DATA so that
			 * if it is non-AH/ESP and we do a pullupmsg in
			 * this function, it would work. Convert it back
			 * to M_CTL before we send up as this is a ICMP
			 * error. This could have been generated locally or
			 * by some router. Validate the inner IPSEC
			 * headers.
			 *
			 * NOTE : ill_index is used by ip_fanout_proto_again
			 * to locate the ill.
			 */
			ASSERT(ill != NULL);
			ii->ipsec_in_ill_index = ill->ill_index;
			first_mp->b_cont->b_datap->db_type = M_CTL;
		} else {
			/*
			 * IPSEC_IN is not present. We attach a ipsec_in
			 * message and send up to IPSEC for validating
			 * and removing the IPSEC headers. Clear
			 * ipsec_in_secure so that when we return
			 * from IPSEC, we don't mistakenly think that this
			 * is a secure packet came from the network.
			 *
			 * NOTE : ill_index is used by ip_fanout_proto_again
			 * to locate the ill.
			 */
			ASSERT(first_mp == mp);
			first_mp = ipsec_in_alloc();
			if (first_mp == NULL) {
				freemsg(mp);
				BUMP_MIB(ip_mib.ipInDiscards);
				return;
			}
			ii = (ipsec_in_t *)first_mp->b_rptr;

			/* This is not a secure packet */
			ii->ipsec_in_secure = B_FALSE;
			first_mp->b_cont = mp;
			mp->b_datap->db_type = M_CTL;
			ASSERT(ill != NULL);
			ii->ipsec_in_ill_index = ill->ill_index;
		}
		ip_fanout_sec_proto(q, first_mp, ipha->ipha_protocol, 0);
		return;
	default:
		/*
		 * The ripha header is only used for the lookup and we
		 * only set the src and dst addresses and protocol.
		 */
		ripha.ipha_src = ipha->ipha_dst;
		ripha.ipha_dst = ipha->ipha_src;
		ripha.ipha_protocol = ipha->ipha_protocol;
		if (ipha->ipha_protocol == IPPROTO_ENCAP) {
			ipha_t *in_ipha;

			if ((uchar_t *)ipha + hdr_length + sizeof (ipha_t) >
			    mp->b_wptr) {
				if (!pullupmsg(mp, (uchar_t *)ipha +
				    hdr_length + sizeof (ipha_t) -
				    mp->b_rptr)) {

					BUMP_MIB(ip_mib.ipInDiscards);
					goto drop_pkt;
				}
				icmph = (icmph_t *)&mp->b_rptr[iph_hdr_length];
				ipha = (ipha_t *)&icmph[1];
			}
			in_ipha = (ipha_t *)((uchar_t *)ipha + hdr_length);

			/* Check for Self-encapsulated tunnels */
			if (in_ipha->ipha_src == ipha->ipha_src &&
			    in_ipha->ipha_dst == ipha->ipha_dst) {

				mp = icmp_inbound_self_encap_error(mp,
				    iph_hdr_length, hdr_length);
				if (mp == NULL)
					goto drop_pkt;
				icmph = (icmph_t *)&mp->b_rptr[iph_hdr_length];
				ipha = (ipha_t *)&icmph[1];

				icmp_inbound_error_fanout(q, ill, first_mp,
				    icmph, ipha, iph_hdr_length, hdr_length,
				    mctl_present);
				return;
			} else if (icmph->icmph_code ==
			    ICMP_FRAGMENTATION_NEEDED && ii != NULL &&
			    ii->ipsec_in_loopback &&
			    ((ii->ipsec_in_ah_done & IPSEC_PREF_REQUIRED) ||
			    (ii->ipsec_in_esp_done & IPSEC_PREF_REQUIRED))) {
				uint_t extra_len, new_len;

				/*
				 * For configured tunnels that get a looped-
				 * back ICMP_FRAGMENTATION_NEEDED message,
				 * adjust the reported new MTU to take into
				 * account the IPsec headers protecting this
				 * configured tunnel.
				 *
				 * This allows the tunnel module (tun.c) to
				 * blindly accept the MTU reported in an
				 * ICMP "too big" message.
				 *
				 * Non-looped back ICMP messages will just be
				 * handled by the security protocols (if
				 * needed), and the first subsequent packet
				 * will hit this path.
				 */

				extra_len = ipsec_extra_length(first_mp);
				new_len = ntohs(icmph->icmph_du_mtu) -
				    extra_len;
				icmph->icmph_du_mtu = htons((uint16_t)new_len);
			}
		}
		/* Have to change db_type after any pullupmsg */
		mp->b_datap->db_type = M_CTL;

		ip_fanout_proto(q, first_mp, ill, &ripha, 0, mctl_present);
		return;
	}
	/* NOTREACHED */
drop_pkt:;
	ip1dbg(("icmp_inbound_error_fanout: drop pkt\n"));
	freemsg(first_mp);
}

/*
 * Update any record route or timestamp options to include this host.
 * Reverse any source route option.
 */
static void
icmp_options_update(ipha_t *ipha)
{
	uint32_t	totallen;
	uchar_t		*opt;
	uint32_t	optval;
	uint32_t	optlen;
	ipaddr_t src;		/* Our local address */
	ipaddr_t dst;

	ip2dbg(("icmp_options_update\n"));
	src = ipha->ipha_src;
	dst = ipha->ipha_dst;
	totallen = ipha->ipha_version_and_hdr_length -
		(uint8_t)((IP_VERSION << 4) + IP_SIMPLE_HDR_LENGTH_IN_WORDS);
	opt = (uchar_t *)&ipha[1];
	totallen <<= 2;
	while (totallen != 0) {
		switch (optval = opt[IPOPT_POS_VAL]) {
		case IPOPT_EOL:
			return;
		case IPOPT_NOP:
			optlen = 1;
			break;
		default:
			optlen = opt[IPOPT_POS_LEN];
		}
		ip2dbg(("icmp_options_update: opt %d, len %d\n",
		    optval, optlen));

		if (optlen == 0 || optlen > totallen)
			break;

		switch (optval) {
			int	off1, off2;
		case IPOPT_SSRR:
		case IPOPT_LSRR:

			/* Reverse source route */
			/*
			 * First entry should be the next to last one in the
			 * current source route (the last entry is our
			 * address).
			 * The last entry should be the final destination.
			 */
			off1 = IPOPT_MINOFF_SR - 1;
			off2 = opt[IPOPT_POS_OFF] - IP_ADDR_LEN - 1;
			if (off2 < 0) {
				/* No entries in source route */
				ip1dbg((
				    "icmp_options_update: bad src route\n"));
				break;
			}
			bcopy((char *)opt + off2, &dst, IP_ADDR_LEN);
			bcopy(&ipha->ipha_dst, (char *)opt + off2, IP_ADDR_LEN);
			bcopy(&dst, &ipha->ipha_dst, IP_ADDR_LEN);
			off2 -= IP_ADDR_LEN;

			while (off1 < off2) {
				bcopy((char *)opt + off1, &src, IP_ADDR_LEN);
				bcopy((char *)opt + off2, (char *)opt + off1,
				    IP_ADDR_LEN);
				bcopy(&src, (char *)opt + off2, IP_ADDR_LEN);
				off1 += IP_ADDR_LEN;
				off2 -= IP_ADDR_LEN;
			}
			opt[IPOPT_POS_OFF] = IPOPT_MINOFF_SR;
			break;
		}
		totallen -= optlen;
		opt += optlen;
	}
}

/*
 * Process received ICMP Redirect messages.
 */
/* ARGSUSED */
static void
icmp_redirect(mblk_t *mp)
{
	ipha_t	*ipha;
	int	iph_hdr_length;
	icmph_t	*icmph;
	ipha_t	*ipha_err;
	ire_t	*ire;
	ire_t	*prev_ire;
	ire_t	*save_ire;
	ipaddr_t  src, dst, gateway;
	iulp_t	ulp_info = { 0 };

	ipha = (ipha_t *)mp->b_rptr;
	iph_hdr_length = IPH_HDR_LENGTH(ipha);
	if (((mp->b_wptr - mp->b_rptr) - iph_hdr_length) <
	    sizeof (icmph_t) + IP_SIMPLE_HDR_LENGTH) {
		BUMP_MIB(icmp_mib.icmpInErrors);
		freemsg(mp);
		return;
	}
	icmph = (icmph_t *)&mp->b_rptr[iph_hdr_length];
	ipha_err = (ipha_t *)&icmph[1];
	src = ipha->ipha_src;
	dst = ipha_err->ipha_dst;
	gateway = icmph->icmph_rd_gateway;
	/* Make sure the new gateway is reachable somehow. */
	ire = ire_route_lookup(gateway, 0, 0, IRE_INTERFACE, NULL, NULL,
	    NULL, MATCH_IRE_TYPE);
	/*
	 * Make sure we had a route for the dest in question and that
	 * that route was pointing to the old gateway (the source of the
	 * redirect packet.)
	 */
	prev_ire = ire_route_lookup(dst, 0, src, 0, NULL, NULL, NULL,
	    MATCH_IRE_GW);
	/*
	 * Check that
	 *	the redirect was not from ourselves
	 *	the new gateway and the old gateway are directly reachable
	 */
	if (!prev_ire ||
	    !ire ||
	    ire->ire_type == IRE_LOCAL) {
		BUMP_MIB(icmp_mib.icmpInBadRedirects);
		freemsg(mp);
		if (ire != NULL)
			ire_refrele(ire);
		if (prev_ire != NULL)
			ire_refrele(prev_ire);
		return;
	}

	/*
	 * Should we use the old ULP info to create the new gateway?  From
	 * a user's perspective, we should inherit the info so that it
	 * is a "smooth" transition.  If we do not do that, then new
	 * connections going thru the new gateway will have no route metrics,
	 * which is counter-intuitive to user.  From a network point of
	 * view, this may or may not make sense even though the new gateway
	 * is still directly connected to us so the route metrics should not
	 * change much.
	 *
	 * But if the old ire_uinfo is not initialized, we do another
	 * recursive lookup on the dest using the new gateway.  There may
	 * be a route to that.  If so, use it to initialize the redirect
	 * route.
	 */
	if (prev_ire->ire_uinfo.iulp_set) {
		bcopy(&prev_ire->ire_uinfo, &ulp_info, sizeof (iulp_t));
	} else {
		ire_t *tmp_ire;
		ire_t *sire;

		tmp_ire = ire_ftable_lookup(dst, 0, gateway, 0, NULL, &sire,
		    NULL, 0,
		    (MATCH_IRE_RECURSIVE | MATCH_IRE_GW | MATCH_IRE_DEFAULT));
		if (sire != NULL) {
			bcopy(&sire->ire_uinfo, &ulp_info, sizeof (iulp_t));
			/*
			 * If sire != NULL, ire_ftable_lookup() should not
			 * return a NULL value.
			 */
			ASSERT(tmp_ire != NULL);
			ire_refrele(tmp_ire);
			ire_refrele(sire);
		} else if (tmp_ire != NULL) {
			bcopy(&tmp_ire->ire_uinfo, &ulp_info,
			    sizeof (iulp_t));
			ire_refrele(tmp_ire);
		}
	}
	if (prev_ire->ire_type == IRE_CACHE)
		ire_delete(prev_ire);
	ire_refrele(prev_ire);
	/*
	 * TODO: more precise handling for cases 0, 2, 3, the latter two
	 * require TOS routing
	 */
	switch (icmph->icmph_code) {
	case 0:
	case 1:
		/* TODO: TOS specificity for cases 2 and 3 */
	case 2:
	case 3:
		break;
	default:
		freemsg(mp);
		BUMP_MIB(icmp_mib.icmpInBadRedirects);
		ire_refrele(ire);
		return;
	}
	/*
	 * Create a Route Association.  This will allow us to remember that
	 * someone we believe told us to use the particular gateway.
	 */
	save_ire = ire;
	ire = ire_create(
		(uchar_t *)&dst,			/* dest addr */
		(uchar_t *)&ip_g_all_ones,		/* mask */
		(uchar_t *)&save_ire->ire_src_addr,	/* source addr */
		(uchar_t *)&gateway,			/* gateway addr */
		save_ire->ire_max_frag,			/* max frag */
		NULL,					/* Fast Path header */
		NULL,					/* no rfq */
		NULL,					/* no stq */
		IRE_HOST_REDIRECT,
		NULL,
		NULL,
		0,
		0,
		0,
		(RTF_DYNAMIC | RTF_GATEWAY | RTF_HOST),
		&ulp_info);

	ire_refrele(save_ire);
	if (ire == NULL) {
		freemsg(mp);
		return;
	}
	ire = ire_add(ire);
	if (ire) {
		ire_refrele(ire);		/* Held in ire_add_v4 */
		/* tell routing sockets that we received a redirect */
		ip_rts_change(RTM_REDIRECT, dst, gateway, IP_HOST_MASK, 0, src,
		    (RTF_DYNAMIC | RTF_GATEWAY | RTF_HOST), 0,
		    (RTA_DST | RTA_GATEWAY | RTA_NETMASK | RTA_AUTHOR));
	}

	/*
	 * Delete any existing IRE_HOST_REDIRECT for this destination.
	 * This together with the added IRE has the effect of
	 * modifying an existing redirect.
	 */
	prev_ire = ire_ftable_lookup(dst, 0, src, IRE_HOST_REDIRECT, NULL, NULL,
	    NULL, 0, (MATCH_IRE_GW | MATCH_IRE_TYPE));
	if (prev_ire) {
		ire_delete(prev_ire);
		ire_refrele(prev_ire);
	}

	freemsg(mp);
}

/*
 * Generate an ICMP parameter problem message.
 */
static void
icmp_param_problem(queue_t *q, mblk_t *mp, uint8_t ptr)
{
	icmph_t	icmph;
	boolean_t mctl_present;
	mblk_t *first_mp;

	EXTRACT_PKT_MP(mp, first_mp, mctl_present);

	if (!(mp = icmp_pkt_err_ok(mp))) {
		if (mctl_present)
			freeb(first_mp);
		return;
	}

	bzero(&icmph, sizeof (icmph_t));
	icmph.icmph_type = ICMP_PARAM_PROBLEM;
	icmph.icmph_pp_ptr = ptr;
	BUMP_MIB(icmp_mib.icmpOutParmProbs);
	icmp_pkt(q, first_mp, &icmph, sizeof (icmph_t), mctl_present);
}

/*
 * Build and ship an IPv4 ICMP message using the packet data in mp, and
 * the ICMP header pointed to by "stuff".  (May be called as writer.)
 * Note: assumes that icmp_pkt_err_ok has been called to verify that
 * an icmp error packet can be sent.
 * Assigns an appropriate source address to the packet. If ipha_dst is
 * one of our addresses use it for source. Otherwise pick a source based
 * on a route lookup back to ipha_src.
 * Note that ipha_src must be set here since the
 * packet is likely to arrive on an ill queue in ip_wput() which will
 * not set a source address.
 */
static void
icmp_pkt(queue_t *q, mblk_t *mp, void *stuff, size_t len,
    boolean_t mctl_present)
{
	ipaddr_t dst;
	icmph_t	*icmph;
	ipha_t	*ipha;
	uint_t	len_needed;
	size_t	msg_len;
	mblk_t	*mp1;
	ipaddr_t src;
	ire_t	*ire;
	mblk_t *ipsec_mp;

	if (mctl_present) {
		/*
		 * If it is :
		 *
		 * 1) a IPSEC_OUT, then this is caused by outbound
		 *    datagram originating on this host. IPSEC processing
		 *    may or may not have been done. Refer to comments above
		 *    icmp_inbound_error_fanout for details.
		 *
		 * 2) a IPSEC_IN if we are generating a icmp_message
		 *    for an incoming datagram destined for us i.e called
		 *    from ip_fanout_send_icmp.
		 */
		ipsec_info_t *in;
		ipsec_mp = mp;
		mp = ipsec_mp->b_cont;

		in = (ipsec_info_t *)ipsec_mp->b_rptr;

		ASSERT(in->ipsec_info_type == IPSEC_OUT ||
		    in->ipsec_info_type == IPSEC_IN);

		if (in->ipsec_info_type == IPSEC_IN) {
			/*
			 * Convert the IPSEC_IN to IPSEC_OUT. ipsec_in_to_out
			 * uses the information contained in the IP header
			 * also. So we need to call ipsec_in_to_out before we
			 * change the b_cont linkage below.
			 */
			(void) ipsec_in_to_out(ipsec_mp);
		}
	} else {
		/*
		 * This is in clear. The icmp message we are building
		 * here should go out in clear.
		 */
		ipsec_in_t *ii;
		ASSERT(mp->b_datap->db_type == M_DATA);
		if ((ipsec_mp = ipsec_in_alloc()) == NULL) {
			freemsg(mp);
			BUMP_MIB(ip_mib.ipInDiscards);
			return;
		}
		ii = (ipsec_in_t *)ipsec_mp->b_rptr;

		/* This is not a secure packet */
		ii->ipsec_in_secure = B_FALSE;
		ipsec_mp->b_cont = mp;
		mctl_present = B_TRUE;
		/*
		 * Convert the IPSEC_IN to IPSEC_OUT. ipsec_in_to_out
		 * uses the information contained in the IP header
		 * also. So we need to call ipsec_in_to_out before we
		 * change the b_cont linkage below.
		 */
		(void) ipsec_in_to_out(ipsec_mp);
	}

	ipha = (ipha_t *)mp->b_rptr;
	/* Remember our eventual destination */
	dst = ipha->ipha_src;

	ire = ire_route_lookup(ipha->ipha_dst, 0, 0, (IRE_LOCAL|IRE_LOOPBACK),
	    NULL, NULL, NULL, MATCH_IRE_TYPE);
	if (ire != NULL) {
		src = ipha->ipha_dst;
	} else {
		ire = ire_route_lookup(ipha->ipha_src, 0, 0, 0,
		    NULL, NULL, NULL, MATCH_IRE_DEFAULT|MATCH_IRE_RECURSIVE);
		if (ire == NULL) {
			BUMP_MIB(ip_mib.ipOutNoRoutes);
			freemsg(ipsec_mp);
			return;
		}
		src = ire->ire_src_addr;
	}

	ire_refrele(ire);
	/*
	 * Check if we can send back more then 8 bytes in addition
	 * to the IP header. We will include as much as 64 bytes.
	 */
	len_needed = IPH_HDR_LENGTH(ipha) + ip_icmp_return;
	msg_len = msgdsize(mp);
	if (msg_len > len_needed) {
		(void) adjmsg(mp, len_needed - msg_len);
		msg_len = len_needed;
	}
	mp1 = allocb(sizeof (icmp_ipha) + len, BPRI_HI);
	if (!mp1) {
		BUMP_MIB(icmp_mib.icmpOutErrors);
		freemsg(ipsec_mp);
		return;
	}
	mp1->b_cont = mp;
	mp = mp1;
	if (mctl_present) {
		ipsec_mp->b_cont = mp;
	} else {
		ipsec_mp = mp;
	}
	ipha = (ipha_t *)mp->b_rptr;
	mp1->b_wptr = (uchar_t *)ipha + (sizeof (icmp_ipha) + len);
	*ipha = icmp_ipha;
	ipha->ipha_src = src;
	ipha->ipha_dst = dst;
	ipha->ipha_ttl = ip_def_ttl;
	msg_len += sizeof (icmp_ipha) + len;
	if (msg_len > IP_MAXPACKET) {
		(void) adjmsg(mp, IP_MAXPACKET - msg_len);
		msg_len = IP_MAXPACKET;
	}
	ipha->ipha_length = htons((uint16_t)msg_len);
	icmph = (icmph_t *)&ipha[1];
	bcopy(stuff, icmph, len);
	icmph->icmph_checksum = 0;
	icmph->icmph_checksum = IP_CSUM(mp, (int32_t)sizeof (ipha_t), 0);
	BUMP_MIB(icmp_mib.icmpOutMsgs);
	put(q, ipsec_mp);
}

/*
 * Determine if an ICMP error packet can be sent given the rate limit.
 * The limit consists of an average frequency (icmp_pkt_err_interval measured
 * in milliseconds) and a burst size. Burst size number of packets can
 * be sent arbitrarely closely spaced.
 * The state is tracked using two variables to implement an approximate
 * token bucket filter:
 *	icmp_pkt_err_last - lbolt value when the last burst started
 *	icmp_pkt_err_sent - number of packets sent in current burst
 */
boolean_t
icmp_err_rate_limit(void)
{
	clock_t now = TICK_TO_MSEC(lbolt);
	uint_t refilled; /* Number of packets refilled in tbf since last */
	uint_t err_interval = ip_icmp_err_interval; /* Guard against changes */

	if (err_interval == 0)
		return (B_FALSE);

	if (icmp_pkt_err_last > now) {
		/* 100HZ lbolt in ms for 32bit arch wraps every 49.7 days */
		icmp_pkt_err_last = 0;
		icmp_pkt_err_sent = 0;
	}
	/*
	 * If we are in a burst update the token bucket filter.
	 * Update the "last" time to be close to "now" but make sure
	 * we don't loose precision.
	 */
	if (icmp_pkt_err_sent != 0) {
		refilled = (now - icmp_pkt_err_last)/err_interval;
		if (refilled > icmp_pkt_err_sent) {
			icmp_pkt_err_sent = 0;
		} else {
			icmp_pkt_err_sent -= refilled;
			icmp_pkt_err_last += refilled * err_interval;
		}
	}
	if (icmp_pkt_err_sent == 0) {
		/* Start of new burst */
		icmp_pkt_err_last = now;
	}
	if (icmp_pkt_err_sent < ip_icmp_err_burst) {
		icmp_pkt_err_sent++;
		ip1dbg(("icmp_err_rate_limit: %d sent in burst\n",
		    icmp_pkt_err_sent));
		return (B_FALSE);
	}
	ip1dbg(("icmp_err_rate_limit: dropped\n"));
	return (B_TRUE);
}

/*
 * Check if it is ok to send an IPv4 ICMP error packet in
 * response to the IPv4 packet in mp.
 * Free the message and return null if no
 * ICMP error packet should be sent.
 */
static mblk_t *
icmp_pkt_err_ok(mblk_t *mp)
{
	icmph_t	*icmph;
	ipha_t	*ipha;
	uint_t	len_needed;
	ire_t	*src_ire;
	ire_t	*dst_ire;

	if (!mp)
		return (NULL);
	ipha = (ipha_t *)mp->b_rptr;
	if (ip_csum_hdr(ipha)) {
		BUMP_MIB(ip_mib.ipInCksumErrs);
		freemsg(mp);
		return (NULL);
	}
	src_ire = ire_ctable_lookup(ipha->ipha_dst, 0, IRE_BROADCAST,
	    NULL, NULL, MATCH_IRE_TYPE);
	dst_ire = ire_ctable_lookup(ipha->ipha_src, 0, IRE_BROADCAST,
	    NULL, NULL, MATCH_IRE_TYPE);
	if (src_ire != NULL || dst_ire != NULL ||
	    CLASSD(ipha->ipha_dst) ||
	    CLASSD(ipha->ipha_src) ||
	    (ntohs(ipha->ipha_fragment_offset_and_flags) & IPH_OFFSET)) {
		/* Note: only errors to the fragment with offset 0 */
		BUMP_MIB(icmp_mib.icmpOutDrops);
		freemsg(mp);
		if (src_ire != NULL)
			ire_refrele(src_ire);
		if (dst_ire != NULL)
			ire_refrele(dst_ire);
		return (NULL);
	}
	if (ipha->ipha_protocol == IPPROTO_ICMP) {
		/*
		 * Check the ICMP type.  RFC 1122 sez:  don't send ICMP
		 * errors in response to any ICMP errors.
		 */
		len_needed = IPH_HDR_LENGTH(ipha) + ICMPH_SIZE;
		if (mp->b_wptr - mp->b_rptr < len_needed) {
			if (!pullupmsg(mp, len_needed)) {
				BUMP_MIB(icmp_mib.icmpInErrors);
				freemsg(mp);
				return (NULL);
			}
			ipha = (ipha_t *)mp->b_rptr;
		}
		icmph = (icmph_t *)
		    (&((char *)ipha)[IPH_HDR_LENGTH(ipha)]);
		switch (icmph->icmph_type) {
		case ICMP_DEST_UNREACHABLE:
		case ICMP_SOURCE_QUENCH:
		case ICMP_TIME_EXCEEDED:
		case ICMP_PARAM_PROBLEM:
		case ICMP_REDIRECT:
			BUMP_MIB(icmp_mib.icmpOutDrops);
			freemsg(mp);
			return (NULL);
		default:
			break;
		}
	}
	if (icmp_err_rate_limit()) {
		/*
		 * Only send ICMP error packets every so often.
		 * This should be done on a per port/source basis,
		 * but for now this will suffice.
		 */
		freemsg(mp);
		return (NULL);
	}
	return (mp);
}

/*
 * Generate an ICMP redirect message.
 */
static void
icmp_send_redirect(queue_t *q, mblk_t *mp, ipaddr_t gateway)
{
	icmph_t	icmph;

	/*
	 * We are called from ip_rput where we could
	 * not have attached an IPSEC_IN.
	 */
	ASSERT(mp->b_datap->db_type == M_DATA);

	if (!(mp = icmp_pkt_err_ok(mp))) {
		return;
	}

	bzero(&icmph, sizeof (icmph_t));
	icmph.icmph_type = ICMP_REDIRECT;
	icmph.icmph_code = 1;
	icmph.icmph_rd_gateway = gateway;
	BUMP_MIB(icmp_mib.icmpOutRedirects);
	icmp_pkt(q, mp, &icmph, sizeof (icmph_t), B_FALSE);
}

/*
 * Generate an ICMP time exceeded message.
 */
void
icmp_time_exceeded(queue_t *q, mblk_t *mp, uint8_t code)
{
	icmph_t	icmph;
	boolean_t mctl_present;
	mblk_t *first_mp;

	EXTRACT_PKT_MP(mp, first_mp, mctl_present);

	if (!(mp = icmp_pkt_err_ok(mp))) {
		if (mctl_present)
			freeb(first_mp);
		return;
	}

	bzero(&icmph, sizeof (icmph_t));
	icmph.icmph_type = ICMP_TIME_EXCEEDED;
	icmph.icmph_code = code;
	BUMP_MIB(icmp_mib.icmpOutTimeExcds);
	icmp_pkt(q, first_mp, &icmph, sizeof (icmph_t), mctl_present);
}

/*
 * Generate an ICMP unreachable message.
 */
void
icmp_unreachable(queue_t *q, mblk_t *mp, uint8_t code)
{
	icmph_t	icmph;
	mblk_t *first_mp;
	boolean_t mctl_present;

	EXTRACT_PKT_MP(mp, first_mp, mctl_present);

	if (!(mp = icmp_pkt_err_ok(mp))) {
		if (mctl_present)
			freeb(first_mp);
		return;
	}

	bzero(&icmph, sizeof (icmph_t));
	icmph.icmph_type = ICMP_DEST_UNREACHABLE;
	icmph.icmph_code = code;
	BUMP_MIB(icmp_mib.icmpOutDestUnreachs);
	icmp_pkt(q, first_mp, (char *)&icmph, sizeof (icmph_t), mctl_present);
}

/*
 * The  "times up" notification is received by ip_wsrv, which then calls
 * the following function to process sending of membership reports.
 * This function calls the handler routine, and sets up the
 * the mi_timer to wake us up again in (next *
 * igmp_timer_interval) millisecs
 */
static void
igmp_timeout(void)
{
	int next;

	next = igmp_timeout_handler();
	if (next != 0 && igmp_timer_ill)
		mi_timer(igmp_timer_ill->ill_rq, igmp_timer_mp,
		    next * igmp_timer_interval);
}

/*
 * This function uses mi_timer to set off a timer in
 * (next * igmp_timer_interval) millisecs.
 */
void
igmp_timeout_start(int next)
{
	if (next != 0 && igmp_timer_ill)
		mi_timer(igmp_timer_ill->ill_rq, igmp_timer_mp,
		    next * igmp_timer_interval);
}

/*
 * The  "times up" notification is received by ip_wsrv, which then calls
 * the following function to process sending of  MLD membership reports.
 * This function calls the handler routine, and sets up
 * the mi_timer to wake us up again in "next" millisecs.
 */
static void
mld_timeout(void)
{
	int next;

	next = mld_timeout_handler();
	if (next != 0 && mld_timer_ill)
		mi_timer(mld_timer_ill->ill_rq, mld_timer_mp,
		    next);
}

/*
 * This function uses mi_timer to set off a timer in
 * "next" millisecs.
 */
void
mld_timeout_start(int next)
{
	if (next != 0 && mld_timer_ill)
		mi_timer(mld_timer_ill->ill_rq, mld_timer_mp,
		    next);
}

/*
 * News from ARP.  ARP sends notification of interesting events down
 * to its clients using M_CTL messages with the interesting ARP packet
 * attached via b_cont.
 */
static void
ip_arp_news(queue_t *q, mblk_t *mp)
{
	arcn_t	*arcn;
	arh_t	*arh;
	char	*cp1;
	uchar_t	*cp2;
	ire_t	*ire;
	int	i1;
	char	hbuf[128];
	char	sbuf[16];
	ipaddr_t src;

	if ((mp->b_wptr - mp->b_rptr) < sizeof (arcn_t)	|| !mp->b_cont) {
		if (q->q_next) {
			putnext(q, mp);
		} else
			freemsg(mp);
		return;
	}
	arh = (arh_t *)mp->b_cont->b_rptr;
	/* Is it one we are interested in? */
	if (BE16_TO_U16(arh->arh_proto) != IP_ARP_PROTO_TYPE) {
		freemsg(mp);
		return;
	}
	bcopy((char *)&arh[1] + (arh->arh_hlen & 0xFF), &src, IP_ADDR_LEN);
	ire = ire_route_lookup(src, 0, 0, 0, NULL, NULL, NULL,
	    MATCH_IRE_DSTONLY);

	arcn = (arcn_t *)mp->b_rptr;
	switch (arcn->arcn_code) {
	case AR_CN_BOGON:
		/*
		 * Someone is sending ARP packets with a source protocol
		 * address which we have published.  Either they are
		 * pretending to be us, or we have been asked to proxy
		 * for a machine that can do fine for itself, or two
		 * different machines are providing proxy service for the
		 * same protocol address, or something.  We try and do
		 * something appropriate here.
		 */
		cp2 = (uchar_t *)&arh[1];
		cp1 = hbuf;
		*cp1 = '\0';
		for (i1 = arh->arh_hlen; i1--; cp1 += 3)
			(void) sprintf(cp1, "%02x:", *cp2++ & 0xff);
		if (cp1 != hbuf)
			cp1[-1] = '\0';
		(void) ip_dot_addr(src, sbuf);
		if (ire != NULL	&& IRE_IS_LOCAL(ire)) {
			cmn_err(CE_WARN,
			    "IP: Hardware address '%s' trying"
			    " to be our address %s!",
			    hbuf, sbuf);
		} else {
			cmn_err(CE_WARN,
			    "IP: Proxy ARP problem?  "
			    "Hardware address '%s' thinks it is %s",
			    hbuf, sbuf);
		}
		if (ire != NULL)
			ire_refrele(ire);
		break;
	case AR_CN_ANNOUNCE:
		/*
		 * ARP gives us a copy of any broadcast packet with identical
		 * sender and receiver protocol address, in
		 * case we want to intuit something from it.  Such a packet
		 * usually means that a machine has just come up on the net.
		 * If we have an IRE_CACHE, we blow it away.  This way we will
		 * immediately pick up the rare case of a host changing
		 * hardware address.
		 */
		if (ire != NULL) {
			if (ire->ire_type == IRE_CACHE)
				ire_delete(ire);
			ire_refrele(ire);
			/*
			 * The address in "src" may be an entry for a router.
			 * (Default router, or non-default router.)  If
			 * that's true, then any off-net IRE_CACHE entries
			 * that go through the router with address "src"
			 * must be clobbered.  Use ire_walk to achieve this
			 * goal.
			 *
			 * It should be possible to determine if the address
			 * in src is or is not for a router.  This way,
			 * the ire_walk() isn't called all of the time here.
			 * Do not pass 'src' value of 0 to ire_delete_cache_gw,
			 * as it would remove all IRE_CACHE entries for onlink
			 * destinations. All onlink destinations have
			 * ire_gateway_addr == 0.
			 */
			if (src != 0)
				ire_walk_v4(ire_delete_cache_gw, (char *)&src);
		}
		break;
	default:
		if (ire != NULL)
			ire_refrele(ire);
		break;
	}
	freemsg(mp);
}

/*
 * If there is some policy for this ipc and the ULP is TCP, we need
 * to pre-allocate a ipsec_out and ipsec_req_in, which will be sent
 * up to TCP during the unbind ACK.
 */
static int
ipsec_attach_tcp_unbind_mps(ipc_t *ipc)
{
	ASSERT(ipc->ipc_out_enforce_policy || ipc->ipc_in_enforce_policy);

	/*
	 * ipsec_out and ipsec_req_in are usually NULL. If an unbind
	 * never happened for a previous TCP connection e.g. disconnect,
	 * these can be non-null.
	 */
	if (ipc->ipc_ipsec_out == NULL) {
		ipc->ipc_ipsec_out = allocb(sizeof (ipsec_info_t), BPRI_HI);
	}
	if (ipc->ipc_ipsec_req_in == NULL) {
		ipc->ipc_ipsec_req_in = allocb(sizeof (ipsec_req_t), BPRI_HI);
	}

	if (ipc->ipc_ipsec_out == NULL || ipc->ipc_ipsec_req_in == NULL) {

		if (ipc->ipc_ipsec_out != NULL)
			freemsg(ipc->ipc_ipsec_out);

		if (ipc->ipc_ipsec_req_in != NULL)
			freemsg(ipc->ipc_ipsec_req_in);

		ASSERT(ipc->ipc_outbound_policy != NULL &&
		    ipc->ipc_inbound_policy != NULL);

		ipc->ipc_out_enforce_policy = B_FALSE;
		ipc->ipc_in_enforce_policy = B_FALSE;

		kmem_free(ipc->ipc_outbound_policy, sizeof (ipsec_req_t));
		kmem_free(ipc->ipc_inbound_policy, sizeof (ipsec_req_t));

		ipc->ipc_ipsec_out = NULL;
		ipc->ipc_ipsec_req_in = NULL;
		ipc->ipc_outbound_policy = NULL;
		ipc->ipc_inbound_policy = NULL;
		return (ENOMEM);
	}
	return (0);
}

/*
 * This is called whenver we are going to set/inherit policy on the
 * ipc. It assumes that both outbound and inbound policy will be
 * present, which is the common case.
 */
static int
ipsec_policy_alloc(ipc_t *ipc)
{
	ASSERT(ipc->ipc_outbound_policy == NULL &&
	    ipc->ipc_inbound_policy == NULL);

	ipc->ipc_outbound_policy = kmem_alloc(sizeof (ipsec_req_t),
	    KM_NOSLEEP);
	ipc->ipc_inbound_policy = kmem_alloc(sizeof (ipsec_req_t),
	    KM_NOSLEEP);

	if (ipc->ipc_outbound_policy == NULL ||
	    ipc->ipc_inbound_policy == NULL) {

		ipc->ipc_out_enforce_policy = B_FALSE;
		ipc->ipc_in_enforce_policy = B_FALSE;

		if (ipc->ipc_outbound_policy != NULL) {
			kmem_free(ipc->ipc_outbound_policy,
			    sizeof (ipsec_req_t));
		}

		if (ipc->ipc_inbound_policy != NULL) {
			kmem_free(ipc->ipc_inbound_policy,
			    sizeof (ipsec_req_t));
		}

		ipc->ipc_outbound_policy = NULL;
		ipc->ipc_inbound_policy = NULL;
		return (ENOMEM);
	}
	return (0);
}

static int
ip_bind_inherit_global_policy(ipc_t *ipc)
{
	ipsec_policy_t *head;
	boolean_t ret;
	ipsec_req_t ipsr;
	ipsec_selector_t sel;
	int err;

	sel.protocol = ipc->ipc_ulp;
	sel.src_addr = ipc->ipc_laddr;
	sel.dst_addr = ipc->ipc_faddr;
	sel.src_port = ipc->ipc_lport;
	sel.dst_port = ipc->ipc_fport;
	sel.src_mask = sel.dst_mask = 0;

	/*
	 * This function is called when somebody is becoming hard bound.
	 * Start fresh on the policy. If somebody rebinds to a new
	 * address e.g disconnect from above does not unbind.
	 * For such cases we have to inherit global policy again.
	 */
	sel.outbound = IPSEC_OUTBOUND;
	head = &ipsec_policy_head[IPSEC_TYPE_OUTBOUND];
	if (head->ipsec_policy_next != NULL) {
		if (ipc->ipc_out_enforce_policy) {
			ASSERT(ipc->ipc_outbound_policy != NULL);
			bcopy(ipc->ipc_outbound_policy, &ipsr,
			    sizeof (ipsec_req_t));
		} else {
			bzero(&ipsr, sizeof (ipsec_req_t));
		}
		ret = ipsec_inherit_global_policy(ipc, &ipsr, &sel);
		if (ret) {
			if (ipc->ipc_outbound_policy == NULL) {
				if ((err = ipsec_policy_alloc(ipc)) != 0) {
					return (err);
				}
			}
			ipc->ipc_out_enforce_policy = ret;
			bcopy(&ipsr, ipc->ipc_outbound_policy,
			    sizeof (ipsec_req_t));
		}
	}

	sel.outbound = IPSEC_INBOUND;
	head = &ipsec_policy_head[IPSEC_TYPE_INBOUND];
	if (head->ipsec_policy_next != NULL) {
		if (ipc->ipc_in_enforce_policy) {
			ASSERT(ipc->ipc_inbound_policy != NULL);
			bcopy(ipc->ipc_inbound_policy, &ipsr,
			    sizeof (ipsec_req_t));
		} else {
			bzero(&ipsr, sizeof (ipsec_req_t));
		}
		ret = ipsec_inherit_global_policy(ipc, &ipsr, &sel);
		if (ret) {
			if (ipc->ipc_inbound_policy == NULL) {
				if ((err = ipsec_policy_alloc(ipc)) != 0) {
					return (err);
				}
			}
			ipc->ipc_in_enforce_policy = ret;
			bcopy(&ipsr, ipc->ipc_inbound_policy,
			    sizeof (ipsec_req_t));
		}
	}
	if ((ipc->ipc_ulp == IPPROTO_TCP) &&
	    (ipc->ipc_out_enforce_policy || ipc->ipc_in_enforce_policy)) {
		return (ipsec_attach_tcp_unbind_mps(ipc));
	}
	return (0);
}

static int
ip_bind_ipsec_policy_set(ipc_t *ipc, mblk_t *policy_mp)
{
	int ret;

	ASSERT(policy_mp != NULL);
	ASSERT(policy_mp->b_datap->db_type == IPSEC_POLICY_SET);

	if ((ret = ipsec_policy_alloc(ipc)) != 0) {
		return (ret);
	}

	bcopy(policy_mp->b_rptr, ipc->ipc_outbound_policy,
	    sizeof (ipsec_req_t));
	bcopy(policy_mp->b_rptr, ipc->ipc_inbound_policy,
	    sizeof (ipsec_req_t));
	ipc->ipc_in_enforce_policy = B_TRUE;
	ipc->ipc_out_enforce_policy = B_TRUE;
	return (0);
}

/*
 * Upper level protocols (ULP) pass through bind requests to IP for inspection
 * and to arrange for power-fanout assist.  The ULP is identified by
 * adding a single byte at the end of the original bind message.
 * A ULP other than UDP or TCP that wishes to be recognized passes
 * down a bind with a zero length address.
 *
 * The binding works as follows:
 * - A zero byte address means just bind to the protocol.
 * - A four byte address is treated as a request to validate
 *   that the address is a valid local address, appropriate for
 *   an application to bind to. This does not affect any fanout
 *   information in IP.
 * - A sizeof sin_t byte address is used to bind to only the local address
 *   and port.
 * - A sizeof ipa_conn_t byte address contains complete fanout information
 *   consisting of local and remote addresses and ports.  In
 *   this case, the addresses are both validated as appropriate
 *   for this operation, and, if so, the information is retained
 *   for use in the inbound fanout.
 *
 * The ULP (except in the zero-length bind) can append an
 * additional mblk of db_type IRE_DB_REQ_TYPE or IPSEC_POLICY_SET to the
 * T_BIND_REQ/O_T_BIND_REQ. IRE_DB_REQ_TYPE indicates that the ULP wants
 * a copy of the source or destination IRE (source for local bind;
 * destination for complete bind). IPSEC_POLICY_SET indicates that the
 * policy information contained should be copied on to the ipc.
 *
 * NOTE : Only one of IRE_DB_REQ_TYPE or IPSEC_POLICY_SET can be present.
 */
static void
ip_bind_v4(queue_t *q, mblk_t *mp)
{
	ipc_t		*ipc = (ipc_t *)q->q_ptr;
	ssize_t		len;
	struct T_bind_req	*tbr;
	sin_t		*sin;
	ipa_conn_t	*ac;
	uchar_t		*ucp;
	mblk_t		*mp1;
	boolean_t	ire_requested;
	boolean_t	ipsec_policy_set = B_FALSE;
	int		error = 0;
	int		protocol;

	ASSERT(!ipc->ipc_af_isv6);
	ipc->ipc_pkt_isv6 = B_FALSE;

	len = mp->b_wptr - mp->b_rptr;
	if (len < (sizeof (*tbr) + 1)) {
		(void) mi_strlog(q, 1, SL_ERROR|SL_TRACE,
		    "ip_bind: bogus msg, len %ld", len);
		freemsg(mp);
		return;
	}
	/* Back up and extract the protocol identifier. */
	mp->b_wptr--;
	protocol = *mp->b_wptr & 0xFF;
	tbr = (struct T_bind_req *)mp->b_rptr;
	/* Reset the message type in preparation for shipping it back. */
	mp->b_datap->db_type = M_PCPROTO;

	ipc->ipc_ulp = (uint8_t)protocol;

	/*
	 * Check for a zero length address.  This is from a protocol that
	 * wants to register to receive all packets of its type.
	 */
	if (tbr->ADDR_length == 0) {
		icf_t	*icf;

		/*
		 * These protocols are now intercepted in ip_bind_v6().
		 * Reject protocol-level binds here for now.
		 */
		if (protocol == IPPROTO_TCP || protocol == IPPROTO_AH ||
		    protocol == IPPROTO_ESP)
			goto bad_addr;

		/* No hash here really.  The table is big enough. */
		ipc->ipc_v6laddr = ipv6_all_zeros;
		icf = &ipc_proto_fanout[protocol];
		ipc_hash_insert_wildcard(icf, ipc);
		tbr->PRIM_type = T_BIND_ACK;
		qreply(q, mp);
		return;
	}

	/* Extract the address pointer from the message. */
	ucp = (uchar_t *)mi_offset_param(mp, tbr->ADDR_offset,
	    tbr->ADDR_length);
	if (ucp == NULL) {
		ip1dbg(("ip_bind: no address\n"));
		goto bad_addr;
	}
	if (!OK_32PTR(ucp)) {
		ip1dbg(("ip_bind: unaligned address\n"));
		goto bad_addr;
	}
	/*
	 * Check for trailing mps.
	 */

	mp1 = mp->b_cont;
	ire_requested = (mp1 && mp1->b_datap->db_type == IRE_DB_REQ_TYPE);
	ipsec_policy_set = (mp1 && mp1->b_datap->db_type == IPSEC_POLICY_SET);


	switch (tbr->ADDR_length) {
	default:
		ip1dbg(("ip_bind: bad address length %d\n",
		    (int)tbr->ADDR_length));
		goto bad_addr;

	case IP_ADDR_LEN:
		/* Verification of local address only */
		error = ip_bind_laddr(ipc, mp, *(ipaddr_t *)ucp, 0,
		    ire_requested, ipsec_policy_set, B_FALSE);
		break;

	case sizeof (sin_t):
		sin = (sin_t *)ucp;
		error = ip_bind_laddr(ipc, mp, sin->sin_addr.s_addr,
		    sin->sin_port, ire_requested, ipsec_policy_set, B_TRUE);
		break;

	case sizeof (ipa_conn_t):
		ac = (ipa_conn_t *)ucp;
		error = ip_bind_connected(ipc, mp, ac->ac_laddr, ac->ac_lport,
		    ac->ac_faddr, ac->ac_fport, ire_requested,
		    ipsec_policy_set, B_TRUE);
		break;
	}
	if (error != 0)
		goto bad_addr;
	/*
	 * Pass the IPSEC headers size in ire_ipsec_options_size.
	 * We can't do this in ip_bind_insert_ire because the policy
	 * may not have been inherited at that point in time and hence
	 * ipc_out_enforce_policy may not be set.
	 */
	if (ire_requested && ipc->ipc_out_enforce_policy) {
		ire_t *ire = (ire_t *)mp->b_cont->b_rptr;
		ire->ire_ipsec_options_size = (ipc_ipsec_length(ipc));
	}

	/* Send it home. */
	mp->b_datap->db_type = M_PCPROTO;
	tbr->PRIM_type = T_BIND_ACK;
	qreply(q, mp);
	return;

bad_addr:
	/*
	 * If error = -1 then we generate a TBADADDR - otherwise error is
	 * a unix errno.
	 */
	if (error > 0)
		mp = mi_tpi_err_ack_alloc(mp, TSYSERR, error);
	else
		mp = mi_tpi_err_ack_alloc(mp, TBADADDR, 0);
	if (mp)
		qreply(q, mp);
}

/*
 * Here address is verified to be a valid local address.
 * If the IRE_DB_REQ_TYPE mp is present, a broadcast/multicast
 * address is also considered a valid local address.
 * In the case of a broadcast/multicast address, however, the
 * upper protocol is expected to reset the src address
 * to 0 if it sees a IRE_BROADCAST type returned so that
 * no packets are emitted with broadcast/multicast address as
 * source address (that violates hosts requirements RFC1122)
 * The addresses valid for bind are:
 *	(1) - INADDR_ANY (0)
 *	(2) - IP address of an UP interface
 *	(3) - IP address of a DOWN interface
 *	(4) - valid local IP broadcast addresses. In this case
 *	the ipc will only receive packets destined to
 *	the specified broadcast address.
 *	(5) - a multicast address. In this case
 *	the ipc will only receive packets destined to
 *	the specified multicast address. Note: the
 *	application still has to issue an
 *	IP_ADD_MEMBERSHIP socket option.
 *
 * On error, return -1 for TBADADDR otherwise pass the
 * errno with TSYSERR reply.
 */
int
ip_bind_laddr(ipc_t *ipc, mblk_t *mp, ipaddr_t src_addr, uint16_t lport,
    boolean_t ire_requested, boolean_t ipsec_policy_set,
    boolean_t fanout_insert)
{
	int		error = 0;
	ire_t		*src_ire;
	struct T_bind_req	*tbr;
	mblk_t		*policy_mp;

	if (ipsec_policy_set) {
		policy_mp = mp->b_cont;
	}

	/*
	 * If it was previously connected, ipc_fully_bound would have
	 * been set.
	 */
	ipc->ipc_fully_bound = B_FALSE;

	tbr = (struct T_bind_req *)mp->b_rptr;
	src_ire = NULL;

	if (src_addr) {
		/*
		 * If proxy addr is used and src_addr is specified,
		 * this must be an active bind and the proxy addr
		 * set must contain just one single host addr.
		 */
		if (ipc->ipc_palist != NULL) {
			proxy_addr_t *pa = ipc->ipc_palist;

			if ((pa->pa_next != NULL) ||
			    (pa->pa_addr != src_addr) ||
			    (pa->pa_mask != IP_HOST_MASK)) {
				error = EADDRNOTAVAIL;
				ip1dbg(("ip_bind: bad src address 0x%x\n",
				    ntohl(src_addr)));
				goto bad_addr;
			}
			/* Bypass src address checking for proxy addr. */
			goto done;
		}
		src_ire = ire_route_lookup(src_addr, 0, 0, 0, NULL,
		    NULL, NULL, MATCH_IRE_DSTONLY);
		/*
		 * If an address other than 0.0.0.0 is requested,
		 * we verify that it is a valid address for bind
		 * Note: Following code is in if-else-if form for
		 * readability compared to a condition check.
		 */
		/* LINTED - statement has no consequent */
		if (IRE_IS_LOCAL(src_ire)) {
			/*
			 * (2) Bind to address of local UP interface
			 */
		} else if (src_ire && src_ire->ire_type == IRE_BROADCAST) {
			/*
			 * (4) Bind to broadcast address
			 * Note: permitted only from transports that
			 * request IRE
			 */
			if (!ire_requested)
				error = EADDRNOTAVAIL;
		} else if (ipif_lookup_addr(src_addr, NULL) != NULL) {
			/*
			 * (3) Bind to address of local DOWN interface
			 * (ipif_lookup_addr() looks up all interfaces
			 * but we do not get here for UP interfaces
			 * - case (2) above)
			 */
			/*EMPTY*/;
		} else if (CLASSD(src_addr)) {

			if (src_ire != NULL)
				ire_refrele(src_ire);
			/*
			 * (5) bind to multicast address.
			 * Fake out the IRE returned to upper layer to
			 * be a broadcast IRE.
			 */
			src_ire = ire_ctable_lookup(INADDR_BROADCAST,
			    INADDR_ANY, IRE_BROADCAST, NULL, NULL,
			    MATCH_IRE_TYPE);
			if (src_ire == NULL || !ire_requested)
				error = EADDRNOTAVAIL;

		} else {
			/*
			 * Not a valid address for bind
			 */
			error = EADDRNOTAVAIL;
		}
		if (error) {
			/* Red Alert!  Attempting to be a bogon! */
			ip1dbg(("ip_bind: bad src address 0x%x\n",
			    ntohl(src_addr)));
			goto bad_addr;
		}
	}

	/*
	 * Allow setting new policies. For example, disconnects come
	 * down as ipa_t bind. As we would have set ipc_policy_cached
	 * to B_TRUE before, we should set it to B_FALSE, so that policy
	 * can change after the disconnect.
	 */
	ipc->ipc_policy_cached = B_FALSE;

	/*
	 * If not fanout_insert this was just an address verification
	 */
	if (fanout_insert) {
		/*
		 * The addresses have been verified. Time to insert in
		 * the correct fanout list.
		 */
		IN6_IPADDR_TO_V4MAPPED(src_addr, &ipc->ipc_v6laddr);
		IN6_IPADDR_TO_V4MAPPED(INADDR_ANY, &ipc->ipc_v6faddr);
		ipc->ipc_lport = lport;
		ipc->ipc_fport = 0;
		error = ip_bind_fanout_insert(ipc, *mp->b_wptr & 0xFF,
		    tbr->ADDR_length);
	}
done:
	if (error == 0) {
		if (ire_requested) {
			if (!ip_bind_insert_ire(mp, src_ire, NULL)) {
				error = -1;
				goto bad_addr;
			}
		} else if (ipsec_policy_set) {
			error = ip_bind_ipsec_policy_set(ipc, policy_mp);
		}
	}
bad_addr:
	if (src_ire != NULL)
		IRE_REFRELE(src_ire);
	if (ipsec_policy_set) {
		ASSERT(policy_mp != NULL);
		freeb(policy_mp);
		/*
		 * As of now assume that nothing else accompanies
		 * IPSEC_POLICY_SET.
		 */
		mp->b_cont = NULL;
	}
	return (error);
}

/*
 * Verify that both the source and destination addresses
 * are valid.
 * Note that we allow connect to broadcast and multicast
 * addresses when ire_requested is set. Thus the ULP
 * has to check for IRE_BROADCAST and multicast.
 *
 * Returns zero if ok.
 * On error: returns -1 to mean TBADADDR otherwise returns an errno
 * (for use with TSYSERR reply).
 */
int
ip_bind_connected(ipc_t *ipc, mblk_t *mp, ipaddr_t src_addr,
    uint16_t lport, ipaddr_t dst_addr, uint16_t fport,
    boolean_t ire_requested, boolean_t ipsec_policy_set,
    boolean_t fanout_insert)
{
	ire_t		*src_ire;
	ire_t		*dst_ire;
	int		error = 0;
	struct T_bind_req	*tbr;
	int 		protocol;
	mblk_t		*policy_mp;
	ire_t		*sire = NULL;

	tbr = (struct T_bind_req *)mp->b_rptr;
	src_ire = dst_ire = NULL;
	protocol = *mp->b_wptr & 0xFF;

	/*
	 * If we never got a disconnect before, clear it now.
	 */
	ipc->ipc_fully_bound = B_FALSE;

	if (ipsec_policy_set) {
		policy_mp = mp->b_cont;
	}

	if (CLASSD(dst_addr)) {
		/* Pick up an IRE_BROADCAST */
		dst_ire = ire_route_lookup(ip_g_all_ones, 0, 0, 0, NULL,
		    NULL, NULL,
		    (MATCH_IRE_RECURSIVE | MATCH_IRE_DEFAULT));
	} else {
		dst_ire = ire_route_lookup(dst_addr, 0, 0, 0, NULL, &sire,
		    NULL, (MATCH_IRE_RECURSIVE | MATCH_IRE_DEFAULT));
	}
	/*
	 * dst_ire can't be a broadcast when not ire_requested.
	 * We also prevent ire's with src address INADDR_ANY to
	 * be used, which are created temporarily for
	 * sending out packets from endpoints that have
	 * IPC_UNSPEC_SRC set.
	 */
	if (dst_ire == NULL || dst_ire->ire_src_addr == INADDR_ANY ||
	    ((dst_ire->ire_type & IRE_BROADCAST) && !ire_requested)) {
		ip1dbg(("ip_bind_connected: bad connected dst 0x%x\n",
		    ntohl(dst_addr)));
		error = ENETUNREACH;
		goto bad_addr;
	}

	/*
	 * Supply a local source address such that
	 * interface group balancing happens.
	 */
	if (src_addr == INADDR_ANY) {
		/*
		 * Do the moral equivalent of parts of
		 * ip_newroute(), including the possible
		 * reassignment of dst_ire.  Reassignment
		 * should happen if it is enabled, and the
		 * logical interface in question isn't in
		 * a singleton group.
		 *
		 * Note: While we pick a src_ipif we are really only interested
		 * in the ill for load balancing. The source ipif is determined
		 * by source address selection below.
		 */
		ipif_t *dst_ipif = dst_ire->ire_ipif;
		ipif_t *sched_ipif;
		ire_t *sched_ire;

		if (ip_enable_group_ifs &&
		    dst_ipif->ipif_ifgrpnext != dst_ipif) {

			/* Reassign dst_ire based on ifgrp. */

			sched_ipif = ifgrp_scheduler(dst_ipif);
			if (sched_ipif != NULL) {
				sched_ire = ipif_to_ire(sched_ipif);
				/*
				 * Reassign dst_ire to
				 * correspond to the results
				 * of ifgrp scheduling.
				 */
				if (sched_ire != NULL) {
					IRE_REFRELE(dst_ire);
					dst_ire = sched_ire;
				}
			}
		}
		/*
		 * Determine the best source address (preferring non-deprecated
		 * ones) on this ill for the destination.
		 */
		if (dst_ire->ire_ipif->ipif_flags & IFF_DEPRECATED) {
			sched_ipif = ipif_select_source(
			    dst_ire->ire_ipif->ipif_ill, dst_addr);
			if (sched_ipif != NULL) {
				sched_ire = ipif_to_ire(sched_ipif);
				if (sched_ire != NULL) {
					IRE_REFRELE(dst_ire);
					dst_ire = sched_ire;
				}
			}
		}
		src_addr = dst_ire->ire_src_addr;
	}

	/*
	 * Bypass src address checking for proxy addr.
	 */
	if (ipc->ipc_palist == NULL) {
		/*
		 * We do ire_route_lookup() here (and not
		 * interface lookup as we assert that
		 * src_addr should only come from an
		 * UP interface for hard binding.
		 */
		ASSERT(src_ire == NULL);
		src_ire = ire_route_lookup(src_addr, 0, 0, 0, NULL,
		    NULL, NULL, MATCH_IRE_DSTONLY);

		/* src_ire must be a local|loopback */
		if (!IRE_IS_LOCAL(src_ire)) {
			ip1dbg(("ip_bind_connected: bad connected src 0x%x\n",
			    ntohl(src_addr)));
			error = EADDRNOTAVAIL;
			goto bad_addr;
		}

		/*
		 * If the source address is a loopback address, the
		 * destination had best be local or multicast.
		 * The transports that can't handle multicast will reject
		 * those addresses.
		 */
		if (src_ire->ire_type == IRE_LOOPBACK &&
		    !(IRE_IS_LOCAL(dst_ire) || CLASSD(dst_addr))) {
			ip1dbg(("ip_bind_connected: bad connected loopback\n"));
			error = -1;
			goto bad_addr;
		}
	}

	/*
	 * Allow setting new policies. For example, disconnects come
	 * down as ipa_t bind. As we would have set ipc_policy_cached
	 * to B_TRUE before, we should set it to B_FALSE, so that policy
	 * can change after the disconnect.
	 */
	ipc->ipc_policy_cached = B_FALSE;

	/*
	 * Set the ipc addresses/ports immediately, so the IPsec policy calls
	 * can handle their passed-in ipc's.
	 */

	IN6_IPADDR_TO_V4MAPPED(src_addr, &ipc->ipc_v6laddr);
	IN6_IPADDR_TO_V4MAPPED(dst_addr, &ipc->ipc_v6faddr);
	ipc->ipc_lport = lport;
	ipc->ipc_fport = fport;

	ASSERT(!(ipsec_policy_set && ire_requested));
	if (ire_requested) {
		iulp_t *ulp_info = NULL;

		/*
		 * Note that sire will not be NULL if this is an off-link
		 * connection and there is not cache for that dest yet.
		 *
		 * XXX Because of an existing bug, if there are multiple
		 * default routes, the IRE returned now may not be the actual
		 * default route used (default routes are chosen in a
		 * round robin fashion).  So if the metrics for different
		 * default routes are different, we may return the wrong
		 * metrics.  This will not be a problem if the existing
		 * bug is fixed.
		 */
		if (sire != NULL) {
			ulp_info = &(sire->ire_uinfo);
		}
		if (!ip_bind_insert_ire(mp, dst_ire, ulp_info)) {
			error = -1;
			goto bad_addr;
		}
	} else if (ipsec_policy_set) {
		error = ip_bind_ipsec_policy_set(ipc, policy_mp);
		if (error != 0) {
			goto bad_addr;
		}
	}

	/*
	 * Inherit global policy if this connected endpoint isn't
	 * an encapsulating (e.g. tunnel) one.
	 */
	if (protocol != IPPROTO_ENCAP && protocol != IPPROTO_IPV6) {
		error = ip_bind_inherit_global_policy(ipc);
		if (error != 0) {
			goto bad_addr;
		}

		/*
		 * We may or may not have policy for this endpoint.
		 * We still set ipc_policy_cached so that inbound
		 * datagrams don't have to look at global policy
		 * as policy is considered latched for these endpoints.
		 * We should not set ipc_policy_cached until the ipc
		 * reflects the actual policy. If we *set* this before
		 * inheriting the policy there is a window where the check
		 * IPC_INBOUND_POLICY_PRESENT, will neither check with
		 * the policy on the ipc (because we have not yet
		 * copied the policy on to ipc and hence not set
		 * ipc_in_enforce_policy) nor with the global policy
		 * (because ipc_policy_cached is already set).
		 */
		ipc->ipc_policy_cached = B_TRUE;
		/*
		 * We can't insert until we set ipc_policy_cached and we
		 * can't set ipc_policy_cached until ipc reflects the
		 * policy. Note that the ipc_policy_cached is set even
		 * for connections where we don't have a policy. This
		 * is to prevent global policy lookups in the inbound
		 * path.
		 *
		 * If we insert before we set ipc_policy_cached,
		 * IPC_INBOUND_POLICY_PRESENT() check can still evaluate true
		 * because global policy cound be non-empty. We normally
		 * call ipsec_check_policy() for ipc_policy_cached connections
		 * only if ipc_in_enforce_policy is set. But in this case,
		 * ipc_policy_cached can get set anytime since we made the
		 * IPC_INBOUND_POLICY_PRESENT() check and ipsec_check_policy()
		 * is called, which will make the above assumption false.
		 * Thus, we need to insert after we set ipc_policy_cached.
		 */
	}

	if (fanout_insert) {
		/*
		 * The addresses have been verified. Time to insert in
		 * the correct fanout list.
		 */
		error = ip_bind_fanout_insert(ipc, protocol,
		    tbr->ADDR_length);
	}

	if (error == 0)
		ipc->ipc_fully_bound = B_TRUE;
bad_addr:
	if (ipsec_policy_set) {
		ASSERT(policy_mp != NULL);
		freeb(policy_mp);
		/*
		 * As of now assume that nothing else accompanies
		 * IPSEC_POLICY_SET.
		 */
		mp->b_cont = NULL;
	}
	if (src_ire != NULL)
		IRE_REFRELE(src_ire);
	if (dst_ire != NULL)
		IRE_REFRELE(dst_ire);
	if (sire != NULL)
		IRE_REFRELE(sire);
	return (error);
}

/*
 * Insert an ipc in the correct fanout table.
 */
static int
ip_bind_fanout_insert(ipc_t *ipc, int protocol, int addr_len)
{
	icf_t		*icf;
	int		error = 0;

	switch (protocol) {
	case IPPROTO_UDP:
	default:
		/*
		 * Note the requested port number and IP address for use
		 * in the inbound fanout.  Validation (and uniqueness) of
		 * the port/address request is UDPs business.
		 */
		if (protocol == IPPROTO_UDP) {
			icf = &ipc_udp_fanout[IP_UDP_HASH(ipc->ipc_lport)];
		} else {
			/* No hash here really.  The table is big enough. */
			icf = &ipc_proto_fanout[protocol];
		}
		/*
		 * Insert entries with a specified remote address first,
		 * followed by those with a specified local address and
		 * ending with those bound to INADDR_ANY. This ensures
		 * that the search from the beginning of a hash bucket
		 * will find the most specific match.
		 * IP_UDP_MATCH assumes this insertion order.
		 */
		if (ipc->ipc_faddr != INADDR_ANY)
			ipc_hash_insert_connected(icf, ipc);
		else if (ipc->ipc_laddr != INADDR_ANY)
			ipc_hash_insert_bound(icf, ipc);
		else
			ipc_hash_insert_wildcard(icf, ipc);
		break;

	case IPPROTO_TCP:
		switch (addr_len) {
		case sizeof (ipa_conn_t):
		case sizeof (ipa6_conn_t):
			ASSERT(ipc->ipc_faddr != INADDR_ANY);
			/* Insert the IPC in the TCP fanout hash table. */
			icf = &ipc_tcp_conn_fanout[
			    IP_TCP_CONN_HASH(ipc->ipc_faddr, ipc->ipc_ports)];
			ipc_hash_insert_connected(icf, ipc);
			break;

		case sizeof (sin_t):
		case sizeof (sin6_t):
			if (ipc->ipc_palist != NULL) {
				/*
				 * This is a proxy listener so insert it into
				 * the proxy listener list. Note that
				 * tcp_bind() w/ tcp_conn_req_max > 0 may be
				 * called multiple times.
				 */
				if (!ipc->ipc_proxy_listen) {
					error = ip_proxy_add_listener(ipc);
				}
				break;
			}
			/* Insert the IPC in the TCP listen hash table. */
			icf = &ipc_tcp_listen_fanout[
			    IP_TCP_LISTEN_HASH(ipc->ipc_lport)];
			if (ipc->ipc_laddr != INADDR_ANY)
				ipc_hash_insert_bound(icf, ipc);
			else
				ipc_hash_insert_wildcard(icf, ipc);
			break;
		}
		break;
	}
	return (error);
}

/*
 * Insert the ire in b_cont. Returns false if it fails (due to lack of space).
 * Prefers dst_ire over src_ire.
 */
static boolean_t
ip_bind_insert_ire(mblk_t *mp, ire_t *ire, iulp_t *ulp_info)
{
	mblk_t	*mp1;
	ire_t *ret_ire = NULL;

	mp1 = mp->b_cont;
	ASSERT(mp1 != NULL);

	if (ire != NULL) {
		/*
		 * mp1 initialized above to IRE_DB_REQ_TYPE
		 * appended mblk. Its <upper protocol>'s
		 * job to make sure there is room.
		 */
		if ((mp1->b_datap->db_lim - mp1->b_rptr) < sizeof (ire_t))
			return (0);

		mp1->b_datap->db_type = IRE_DB_TYPE;
		mp1->b_wptr = mp1->b_rptr + sizeof (ire_t);
		bcopy(ire, mp1->b_rptr, sizeof (ire_t));
		ret_ire = (ire_t *)mp1->b_rptr;
		/*
		 * Pass the latest setting of the ip_path_mtu_discovery and
		 * copy the ulp info if any.
		 */
		ret_ire->ire_frag_flag = (ip_path_mtu_discovery) ?
		    IPH_DF : 0;
		if (ulp_info != NULL) {
			bcopy(ulp_info, &(ret_ire->ire_uinfo),
			    sizeof (iulp_t));
		}
		if (ire->ire_ipif != NULL) {
			ill_t   *ill = ire->ire_ipif->ipif_ill;

			if ((ill != NULL) &&
			    (ill->ill_ick.ick_magic == ICK_M_CTL_MAGIC))
				mp1->b_ick_flag = ICK_VALID;
		}
	} else {
		/*
		 * No IRE was found. Remove IRE mblk.
		 */
		mp->b_cont = mp1->b_cont;
		freeb(mp1);
	}

	return (1);
}

/*
 * Carve "len" bytes out of an mblk chain, consuming any we empty, and duping
 * the final piece where we don't.  Return a pointer to the first mblk in the
 * result, and update the pointer to the next mblk to chew on.  If anything
 * goes wrong (i.e., dupb fails), we waste everything in sight and return a
 * NULL pointer.
 */
mblk_t *
ip_carve_mp(mblk_t **mpp, ssize_t len)
{
	mblk_t	*mp0;
	mblk_t	*mp1;
	mblk_t	*mp2;

	if (!len || !mpp || !(mp0 = *mpp))
		return (NULL);
	/* If we aren't going to consume the first mblk, we need a dup. */
	if (mp0->b_wptr - mp0->b_rptr > len) {
		mp1 = dupb(mp0);
		if (mp1) {
			/* Partition the data between the two mblks. */
			mp1->b_wptr = mp1->b_rptr + len;
			mp0->b_rptr = mp1->b_wptr;
			/*
			 * after adjustments if mblk not consumed is now
			 * unaligned, try to align it. If this fails free
			 * all messages and let upper layer recover.
			 */
			if (!OK_32PTR(mp0->b_rptr)) {
				if (!pullupmsg(mp0, -1)) {
					freemsg(mp0);
					freemsg(mp1);
					*mpp = NULL;
					return (NULL);
				}
			}
		}
		return (mp1);
	}
	/* Eat through as many mblks as we need to get len bytes. */
	len -= mp0->b_wptr - mp0->b_rptr;
	for (mp2 = mp1 = mp0; (mp2 = mp2->b_cont) != 0 && len; mp1 = mp2) {
		if (mp2->b_wptr - mp2->b_rptr > len) {
			/*
			 * We won't consume the entire last mblk.  Like
			 * above, dup and partition it.
			 */
			mp1->b_cont = dupb(mp2);
			mp1 = mp1->b_cont;
			if (!mp1) {
				/*
				 * Trouble.  Rather than go to a lot of
				 * trouble to clean up, we free the messages.
				 * This won't be any worse than losing it on
				 * the wire.
				 */
				freemsg(mp0);
				freemsg(mp2);
				*mpp = NULL;
				return (NULL);
			}
			mp1->b_wptr = mp1->b_rptr + len;
			mp2->b_rptr = mp1->b_wptr;
			/*
			 * after adjustments if mblk not consumed is now
			 * unaligned, try to align it. If this fails free
			 * all messages and let upper layer recover.
			 */
			if (!OK_32PTR(mp2->b_rptr)) {
				if (!pullupmsg(mp2, -1)) {
					freemsg(mp0);
					freemsg(mp2);
					*mpp = NULL;
					return (NULL);
				}
			}
			*mpp = mp2;
			return (mp0);
		}
		/* Decrement len by the amount we just got. */
		len -= mp2->b_wptr - mp2->b_rptr;
	}
	/*
	 * len should be reduced to zero now.  If not our caller has
	 * screwed up.
	 */
	if (len) {
		/* Shouldn't happen! */
		freemsg(mp0);
		*mpp = NULL;
		return (NULL);
	}
	/*
	 * We consumed up to exactly the end of an mblk.  Detach the part
	 * we are returning from the rest of the chain.
	 */
	mp1->b_cont = NULL;
	*mpp = mp2;
	return (mp0);
}

/*
 * IP has been configured as _D_QNEXTLESS for the client side i.e the driver
 * instance. This implies that
 * 1. IP cannot access the read side q_next pointer directly - it must
 *    use routines like putnext and canputnext.
 * 2. ip_close must ensure that all sources of messages being putnext upstream
 *    are gone before qprocsoff is called.
 *
 * #2 is handled by having ip_close do the ipc_hash_remove and wait for
 * ipc_refcnt to drop to zero before calling qprocsoff.
 */
int
ip_close(queue_t *q)
{
	ill_t *ill;
	ipc_t *ipc;
	void  *ptr = q->q_ptr;

	TRACE_1(TR_FAC_IP, TR_IP_CLOSE, "ip_close: q %p", q);

	/*
	 * Call the appropriate delete routine depending on whether this is
	 * a module or device. Since we are _D_MTOCSHARED anything that
	 * needs to run as a writer is handled by the service
	 * procedure. The close routine only handles removing ipc's from
	 * the binding hash lists.
	 * The close routine uses ill_close_flags/ipc_close_flags to
	 * synchronize with ip_wsrv. These fields are only accessed
	 * by ip_open, ip_close, and ip_wsrv and the ordering of setting
	 * the flags ensures that the atomicify of 32 bit load/store
	 * is sufficient thus no locking is needed.
	 */
	if (WR(q)->q_next) {
		ill = (ill_t *)ptr;

		ill->ill_close_flags |= IPCF_CLOSING;
		while (!(ill->ill_close_flags & IPCF_CLOSE_DONE)) {
			qenable(ill->ill_wq);
			qwait(ill->ill_wq);
		}
	} else {
		ipc = (ipc_t *)ptr;

		/*
		 * First remove this ipc from any fanout list it is on.
		 * Then wait until the number of pending putnexts from
		 * the fanout code drops to zero.
		 * Finally, if some other cleanup is needed (multicast router,
		 * multicast membership, or pending interface bringup on this
		 * ipc) let the service procedure handle that since this
		 * close procedure is not running as a writer.
		 */
		if (ipc->ipc_palist != NULL) {
			ip_proxy_remove_listener(ipc);
		} else {
			ipc_hash_remove(ipc);
		}
		if (ipc->ipc_ire_cache != NULL)
			IRE_REFRELE(ipc->ipc_ire_cache);

		mutex_enter(&ipc->ipc_reflock);
		while (ipc->ipc_refcnt != 0)
			cv_wait(&ipc->ipc_refcv, &ipc->ipc_reflock);
		mutex_exit(&ipc->ipc_reflock);

		mutex_destroy(&ipc->ipc_reflock);
		mutex_destroy(&ipc->ipc_irc_lock);
		cv_destroy(&ipc->ipc_refcv);
		if (q == ip_g_mrouter || WR(q) == ip_g_mrouter ||
		    ipc->ipc_ilg_inuse != 0 || ipc->ipc_pending_ill != NULL ||
		    ipc->ipc_wq == ipsec_loader_q) {
			ipc->ipc_close_flags |= IPCF_CLOSING;
			while (!(ipc->ipc_close_flags & IPCF_CLOSE_DONE)) {
				qenable(ipc->ipc_wq);
				qwait(ipc->ipc_wq);
			}
		}
		if (ipc->ipc_outbound_policy != NULL) {
			kmem_free(ipc->ipc_outbound_policy,
			    sizeof (ipsec_req_t));
		}
		if (ipc->ipc_inbound_policy != NULL) {
			kmem_free(ipc->ipc_inbound_policy,
			    sizeof (ipsec_req_t));
		}
		if (ipc->ipc_ipsec_out != NULL)
			freemsg(ipc->ipc_ipsec_out);
		if (ipc->ipc_ipsec_req_in != NULL)
			freemsg(ipc->ipc_ipsec_req_in);
	}
	qprocsoff(q);

	mutex_enter(&ip_mi_lock);
	mi_close_unlink(&ip_g_head, (IDP)ptr);
	mutex_exit(&ip_mi_lock);

	mi_close_free((IDP)ptr);
	q->q_ptr = WR(q)->q_ptr = NULL;

	return (0);
}

/* Return the IP checksum for the IP header at "iph". */
uint16_t
ip_csum_hdr(ipha_t *ipha)
{
	uint16_t	*uph;
	uint32_t	sum;
	int	opt_len;

	opt_len = (ipha->ipha_version_and_hdr_length & 0xF) -
	    IP_SIMPLE_HDR_LENGTH_IN_WORDS;
	uph = (uint16_t *)ipha;
	sum = uph[0] + uph[1] + uph[2] + uph[3] + uph[4] +
		uph[5] + uph[6] + uph[7] + uph[8] + uph[9];
	if (opt_len > 0) {
		do {
			sum += uph[10];
			sum += uph[11];
			uph += 2;
		} while (--opt_len);
	}
	sum = (sum & 0xFFFF) + (sum >> 16);
	sum = ~(sum + (sum >> 16)) & 0xFFFF;
	if (sum == 0xffff)
		sum = 0;
	return ((uint16_t)sum);
}

void
ip_ddi_destroy(void)
{
	extern kmutex_t igmp_ilm_lock;
	extern kmutex_t ifgrp_l_mutex;

	mutex_enter(&ipsec_loader_lock);
	if (ipsec_loader_thread != NULL &&
	    ipsec_loader_thread != IPSEC_LOADER_LOAD_FAILED &&
	    ipsec_loader_thread != IPSEC_LOADER_LOAD_OK) {
		ipsec_loader_sig = IPSEC_LOADER_EXITNOW;
		cv_signal(&ipsec_loader_sig_cv);

		/*
		 * Wait for ipsec_loader() to finish before we destroy
		 * cvs and mutexes.
		 */
		while (ipsec_loader_sig != IPSEC_LOADER_EXITED)
			cv_wait(&ipsec_loader_exit_cv, &ipsec_loader_lock);
	}
	mutex_exit(&ipsec_loader_lock);


	nd_free(&ip_g_nd);

	mutex_destroy(&igmp_ilm_lock);
	mutex_destroy(&ifgrp_l_mutex);
	mutex_destroy(&ip_mi_lock);
	mutex_destroy(&ipsec_loader_lock);
	cv_destroy(&ipsec_loader_cv);
	cv_destroy(&ipsec_loader_sig_cv);
	cv_destroy(&ipsec_loader_exit_cv);
	rw_destroy(&ipsec_conf_lock);
	rw_destroy(&ip_palist_lock);
	ip_fanout_destroy();
	ip_fanout_destroy_v6();
	ip_ire_fini();
}

void
ip_ddi_init(void)
{
	extern kmutex_t igmp_ilm_lock;
	extern kmutex_t ifgrp_l_mutex;

	if (!ip_g_nd) {
		if (!ip_param_register(lcl_param_arr, A_CNT(lcl_param_arr))) {
			nd_free(&ip_g_nd);
		}
	}

	mutex_init(&igmp_ilm_lock, NULL, MUTEX_DEFAULT, 0);
	mutex_init(&ifgrp_l_mutex, NULL, MUTEX_DEFAULT, 0);
	mutex_init(&ip_mi_lock, NULL, MUTEX_DEFAULT, NULL);
	mutex_init(&ipsec_loader_lock, NULL, MUTEX_DEFAULT, NULL);
	cv_init(&ipsec_loader_cv, NULL, CV_DEFAULT, NULL);
	cv_init(&ipsec_loader_sig_cv, NULL, CV_DEFAULT, NULL);
	cv_init(&ipsec_loader_exit_cv, NULL, CV_DEFAULT, NULL);
	rw_init(&ipsec_conf_lock, NULL, RW_DEFAULT, NULL);
	rw_init(&ip_palist_lock, NULL, RW_DEFAULT, NULL);

	ip_fanout_init();
	ip_fanout_init_v6();
	ip_ire_init();
}

/*
 * Allocate and initialize a DLPI template of the specified length.  (May be
 * called as writer.)
 */
mblk_t *
ip_dlpi_alloc(size_t len, t_uscalar_t prim)
{
	mblk_t	*mp;

	mp = allocb(len, BPRI_MED);
	if (!mp)
		return (NULL);
	mp->b_datap->db_type = M_PROTO;
	mp->b_wptr = mp->b_rptr + len;
	bzero(mp->b_rptr, len);
	((dl_unitdata_req_t *)mp->b_rptr)->dl_primitive = prim;
	return (mp);
}

char *
dlpi_prim_str(int prim)
{
	switch (prim) {
	case DL_INFO_REQ:	return ("DL_INFO_REQ");
	case DL_INFO_ACK:	return ("DL_INFO_ACK");
	case DL_ATTACH_REQ:	return ("DL_ATTACH_REQ");
	case DL_DETACH_REQ:	return ("DL_DETACH_REQ");
	case DL_BIND_REQ:	return ("DL_BIND_REQ");
	case DL_BIND_ACK:	return ("DL_BIND_ACK");
	case DL_UNBIND_REQ:	return ("DL_UNBIND_REQ");
	case DL_OK_ACK:		return ("DL_OK_ACK");
	case DL_ERROR_ACK:	return ("DL_ERROR_ACK");
	case DL_ENABMULTI_REQ:	return ("DL_ENABMULTI_REQ");
	case DL_DISABMULTI_REQ:	return ("DL_DISABMULTI_REQ");
	case DL_PROMISCON_REQ:	return ("DL_PROMISCON_REQ");
	case DL_PROMISCOFF_REQ:	return ("DL_PROMISCOFF_REQ");
	case DL_UNITDATA_REQ:	return ("DL_UNITDATA_REQ");
	case DL_UNITDATA_IND:	return ("DL_UNITDATA_IND");
	case DL_UDERROR_IND:	return ("DL_UDERROR_IND");
	case DL_PHYS_ADDR_REQ:	return ("DL_PHYS_ADDR_REQ");
	case DL_PHYS_ADDR_ACK:	return ("DL_PHYS_ADDR_ACK");
	case DL_SET_PHYS_ADDR_REQ:	return ("DL_SET_PHYS_ADDR_REQ");
	default:		return ("<unknown primitive>");
	}
}

/*
 * Debug formatting routine.  Returns a character string representation of the
 * addr in buf, of the form xxx.xxx.xxx.xxx.  This routine takes the address
 * in the form of a ipaddr_t and calls ip_dot_saddr with a pointer.
 */
char *
ip_dot_addr(ipaddr_t addr, char *buf)
{
	return (ip_dot_saddr((uchar_t *)&addr, buf));
}

/*
 * Debug formatting routine.  Returns a character string representation of the
 * addr in buf, of the form xxx.xxx.xxx.xxx.  This routine takes the address
 * as a pointer.  The "xxx" parts including left zero padding so the final
 * string will fit easily in tables.  It would be nice to take a padding
 * length argument instead.
 */
static char *
ip_dot_saddr(uchar_t *addr, char *buf)
{
	(void) mi_sprintf(buf, "%03d.%03d.%03d.%03d",
	    addr[0] & 0xFF, addr[1] & 0xFF, addr[2] & 0xFF, addr[3] & 0xFF);
	return (buf);
}

void
ip_fanout_destroy(void)
{
	int i;

	for (i = 0; i < A_CNT(ipc_udp_fanout); i++) {
		mutex_destroy(&ipc_udp_fanout[i].icf_lock);
	}
	for (i = 0; i < ipc_tcp_conn_hash_size; i++) {
		mutex_destroy(&ipc_tcp_conn_fanout[i].icf_lock);
	}
	for (i = 0; i < A_CNT(ipc_tcp_listen_fanout); i++) {
		mutex_destroy(&ipc_tcp_listen_fanout[i].icf_lock);
	}
	for (i = 0; i < A_CNT(ipc_proto_fanout); i++) {
		mutex_destroy(&ipc_proto_fanout[i].icf_lock);
	}

	mutex_destroy(&rts_clients.icf_lock);
}

void
ip_fanout_init(void)
{
	int i;

	for (i = 0; i < A_CNT(ipc_udp_fanout); i++) {
		mutex_init(&ipc_udp_fanout[i].icf_lock, NULL,
		    MUTEX_DEFAULT, NULL);
	}

	if (ipc_tcp_conn_hash_size & (ipc_tcp_conn_hash_size - 1)) {
		/* Not a power of two. Round up to nearest power of two */
		for (i = 0; i < 31; i++) {
			if (ipc_tcp_conn_hash_size < (1 << i))
				break;
		}
		ipc_tcp_conn_hash_size = 1 << i;
	}
	if (ipc_tcp_conn_hash_size < IP_TCP_CONN_HASH_SIZE) {
		ipc_tcp_conn_hash_size = IP_TCP_CONN_HASH_SIZE;
		cmn_err(CE_CONT, "?ip: using ipc_tcp_conn_hash_size = %d\n",
		    ipc_tcp_conn_hash_size);
	}
	ipc_tcp_conn_fanout =
	    (icf_t *)kmem_zalloc(ipc_tcp_conn_hash_size * sizeof (icf_t),
	    KM_SLEEP);
	for (i = 0; i < ipc_tcp_conn_hash_size; i++) {
		mutex_init(&ipc_tcp_conn_fanout[i].icf_lock, NULL,
		    MUTEX_DEFAULT, NULL);
	}

	for (i = 0; i < A_CNT(ipc_tcp_listen_fanout); i++) {
		mutex_init(&ipc_tcp_listen_fanout[i].icf_lock, NULL,
		    MUTEX_DEFAULT, NULL);
	}
	for (i = 0; i < A_CNT(ipc_proto_fanout); i++) {
		mutex_init(&ipc_proto_fanout[i].icf_lock, NULL,
		    MUTEX_DEFAULT, NULL);
	}

	mutex_init(&rts_clients.icf_lock, NULL, MUTEX_DEFAULT, NULL);
}

/*
 * Send an ICMP error after patching up the packet appropriately.
 */
static void
ip_fanout_send_icmp(queue_t *q, mblk_t *mp, uint_t flags, Counter *mibincr,
		    uint_t icmp_type, uint_t icmp_code, boolean_t mctl_present)
{
	ipha_t *ipha;
	mblk_t *first_mp;
	boolean_t secure;
	unsigned char db_type;

	first_mp = mp;
	if (mctl_present) {
		mp = mp->b_cont;
		secure = ipsec_in_is_secure(first_mp);
		ASSERT(mp != NULL);
	} else {
		/*
		 * If this is an ICMP error being reported - which goes
		 * up as M_CTLs, we need to convert them to M_DATA till
		 * we finish checking with global policy because
		 * ipsec_check_global_policy() assumes M_DATA as clear
		 * and M_CTL as secure.
		 */
		db_type = mp->b_datap->db_type;
		mp->b_datap->db_type = M_DATA;
		secure = B_FALSE;
	}
	/*
	 * We are generating an icmp error for some inbound packet.
	 * Called from all ip_fanout_(udp, tcp, proto) functions.
	 * Before we generate an error, check with global policy
	 * to see whether this is allowed to enter the system. As
	 * there is no "ipc", we are checking with global policy.
	 */
	ipha = (ipha_t *)mp->b_rptr;
	if (!ipsec_check_global_policy(first_mp, NULL)) {
		ipsec_log_policy_failure(q, IPSEC_POLICY_MISMATCH,
		    "ip_fanout_send_icmp", ipha, secure);
		freemsg(first_mp);
		return;
	}

	if (!mctl_present)
		mp->b_datap->db_type = db_type;

	if (flags & IP_FF_SEND_ICMP) {
		BUMP_MIB(*mibincr);
		if (flags & IP_FF_HDR_COMPLETE) {
			if (ip_hdr_complete(ipha)) {
				freemsg(first_mp);
				return;
			}
		}
		if (flags & IP_FF_CKSUM) {
			/*
			 * Have to correct checksum since
			 * the packet might have been
			 * fragmented and the reassembly code in ip_rput
			 * does not restore the IP checksum.
			 */
			ipha->ipha_hdr_checksum = 0;
			ipha->ipha_hdr_checksum = ip_csum_hdr(ipha);
		}
		switch (icmp_type) {
		case ICMP_DEST_UNREACHABLE:
			icmp_unreachable(WR(q), first_mp, icmp_code);
			break;
		default:
			freemsg(first_mp);
			break;
		}
	} else {
		freemsg(first_mp);
	}
}

/*
 * Used to fanout to security protocols. It is very simple
 * minded. There is only one upstream to receive all
 * these messages. If nobody is there to receive these,
 * just free the message. It is different from other fanouts
 * in 2 ways.
 *
 * 1) It does not do policy checks as it is delivering to
 *    AH/ESP.
 * 2) It does not free the ipsec_mp because it might take
 *    more trips to AH/ESP.
 *
 * In other fanouts it is sure that it goes to other ULP's
 * and hence policy check and freeing should take place.
 */
static void
ip_fanout_sec_proto(queue_t *q, mblk_t *ipsec_mp, int protocol, uint_t flags)
{
	icf_t	*icf;
	ipc_t	*ipc;
	queue_t	*rq;

	ASSERT(ipsec_mp->b_datap->db_type == M_CTL);
	ASSERT(ipsec_mp->b_cont != NULL);

	icf = &ipc_proto_fanout[protocol];
	mutex_enter(&icf->icf_lock);
	ipc = (ipc_t *)icf->icf_ipc;
	if (ipc == NULL) {
		/*
		 * No one bound to this protocol.
		 */
		mutex_exit(&icf->icf_lock);

		/*
		 * Check IPv6 version...
		 */
		icf = &ipc_proto_fanout_v6[protocol];
		mutex_enter(&icf->icf_lock);
		ipc = (ipc_t *)icf->icf_ipc;

		if (ipc == NULL) {
			mutex_exit(&icf->icf_lock);
			if (flags & IP_FF_SEND_ICMP) {
				ip_fanout_send_icmp(q, ipsec_mp->b_cont, flags,
				    &ip_mib.ipInUnknownProtos,
				    ICMP_DEST_UNREACHABLE,
				    ICMP_PROTOCOL_UNREACHABLE, B_FALSE);
				freeb(ipsec_mp);
				return;
			} else {
				freemsg(ipsec_mp);
				return;
			}
		}
	}

	ASSERT(ipc->ipc_hash_next == NULL);	/* Only one of a kind */
	IPC_REFHOLD(ipc);
	mutex_exit(&icf->icf_lock);
	rq = ipc->ipc_rq;
	putnext(rq, ipsec_mp);
	IPC_REFRELE(ipc);
}

/*
 * Extract the string from ipsec_policy_failure_msgs[type] and
 * log it.
 *
 * This function needs to be kept in synch with ipsec_rl_strlog() in sadb.h.
 * Eventually this function should use the ipsec_rl_strlog() macro.
 */
void
ipsec_log_policy_failure(queue_t *q, int type, char *func_name, ipha_t *ipha,
    boolean_t secure)
{
	char sbuf[16];
	char dbuf[16];
	hrtime_t current = gethrtime();

	/* Always bump the policy failure counter. */
	ipsec_policy_failure_count[type]++;

	/* Convert interval (in msec) to hrtime (in nsec), which means * 10^6 */
	if (ipsec_policy_failure_last +
	    ((hrtime_t)ipsec_policy_log_interval * (hrtime_t)1000000) <=
	    current) {
		/*
		 * Throttle the logging such that I only log one message
		 * every 'ipsec_policy_log_interval' amount of time.
		 */
		(void) mi_strlog(q, 0, SL_ERROR|SL_WARN|SL_CONSOLE,
		    ipsec_policy_failure_msgs[type],
		    func_name,
		    (secure ? "secure" : "not secure"),
		    ip_dot_addr(ipha->ipha_src, sbuf),
		    ip_dot_addr(ipha->ipha_dst, dbuf));

		ipsec_policy_failure_last = current;
	}

}

boolean_t
ipsec_in_is_secure(mblk_t *ipsec_mp)
{
	ipsec_in_t *ii;

	ii = (ipsec_in_t *)ipsec_mp->b_rptr;
	ASSERT(ii->ipsec_in_type == IPSEC_IN);

	if (ii->ipsec_in_loopback) {
		return ((ii->ipsec_in_ah_done & IPSEC_PREF_REQUIRED) ||
		    (ii->ipsec_in_esp_done & IPSEC_PREF_REQUIRED) ||
		    (ii->ipsec_in_self_encap_done & IPSEC_PREF_REQUIRED));
	} else {
		return (ii->ipsec_in_ah_spi != 0 ||
		    ii->ipsec_in_esp_spi != 0 ||
		    ii->ipsec_in_decaps);
	}
}

/*
 * Handle protocols with which IP is less intimate.  There
 * can be more than one stream bound to a particular
 * protocol.  When this is the case, normally each one gets a copy
 * of any incoming packets.
 *
 * IPSEC NOTE :
 *
 * Don't allow a secure packet going up a non-secure connection.
 * We don't allow this because
 *
 * 1) Reply might go out in clear which will be dropped at
 *    the sending side.
 * 2) If the reply goes out in clear it will give the
 *    adversary enough information for getting the key in
 *    most of the cases.
 *
 * Moreover getting a secure packet when we expect clear
 * implies that SA's were added without checking for
 * policy on both ends. This should not happen once ISAKMP
 * is used to negotiate SAs as SAs will be added only after
 * verifying the policy.
 *
 * NOTE : If the packet was tunneled and not multicast we only send
 * to it the first match. Unlike TCP and UDP fanouts this doesn't fall
 * back to delivering packets to AF_INET6 raw sockets.
 */
static void
ip_fanout_proto(queue_t *q, mblk_t *mp, ill_t *ill, ipha_t *ipha, uint_t flags,
    boolean_t mctl_present)
{
	icf_t	*icf;
	ipc_t	*ipc, *first_ipc, *next_ipc;
	queue_t	*rq;
	mblk_t	*mp1;
	uint_t	protocol = ipha->ipha_protocol;
	ipaddr_t dst;
	ipaddr_t src;
	boolean_t one_only;
	mblk_t *first_mp;
	boolean_t secure;

	first_mp = mp;
	if (mctl_present) {
		mp = first_mp->b_cont;
		secure = ipsec_in_is_secure(first_mp);
		ASSERT(mp != NULL);
	} else {
		secure = B_FALSE;
	}
	dst = ipha->ipha_dst;
	src = ipha->ipha_src;
	/*
	 * If the packet was tunneled and not multicast we only send to it
	 * the first match.
	 */
	one_only = ((protocol == IPPROTO_ENCAP || protocol == IPPROTO_IPV6) &&
	    !CLASSD(dst));

	icf = &ipc_proto_fanout[protocol];
	mutex_enter(&icf->icf_lock);
	ipc = (ipc_t *)&icf->icf_ipc;	/* ipc_hash_next will get first */
	do {
		ipc = ipc->ipc_hash_next;
		if (ipc == NULL) {
			/*
			 * No one bound to these addresses.  Is
			 * there a client that wants all
			 * unclaimed datagrams?
			 */
			mutex_exit(&icf->icf_lock);

			/*
			 * Check for IPPROTO_ENCAP...
			 */
			if (protocol == IPPROTO_ENCAP && ip_g_mrouter) {
				/*
				 * XXX If an IPsec mblk is here on a multicast
				 * tunnel (using ip_mroute stuff), what should
				 * I do?
				 *
				 * For now, just free the IPsec mblk before
				 * passing it up to the multicast routing
				 * stuff.
				 *
				 * BTW,  If I match a configured IP-in-IP
				 * tunnel, ip_mroute_decap will never be
				 * called.
				 */
				if (mp != first_mp)
					freeb(first_mp);
				ip_mroute_decap(q, mp);
			} else {
				/*
				 * Otherwise send an ICMP protocol unreachable.
				 */
				ip_fanout_send_icmp(q, first_mp, flags,
				    &ip_mib.ipInUnknownProtos,
				    ICMP_DEST_UNREACHABLE,
				    ICMP_PROTOCOL_UNREACHABLE, mctl_present);
			}
			return;
		}
	} while (!IP_PROTO_MATCH(ipc, protocol, dst, src));

	IPC_REFHOLD(ipc);
	first_ipc = ipc;
	if (one_only) {
		/*
		 * Only send message to one tunnel driver by immediately
		 * terminating the loop.
		 */
		ipc = NULL;
	} else {
		ipc = ipc->ipc_hash_next;
	}
	for (;;) {
		while (ipc != NULL) {
			if (IP_PROTO_MATCH(ipc, protocol, dst, src))
				break;
			ipc = ipc->ipc_hash_next;
		}

		/*
		 * Just copy the data part alone. The mctl part is
		 * needed just for verifying policy and it is never
		 * sent up.
		 */
		if (ipc == NULL || (((mp1 = dupmsg(mp)) == NULL) &&
		    ((mp1 = copymsg(mp)) == NULL))) {
			/*
			 * No more intested clients or memory
			 * allocation failed
			 */
			ipc = first_ipc;
			break;
		}
		IPC_REFHOLD(ipc);
		mutex_exit(&icf->icf_lock);
		rq = ipc->ipc_rq;
		if (!canputnext(rq)) {
			Counter *mibincr;

			if (flags & IP_FF_RAWIP)
				mibincr = &ip_mib.rawipInOverflows;
			else
				mibincr = &icmp_mib.icmpInOverflows;

			BUMP_MIB(*mibincr);
			freemsg(mp1);
		} else {
			if (IPC_INBOUND_POLICY_PRESENT(ipc)) {
				if (!ipsec_check_policy(first_mp,
				    ipc, IPSEC_INBOUND, mctl_present)) {
					/*
					 * POLICY FAILURE : Loggable Event.
					 */
					ipsec_log_policy_failure(q,
					    IPSEC_POLICY_MISMATCH,
					    "ip_fanout_proto", ipha, secure);
					freemsg(mp1);
				} else {
					BUMP_MIB(ip_mib.ipInDelivers);
					putnext(rq, mp1);
				}
			} else if (secure) {
				/*
				 * Don't allow this. Check IPSEC NOTE above
				 * ip_fanout_proto().
				 */
				ipsec_log_policy_failure(q,
				    IPSEC_POLICY_NOT_NEEDED,
				    "ip_fanout_proto", ipha, secure);
				freemsg(mp1);
			} else {
				BUMP_MIB(ip_mib.ipInDelivers);
				putnext(rq, mp1);
			}
		}
		mutex_enter(&icf->icf_lock);
		/* Follow the next pointer before releasing the ipc. */
		next_ipc = ipc->ipc_hash_next;
		IPC_REFRELE(ipc);
		ipc = next_ipc;
	}

	/* Last one.  Send it upstream. */
only_one:
	mutex_exit(&icf->icf_lock);
	rq = ipc->ipc_rq;
	if (!canputnext(rq)) {
		Counter *mibincr;

		if (flags & IP_FF_RAWIP)
			mibincr = &ip_mib.rawipInOverflows;
		else
			mibincr = &icmp_mib.icmpInOverflows;

		BUMP_MIB(*mibincr);
		freemsg(first_mp);
	} else {
		if (IPC_INBOUND_POLICY_PRESENT(ipc)) {
			if (!ipsec_check_policy(first_mp, ipc, IPSEC_INBOUND,
			    mctl_present)) {
				/*
				 * POLICY FAILURE : Loggable Event.
				 *
				 * NOTE:  If this is a tunnel spoof attempt,
				 *	  special note should be made.
				 */
				IPC_REFRELE(ipc);
				ipsec_log_policy_failure(q,
				    IPSEC_POLICY_MISMATCH,
				    "ip_fanout_proto", ipha, secure);
				freemsg(first_mp);
				return;
			}
		} else if (secure) {
			/*
			 * Don't allow this. Check IPSEC NOTE above
			 * ip_fanout_proto().
			 */
			IPC_REFRELE(ipc);
			ipsec_log_policy_failure(q, IPSEC_POLICY_NOT_NEEDED,
			    "ip_fanout_proto", ipha, secure);
			freemsg(first_mp);
			return;
		}
		BUMP_MIB(ip_mib.ipInDelivers);
		putnext(rq, mp);
		if (mctl_present)
			freeb(first_mp);
	}
	IPC_REFRELE(ipc);
}

/*
 * We have this special function because tcp_g_q's ipc is not the right
 * representative of the policy information of the detached tcp's.
 * So, we defer checking the policy to TCP.
 *
 * This packet is either going up for some detached connection
 * or we could not find a listener here and hence sending up.
 *
 * If the packet is destined for the detached connection
 * we cannot check the policy here as we don't have the
 * right information here e.g. the connection could have
 * used per-socket policy. Hence, we need to check the policy
 * at the TCP level.
 *
 * But if the datagram is not destined for the detached
 * connection, 2 things could happen.
 *
 * 1) It does not find a listener and thus send a reset.
 * 2) It might find a listener by the time it gets to TCP.
 *
 * If we had known that the packet is destined for these
 * two cases, we would have checked policy here. Unfortunately,
 * only TCP knows this. So, we have to check policy only in
 * TCP. For (1), TCP needs to check global policy to see whether
 * this packet is allowed to enter the machine using
 * ipsec_check_global_policy.
 *
 * NOTE: For (2), if there is some per-socket policy for the
 *	connection, we might not be able to verify the policy
 *	correctly as TCP has no idea of accessing the per-socket
 *	policy of the listener. At most, we will accept *one*
 *	packet *wrongly*, and all future packets will be subjected
 *	to policy checks in ip_fanout_tcp_listen. This is not
 *	a *security breach*. It is just a race condition which is
 *	*equivalent* to setting up a global policy while the packet
 *	that entered just before setting up policy, getting accepted.
 *
 *
 * NOTE : Checks for IPv4 fanout first and then IPv6 fanout.
 * strplumb currently only sets up a TCP default queue for IPv6.
 */
void
ip_fanout_tcp_defq(queue_t *q, mblk_t *mp, uint_t flags, boolean_t mctl_present)
{
	icf_t	*icf;
	ipc_t	*ipc;
	queue_t	*rq;

	/*
	 * Assume there is only one TCP STREAM bound to
	 * recieve all the messages.
	 */
	icf = &ipc_proto_fanout[IPPROTO_TCP];
	mutex_enter(&icf->icf_lock);
	ipc = (ipc_t *)icf->icf_ipc;
	if (ipc == NULL) {
		mutex_exit(&icf->icf_lock);
		icf = &ipc_proto_fanout_v6[IPPROTO_TCP];
		mutex_enter(&icf->icf_lock);
		ipc = (ipc_t *)icf->icf_ipc;
		if (ipc == NULL) {
			mutex_exit(&icf->icf_lock);
			/*
			 * tcp_g_q not yet bound. Send an ICMP protocol
			 * unreachable.
			 */
			ip_fanout_send_icmp(q, mp, flags,
			    &ip_mib.ipInUnknownProtos,
			    ICMP_DEST_UNREACHABLE,
			    ICMP_PROTOCOL_UNREACHABLE, mctl_present);
			return;
		}
	}
	IPC_REFHOLD(ipc);
	mutex_exit(&icf->icf_lock);
	rq = ipc->ipc_rq;
	BUMP_MIB(ip_mib.ipInDelivers);
	putnext(rq, mp);
	IPC_REFRELE(ipc);
}

/*
 * Fanout for TCP packets
 * The caller puts <fport, lport> in the ports parameter.
 *
 * IPSEC NOTE :
 *
 * 1) SYN, ACK for the SYN/ACK packets go up the listener and we need
 *    to verify the policy here as the listeners in TCP don't maintain
 *    any policy information for the listeners/ESTABLISHED connections.
 * 2) Packets that are destined for the detached connections also go up
 *    the listener. We can verify the policy for those packets here,
 *    as we *assume* that the *listener* is a good representative of
 *    the actual connection because we inherit policy from the listener
 *    during *ACCEPT* and thus we don't have to send the IPSEC_IN
 *    - message that contains all the policy information - up for
 *    verification of policy at the TCP level. We *can* also drop the
 *    packet here itself, if the listener has no policy and we receive
 *    a secure packet, rather than percolate this up to TCP as the
 *    actual connection cannot have anything different.
 *
 * But, we have one pathological case which happens very rarely in
 * practice, which complicates the issue. If not for this issue, it
 * would be enough if we verify the packet here and not do it in TCP.
 *
 * PATHOLOGICAL CASE :
 *
 * Because of SO_REUSEADDR, there can a detached connection
 * in TIME_WAIT which is in the NON-ACCEPT side (side that did
 * the connect), and a listener bound to the same port. Because
 * it is on the side which did the *CONNECT*, we would not have
 * inherited the policy from the listener and hence this listener
 * is not the right representative of the policy. So, we might be
 * verifying the policy for the packets destined to the *detached*
 * connection wrongly if such a listener exists. TCP works fine
 * because of the lateral_put in tcp_rput_data, which will get to
 * the right tcp_t. For IPSEC, because of case(1) we need to do
 * policy checks here, which means case(2) might be affected
 * which only leads to dropping of packets. To make sure that
 * we don't wrongly accept the packet, we need to check the policy
 * in TCP also which is enumerated in the following cases.
 *
 * A packet is coming in for a detached connection.
 * 1) If this detached connection is in the ACCEPT side,
 *
 *      a) if the listener exists, we will send it up through the
 *	   listener and hence we will verify with the policy
 *	   here.
 *
 *	b) if the listener does not exist, we will send it up through
 *	   the global queue and hence policy check will happen in
 *	   TCP with the right tcp_t.
 *
 *	c) if the listener was re-started with a different policy,
 *	   we might drop packets WRONGLY as we will be comparing policy
 *	   with the wrong listener.
 *
 * 2) If this detached connection is in the CONNECT side,
 *
 *	a) if there is no listener, we will send it up through the
 *	   global queue and hence this is like case(b) above.
 *
 *	b) If somebody had started a listener with SO_REUSEADDR
 *	   and same port, we will send it up through this listener.
 *	   We will be verifying the policy with this listener which
 *	   leads to the following cases :
 *		- If the packet comes in CLEAR and this listener
 *		  expects CLEAR, we will pass the policy checks here
 *		  without checking with the actual connection. To make
 *		  sure we check with the actual connection we need to
 *		  send the IPSEC_IN up to TCP.
 *		- If the packet comes in CLEAR and this listener
 *		  expects SECURE packets, we will WRONGLY drop the packet.
 *		- If the packet comes in SECURE and this listener expects
 *		  SECURE packets, we will WRONGLY VERIFY the packet that
 *		  could lead to WRONGLY DROPPING the packet. If we pass
 *		  the policy verification here, policy check will again
 *		  happen in TCP with the actual tcp_t which will do the
 *		  right thing.
 *		- If the packet comes in SECURE and this listener
 *		  expects CLEAR, we will WRONGLY DROP the packet.
 *
 *		  Wrongly dropping the packets is okay as having two
 *		  states with the same port cannot lead to anything better.
 *		  Thus we might DROP the packet rather than accepting
 *		  it which is okay as we are erring on the side of
 *		  paranoia. This implies that TCP may not generate RSTs
 *		  under these conditions.
 *
 * Thus packets meant for the detached connection might be verified twice,
 * once here and once in TCP. And in TCP, we don't have to verify policy
 * meant for non-detached connections i.e for eager and listener.
 */
static void
ip_fanout_tcp_listen(queue_t *q, mblk_t *mp, ipha_t *ipha,
    uint32_t ports, uint_t flags, boolean_t mctl_present)
{
	icf_t	*icf;
	ipc_t	*ipc;
	queue_t	*rq;
	int	hdr_len;
	tcph_t	*tcph;
	mblk_t *first_mp;
	ire_t *ire = NULL;
	boolean_t secure;
	boolean_t syn_present = B_FALSE;
	uint16_t	dstport;
	in6_addr_t	v6dst;

	first_mp = mp;
	if (mctl_present) {
		mp = first_mp->b_cont;
		secure = ipsec_in_is_secure(first_mp);
		ASSERT(mp != NULL);
	} else {
		secure = B_FALSE;
	}

	/* If this is a SYN packet attempt to add an IRE_DB_TYPE */
	hdr_len = IPH_HDR_LENGTH(mp->b_rptr);
	tcph = (tcph_t *)&mp->b_rptr[hdr_len];

	if ((flags & IP_FF_SYN_ADDIRE) &&
	    (tcph->th_flags[0] & (TH_SYN|TH_ACK)) == TH_SYN) {
		mblk_t *tmp;

		/* SYN without the ACK - add an IRE for the source */
		ip_ire_append(mp, ipha->ipha_src);
		/*
		 * ip_ire_append linkb's the ire to mp though there
		 * should not be data for a SYN. Do the same here.
		 */
		tmp = mp;
		while (tmp->b_cont != NULL) {
			tmp = tmp->b_cont;
		}
		ire = (ire_t *)tmp->b_rptr;
		syn_present = B_TRUE;
	}

	/* Extract port in net byte order */
	dstport = htons(ntohl(ports) & 0xFFFF);

	/*
	 * We MUST direct all the SYN packets to the right proxy listeners
	 * here because TCP doesn't know how to match them.
	 * Note that the rest of packets are normally picked up by the tcp
	 * conn fanout table.
	 */
	if (flags & IP_FF_PROXY_ONLY) {
		ipc = ip_proxy_match_listener(ipha->ipha_dst, dstport, B_FALSE);
		if (ipc == NULL) {
			freemsg(mp);
		} else {
			ASSERT(ipc->ipc_refcnt != 0);
			rq = ipc->ipc_rq;
			BUMP_MIB(ip_mib.ipInDelivers);
			putnext(rq, mp);
			IPC_REFRELE(ipc);
		}
		return;
	}

	icf = &ipc_tcp_listen_fanout[IP_TCP_LISTEN_HASH(dstport)];
	mutex_enter(&icf->icf_lock);
	ipc = (ipc_t *)&icf->icf_ipc;	/* ipc_hash_next will get first */
	do {
		ipc = ipc->ipc_hash_next;
		if (ipc == NULL) {
			mutex_exit(&icf->icf_lock);
			goto notfound;
		}
	} while (!IP_TCP_LISTEN_MATCH(ipc, dstport, ipha->ipha_dst));
	/* Got a client, up it goes. */
	IPC_REFHOLD(ipc);
	mutex_exit(&icf->icf_lock);
	rq = ipc->ipc_rq;
	goto send;

notfound:
	/*
	 * Check IPv6 listen fanout for in6addr_any.
	 */
	v6dst = ipv6_all_zeros;
	icf = &ipc_tcp_listen_fanout_v6[IP_TCP_LISTEN_HASH_V6(dstport)];
	mutex_enter(&icf->icf_lock);
	ipc = (ipc_t *)&icf->icf_ipc;
	do {
		ipc = ipc->ipc_hash_next;
		if (ipc == NULL) {
			/*
			 * No match on local port. Look for a client
			 * that wants all unclaimed.  Note
			 * that TCP must normally make sure that
			 * there is such a stream, otherwise it
			 * will be tough to get inbound connections
			 * going.
			 */
			mutex_exit(&icf->icf_lock);

			ip_fanout_tcp_defq(q, first_mp,
			    flags | IP_FF_RAWIP, mctl_present);
			return;
		}
	} while (!IP_TCP_LISTEN_MATCH_V6(ipc, dstport, v6dst));
	IPC_REFHOLD(ipc);
	mutex_exit(&icf->icf_lock);
	rq = ipc->ipc_rq;
send :
	if (IPC_INBOUND_POLICY_PRESENT(ipc)) {
		if (!ipsec_check_policy(first_mp, ipc, IPSEC_INBOUND,
		    mctl_present)) {
			/*
			 * POLICY FAILURE : Loggable Event.
			 */
			IPC_REFRELE(ipc);
			ipsec_log_policy_failure(q, IPSEC_POLICY_MISMATCH,
			    "ip_fanout_tcp_listen", ipha, secure);
			freemsg(first_mp);
			return;
		}
	} else if (secure) {
		/*
		 * Don't allow this. Check IPSEC NOTE above
		 * ip_fanout_proto().
		 */
		IPC_REFRELE(ipc);
		ipsec_log_policy_failure(q, IPSEC_POLICY_NOT_NEEDED,
		    "ip_fanout_tcp_listen", ipha, secure);
		freemsg(first_mp);
		return;
	}
	/*
	 * We know we have some policy. Pass the "IPSEC" options size.
	 * TCP uses this adjust the MSS.
	 */
	if (ire != NULL && secure)
		ire->ire_ipsec_options_size = ipsec_extra_length(first_mp);

	if (syn_present && ipc->ipc_in_enforce_policy) {
		mblk_t *policy_mp;

		/*
		 * Attach a policy_mp which comes back later with
		 * accept's  O_T_BIND_REQ for setting policy on
		 * the new endpoint.
		 */
		policy_mp = allocb(sizeof (ipsec_req_t), BPRI_HI);
		if (policy_mp == NULL) {
			IPC_REFRELE(ipc);
			freemsg(first_mp);
			BUMP_MIB(ip_mib.ipInDiscards);
			return;
		}
		policy_mp->b_datap->db_type = IPSEC_POLICY_SET;
		policy_mp->b_wptr += sizeof (ipsec_req_t);
		bcopy(ipc->ipc_inbound_policy, policy_mp->b_rptr,
		    sizeof (ipsec_req_t));
		linkb(first_mp, policy_mp);
	}
	BUMP_MIB(ip_mib.ipInDelivers);
	/*
	 * We are sending the IPSEC_IN message also up. Refer
	 * to comments above this function.
	 */
	putnext(rq, first_mp);
	IPC_REFRELE(ipc);
}

/*
 * Fanout for TCP packets
 * The caller puts <fport, lport> in the ports parameter.
 */
static void
ip_fanout_tcp(queue_t *q, mblk_t *mp, ipha_t *ipha,
    uint32_t ports, uint_t flags, boolean_t mctl_present)
{
	icf_t	*icf;
	ipc_t	*ipc;
	queue_t	*rq;
	mblk_t  *first_mp;
	boolean_t secure;

	first_mp = mp;
	if (mctl_present) {
		mp = first_mp->b_cont;
		secure = ipsec_in_is_secure(first_mp);
		ASSERT(mp != NULL);
	} else {
		secure = B_FALSE;
	}

	/* Find a TCP client stream for this packet. */
	icf = &ipc_tcp_conn_fanout[IP_TCP_CONN_HASH(ipha->ipha_src, ports)];
	mutex_enter(&icf->icf_lock);
	ipc = (ipc_t *)&icf->icf_ipc;	/* ipc_hash_next will get first */
	do {
		ipc = ipc->ipc_hash_next;
		if (ipc == NULL) {
			/*
			 * No hard-bound match.  Look for a
			 * stream bound to the local port only.
			 */
			dblk_t *dp = mp->b_datap;

			mutex_exit(&icf->icf_lock);

			if (dp->db_struioflag & STRUIO_IP) {
				/*
				 * Do the postponed checksum now.
				 */
				mblk_t *mp1;
				ssize_t off = dp->db_struioptr - mp->b_rptr;

				if (IP_CSUM(mp, (uint32_t)off, 0)) {
					ipcsumdbg("swcksumerr1\n", mp);
					BUMP_MIB(ip_mib.tcpInErrs);
					freemsg(mp);
					return;
				}
				mp1 = mp;
				do {
					mp1->b_datap->db_struioflag &=
						~STRUIO_IP;
				} while ((mp1 = mp1->b_cont) != NULL);
			}
			ip_fanout_tcp_listen(q, first_mp, ipha, ports, flags,
			    mctl_present);
			return;
		}
	} while (!IP_TCP_CONN_MATCH(ipc, ipha, ports));
	/* Got a client, up it goes. */
	IPC_REFHOLD(ipc);
	mutex_exit(&icf->icf_lock);
	rq = ipc->ipc_rq;
	if (IPC_INBOUND_POLICY_PRESENT(ipc)) {
		if (!ipsec_check_policy(first_mp, ipc, IPSEC_INBOUND,
		    mctl_present)) {
			/*
			 * POLICY FAILURE : Loggable Event.
			 */
			IPC_REFRELE(ipc);
			ipsec_log_policy_failure(q, IPSEC_POLICY_MISMATCH,
			    "ip_fanout_tcp", ipha, secure);
			freemsg(first_mp);
			return;
		}
	} else if (secure) {
		/*
		 * Don't allow this. Check IPSEC NOTE above
		 * ip_fanout_proto().
		 */
		IPC_REFRELE(ipc);
		ipsec_log_policy_failure(q, IPSEC_POLICY_NOT_NEEDED,
		    "ip_fanout_tcp", ipha, secure);
		freemsg(first_mp);
		return;
	}
	BUMP_MIB(ip_mib.ipInDelivers);
	putnext(rq, mp);
	IPC_REFRELE(ipc);
	if (mctl_present) {
		freeb(first_mp);
	}
}

/*
 * Fanout for UDP packets.
 * The caller puts <fport, lport> in the ports parameter.
 * ire_type must be IRE_BROADCAST for multicast and broadcast packets.
 *
 * If SO_REUSEADDR is set all multicast and broadcast packets
 * will be delivered to all streams bound to the same port.
 */
static void
ip_fanout_udp(queue_t *q, mblk_t *mp, ill_t *ill, ipha_t *ipha,
    uint32_t ports, ushort_t ire_type, uint_t flags, boolean_t mctl_present)
{
	icf_t	*icf;
	ipc_t	*ipc;
	queue_t	*rq;
	uint32_t	dstport, srcport;
	ipaddr_t	dst;
	mblk_t *first_mp;
	boolean_t secure;
	in6_addr_t	v6dst;
	in6_addr_t	v6src;

	first_mp = mp;
	if (mctl_present) {
		mp = first_mp->b_cont;
		secure = ipsec_in_is_secure(first_mp);
		ASSERT(mp != NULL);
	} else {
		secure = B_FALSE;
	}

	/* Extract ports in net byte order */
	dstport = htons(ntohl(ports) & 0xFFFF);
	srcport = htons(ntohl(ports) >> 16);
	dst = ipha->ipha_dst;

	/* Attempt to find a client stream based on destination port. */
	icf = &ipc_udp_fanout[IP_UDP_HASH(dstport)];
	mutex_enter(&icf->icf_lock);
	ipc = (ipc_t *)&icf->icf_ipc;	/* ipc_hash_next will get first */
	if (ire_type != IRE_BROADCAST) {
		/*
		 * Not broadcast or multicast. Send to the one (first)
		 * client we find. No need to check ipc_wantpacket().
		 */
		do {
			ipc = ipc->ipc_hash_next;
			if (ipc == NULL) {
				mutex_exit(&icf->icf_lock);
				goto notfound;
			}
		} while (!IP_UDP_MATCH(ipc, dstport, dst,
		    srcport, ipha->ipha_src));

		/* Found a client */
		IPC_REFHOLD(ipc);
		mutex_exit(&icf->icf_lock);
		rq = ipc->ipc_rq;
		if (canputnext(rq)) {
			if (IPC_INBOUND_POLICY_PRESENT(ipc)) {
				if (!ipsec_check_policy(first_mp, ipc,
				    IPSEC_INBOUND, mctl_present)) {
					/*
					 * POLICY FAILURE : Loggable Event.
					 */
					IPC_REFRELE(ipc);
					ipsec_log_policy_failure(q,
					    IPSEC_POLICY_MISMATCH,
					    "ip_fanout_udp", ipha, secure);
					freemsg(first_mp);
					return;
				}
			} else if (secure) {
				/*
				 * Don't allow this. Check IPSEC NOTE above
				 * ip_fanout_proto().
				 */
				IPC_REFRELE(ipc);
				ipsec_log_policy_failure(q,
				    IPSEC_POLICY_NOT_NEEDED,
				    "ip_fanout_udp", ipha, secure);
				freemsg(first_mp);
				return;
			}
			BUMP_MIB(ip_mib.ipInDelivers);
			putnext(rq, mp);
		} else {
			freemsg(mp);
		}
		IPC_REFRELE(ipc);
		if (mctl_present)
			freeb(first_mp);
		return;
	}

	/*
	 * Broadcast and multicast case
	 * (CLASSD addresses will come in on an IRE_BROADCAST)
	 * Need to check ipc_wantpacket().
	 * If SO_REUSEADDR has been set on the first we send the
	 * packet to all clients that have joined the group and
	 * match the port.
	 */
	do {
		ipc = ipc->ipc_hash_next;
		if (ipc == NULL) {
			mutex_exit(&icf->icf_lock);
			goto notfound;
		}
	} while (!(IP_UDP_MATCH(ipc, dstport, dst,
	    srcport, ipha->ipha_src) && ipc_wantpacket(ipc, ill, dst)));

	if (ipc->ipc_reuseaddr) {
		ipc_t		*first_ipc = ipc;
		ipc_t		*next_ipc;
		mblk_t		*mp1;
		ipaddr_t	src = ipha->ipha_src;

		IPC_REFHOLD(ipc);
		ipc = ipc->ipc_hash_next;
		for (;;) {
			while (ipc != NULL) {
				if (IP_UDP_MATCH(ipc, dstport, dst,
				    srcport, src) &&
				    ipc_wantpacket(ipc, ill, dst))
					break;
				ipc = ipc->ipc_hash_next;
			}
			/*
			 * Just copy the data part alone. The mctl part is
			 * needed just for verifying policy and it is never
			 * sent up.
			 */
			if (ipc == NULL || (((mp1 = dupmsg(mp)) == NULL) &&
			    ((mp1 = copymsg(mp)) == NULL))) {
				/*
				 * No more intested clients or memory
				 * allocation failed
				 */
				ipc = first_ipc;
				break;
			}
			IPC_REFHOLD(ipc);
			mutex_exit(&icf->icf_lock);
			rq = ipc->ipc_rq;
			if (!canputnext(rq)) {
				BUMP_MIB(ip_mib.udpInOverflows);
				freemsg(mp1);
			} else {
				if (IPC_INBOUND_POLICY_PRESENT(ipc)) {
					if (!ipsec_check_policy(first_mp,
					    ipc, IPSEC_INBOUND, mctl_present)) {
						/*
						 * POLICY FAILURE : Loggable
						 * Event.
						 */
						ipsec_log_policy_failure(q,
						    IPSEC_POLICY_MISMATCH,
						    "ip_fanout_udp", ipha,
						    secure);
						freemsg(mp1);
					} else {
						BUMP_MIB(ip_mib.ipInDelivers);
						putnext(rq, mp1);
					}
				} else if (secure) {
					/*
					 * Don't allow this. Check IPSEC NOTE
					 * above ip_fanout_proto().
					 */
					ipsec_log_policy_failure(q,
					    IPSEC_POLICY_NOT_NEEDED,
					    "ip_fanout_udp", ipha, secure);
					freemsg(mp1);
				} else {
					BUMP_MIB(ip_mib.ipInDelivers);
					putnext(rq, mp1);
				}
			}
			mutex_enter(&icf->icf_lock);
			/* Follow the next pointer before releasing the ipc. */
			next_ipc = ipc->ipc_hash_next;
			IPC_REFRELE(ipc);
			ipc = next_ipc;
		}
	} else {
		IPC_REFHOLD(ipc);
	}

	/* Last one.  Send it upstream. */
	mutex_exit(&icf->icf_lock);
	rq = ipc->ipc_rq;
	if (!canputnext(rq)) {
		BUMP_MIB(ip_mib.udpInOverflows);
		freemsg(mp);
	} else {
		if (IPC_INBOUND_POLICY_PRESENT(ipc)) {
			if (!ipsec_check_policy(first_mp, ipc, IPSEC_INBOUND,
			    mctl_present)) {
				/*
				 * POLICY FAILURE : Loggable Event.
				 */
				IPC_REFRELE(ipc);
				ipsec_log_policy_failure(q,
				    IPSEC_POLICY_MISMATCH,
				    "ip_fanout_udp", ipha, secure);
				freemsg(first_mp);
				return;
			}
		} else if (secure) {
			IPC_REFRELE(ipc);
			ipsec_log_policy_failure(q, IPSEC_POLICY_NOT_NEEDED,
			    "ip_fanout_udp", ipha, secure);
			freemsg(first_mp);
			return;
		}
		BUMP_MIB(ip_mib.ipInDelivers);
		putnext(rq, mp);
	}
	IPC_REFRELE(ipc);
	if (mctl_present) {
		freeb(first_mp);
	}
	return;

notfound:
	/*
	 * Check for IPv6 in6addr_any receiver
	 */
	v6dst = ipv6_all_zeros;
	IN6_IPADDR_TO_V4MAPPED(ipha->ipha_src, &v6src);
	icf = &ipc_udp_fanout_v6[IP_UDP_HASH(dstport)];
	mutex_enter(&icf->icf_lock);
	ipc = (ipc_t *)&icf->icf_ipc;
	do {
		ipc = ipc->ipc_hash_next;
		if (ipc == NULL) {
			break;
		}
	} while (!IP_UDP_MATCH_V6(ipc, dstport, v6dst, srcport, v6src));
	if (ipc == NULL) {
		/*
		 * No one bound to this port.  Is
		 * there a client that wants all
		 * unclaimed datagrams?
		 */
		mutex_exit(&icf->icf_lock);

		if (ipc_proto_fanout[IPPROTO_UDP].icf_ipc != NULL) {
			ip_fanout_proto(q, first_mp, ill, ipha,
			    flags | IP_FF_RAWIP, mctl_present);
		} else {
			ip_fanout_send_icmp(q, first_mp, flags,
			    &ip_mib.udpNoPorts,
			    ICMP_DEST_UNREACHABLE,
			    ICMP_PORT_UNREACHABLE,
			    mctl_present);
		}
		return;
	}
	IPC_REFHOLD(ipc);
	mutex_exit(&icf->icf_lock);
	rq = ipc->ipc_rq;
	if (!canputnext(rq)) {
		BUMP_MIB(ip_mib.udpInOverflows);
		freemsg(mp);
	} else {
		BUMP_MIB(ip_mib.ipInDelivers);
		putnext(rq, mp);
	}
	IPC_REFRELE(ipc);
	if (mctl_present) {
		freeb(first_mp);
	}
}

/*
 * Complete the ip_wput header so that it
 * is possible to generate ICMP
 * errors.
 */
int
ip_hdr_complete(ipha_t *ipha)
{
	ire_t *ire;

	if (ipha->ipha_src == INADDR_ANY) {
		ire = ire_lookup_local();
		if (ire == NULL) {
			ip1dbg(("ip_hdr_complete: no source IRE\n"));
			return (1);
		}
		ipha->ipha_src = ire->ire_addr;
		ire_refrele(ire);
	}
	ipha->ipha_ttl = ip_def_ttl;
	ipha->ipha_hdr_checksum = 0;
	ipha->ipha_hdr_checksum = ip_csum_hdr(ipha);
	return (0);
}

/*
 * Nobody should be sending
 * packets up this stream
 */
static void
ip_lrput(queue_t *q, mblk_t *mp)
{
	mblk_t *mp1;

	switch (mp->b_datap->db_type) {
	case M_FLUSH:
		/* Turn around */
		if (*mp->b_rptr & FLUSHW) {
			*mp->b_rptr &= ~FLUSHR;
			qreply(q, mp);
			return;
		}
		break;
	}
	/* Could receive messages that passed through ar_rput */
	for (mp1 = mp; mp1; mp1 = mp1->b_cont)
		mp1->b_prev = mp1->b_next = NULL;
	freemsg(mp);
}

/* Nobody should be sending packets down this stream */
/* ARGSUSED */
static void
ip_lwput(queue_t *q, mblk_t *mp)
{
	freemsg(mp);
}

/*
 * Move the first hop in any source route to ipha_dst and remove that part of
 * the source route.  Called by other protocols.  Errors in option formatting
 * are ignored - will be handled by ip_wput_options Return the final
 * destination (either ipha_dst or the last entry in a source route.)
 */
ipaddr_t
ip_massage_options(ipha_t *ipha)
{
	uint32_t	totallen;
	uchar_t		*opt;
	uint32_t	optval;
	uint32_t	optlen;
	ipaddr_t	dst;
	int		i;
	ire_t		*ire;

	ip2dbg(("ip_massage_options\n"));
	dst = ipha->ipha_dst;
	totallen = ipha->ipha_version_and_hdr_length -
		(uint8_t)((IP_VERSION << 4) + IP_SIMPLE_HDR_LENGTH_IN_WORDS);
	opt = (uchar_t *)&ipha[1];
	totallen <<= 2;
	while (totallen != 0) {
		switch (optval = opt[IPOPT_POS_VAL]) {
		case IPOPT_EOL:
			return (dst);
		case IPOPT_NOP:
			optlen = 1;
			break;
		default:
			optlen = opt[IPOPT_POS_LEN];
		}
		if (optlen == 0 || optlen > totallen)
			return (dst);

		switch (optval) {
			uint32_t off;
		case IPOPT_SSRR:
		case IPOPT_LSRR:
			off = opt[IPOPT_POS_OFF];
			if (off < IPOPT_MINOFF_SR) {
				ip1dbg(("ip_massage_options: bad option offset "
					"%d\n", off));
				return (dst);
			}
			off--;
		redo_srr:
			if (optlen < IP_ADDR_LEN ||
			    off > optlen - IP_ADDR_LEN) {
				/* End of source route */
				ip1dbg(("ip_massage_options: end of SR\n"));
				break;
			}
			bcopy((char *)opt + off, &dst, IP_ADDR_LEN);
			ip1dbg(("ip_massage_options: next hop 0x%x\n",
			    ntohl(dst)));
			/*
			 * Check if our address is present more than
			 * once as consecutive hops in source route.
			 * XXX verify per-interface ip_forwarding
			 * for source route?
			 */
			ire = ire_ctable_lookup(dst, 0, IRE_LOCAL, NULL,
			    NULL, MATCH_IRE_TYPE);
			if (ire != NULL) {
				ire_refrele(ire);
				off += IP_ADDR_LEN;
				goto redo_srr;
			}
			if (dst == htonl(INADDR_LOOPBACK)) {
				ip1dbg(("ip_massage_options: loopback addr in "
				    "source route!\n"));
				break;
			}
			/*
			 * Update ipha_dst to be the first hop and remove the
			 * first hop from the source route (by overwriting
			 * part of the option with NOP options).
			 */
			ipha->ipha_dst = dst;
			/* Put the last entry in dst */
			off = ((optlen - IP_ADDR_LEN - 3) & ~(IP_ADDR_LEN-1)) +
			    3;
			bcopy(&opt[off], &dst, IP_ADDR_LEN);

			ip1dbg(("ip_massage_options: last hop 0x%x\n",
			    ntohl(dst)));
			/* Move down and overwrite */
			opt[IP_ADDR_LEN] = opt[0];
			opt[IP_ADDR_LEN+1] = opt[IPOPT_POS_LEN] - IP_ADDR_LEN;
			opt[IP_ADDR_LEN+2] = opt[IPOPT_POS_OFF];
			for (i = 0; i < IP_ADDR_LEN; i++)
				opt[i] = IPOPT_NOP;
			break;
		}
		totallen -= optlen;
		opt += optlen;
	}
	return (dst);
}

/*
 * Return the network mask
 * associated with the specified address.
 */
ipaddr_t
ip_net_mask(ipaddr_t addr)
{
	uchar_t	*up = (uchar_t *)&addr;
	ipaddr_t mask = 0;
	uchar_t	*maskp = (uchar_t *)&mask;

#ifdef i386
#define	TOTALLY_BRAIN_DAMAGED_C_COMPILER
#endif
#ifdef  TOTALLY_BRAIN_DAMAGED_C_COMPILER
	maskp[0] = maskp[1] = maskp[2] = maskp[3] = 0;
#endif
	if (CLASSD(addr)) {
		maskp[0] = 0xF0;
		return (mask);
	}
	if (addr == 0)
		return (0);
	maskp[0] = 0xFF;
	if ((up[0] & 0x80) == 0)
		return (mask);

	maskp[1] = 0xFF;
	if ((up[0] & 0xC0) == 0x80)
		return (mask);

	maskp[2] = 0xFF;
	if ((up[0] & 0xE0) == 0xC0)
		return (mask);

	/* Must be experimental or multicast, indicate as much */
	return ((ipaddr_t)0);
}

static ipif_t *
ip_newroute_get_src_ipif(
    ipif_t *dst_ipif,
    boolean_t islocal,
    ipaddr_t src_addr)
{
	ipif_t *src_ipif;

	if (ip_enable_group_ifs == 0 || dst_ipif->ipif_ifgrpnext == dst_ipif)
		return (dst_ipif);
	if (src_addr != INADDR_ANY && islocal) {
		/*
		 * We already have a source address, and the packet
		 * originated here.
		 *
		 * Perform the following sets of reality checks:
		 *	- Find an ipif that is up for this source
		 *	  address.
		 *	- If it is the same ipif as for the route,
		 *	  cool, return dst_ipif.
		 *	  (Except when instructed to do otherwise.)
		 *	- If the ipif is not in the same ifgrp as
		 *	  for the route, return dst_ipif
		 *	  because the request source
		 *	  address doesn't seem to even come CLOSE
		 *	  to what routing says.
		 *	- If the ipif is in the same ifgrp but not
		 *	  the same ipif as the ire, set src_ipif to
		 *	  this ipif.  Most likely, this source
		 *	  address was set by bind() in user space or
		 *	  by a call to ifgrp_scheduler() in
		 *	  ip_bind() or ip_ire_req() because of TCP
		 *	  source address selection.
		 *
		 * REMEMBER, if there is no ipif for the source
		 * address, then the packet is bogus.  The islocal
		 * ensures that this is not a forwarded packet.
		 */
		ire_t *src_ire;

		src_ire = ire_ctable_lookup(src_addr, 0,
		    IRE_LOCAL, NULL, NULL, MATCH_IRE_TYPE);
		if (src_ire == NULL) {
			char abuf[30];

			/*
			 * Locally-originated packet with source
			 * address that's not attached to an up
			 * interface.  Possibly a deliberately forged
			 * IP datagram.
			 */
			ip1dbg(("Packet from me with non-up src!\n"));
			ip1dbg(("Address is %s.\n",
			    ip_dot_addr(src_addr, abuf)));
			src_ipif = ifgrp_scheduler(dst_ipif);
		} else {
			src_ipif = src_ire->ire_ipif;
			ASSERT(src_ipif != NULL);
			ire_refrele(src_ire);
		}

		if (src_ipif == dst_ipif ||
		    src_ipif->ipif_net_mask != dst_ipif->ipif_net_mask ||
		    src_ipif->ipif_subnet != dst_ipif->ipif_subnet) {
			/*
			 * Returning dst_ipif means to just
			 * use the ire obtained by the initial
			 * ire_ftable_lookup.  We do this if the source
			 * address matches the ire's source address,
			 * or if the ire's outbound interface is not
			 * in the same ifgrp as the source address.
			 * (There is a possibility of multiple
			 * prefixes on the same interface, or
			 * interface group, but we punt on that for
			 * now.)
			 *
			 * We perform that last reality check by
			 * checking prefixes.
			 */
			return (dst_ipif);
		}

		/*
		 * If I reach here without explicitly scheduling
		 * src_ipif or returning dst_ipif, then the
		 * source address ipif is in the same interface
		 * group as the ire's ipif, but it is not the
		 * same actual ipif.  So basically fallthrough
		 * with src_ipif set to what the source address
		 * says.  This means the new route will go out
		 * the interface assigned to the (probably
		 * user-specified) source address.  This may
		 * upset the balance.
		 */
	} else {
		/* No specified source address or forwarded packet. */
		src_ipif = ifgrp_scheduler(dst_ipif);
	}

	/*
	 * If the new source ipif isn't the same type as the dest, I can't
	 * send packets out the other interface in the group, because of
	 * potential link-level header differences, and a bunch of other
	 * cruft.
	 */
	if (dst_ipif->ipif_type != src_ipif->ipif_type)
		return (dst_ipif);

	return (src_ipif);
}

/*
 * IPv4 -
 * ip_newroute is called by ip_rput or ip_wput whenever we need to send
 * out a packet to a destination address for which we do not have specific
 * routing information.
 *
 * NOTE : These are the scopes of some of the variables that point at IRE,
 *	  which needs to be followed while making any future modifications
 *	  to avoid memory leaks.
 *
 *	- ire and sire are the entries looked up initially by
 *	  ire_ftable_lookup.
 *	- ipif_ire is used to hold the interface ire associated with
 *	  the new cache ire. But it's scope is limited, so we always REFRELE
 *	  it before branching out to error paths.
 *	- save_ire is initialized before ire_create, so that ire returned
 *	  by ire_create will not over-write the ire. We REFRELE save_ire
 *	  before breaking out of the switch.
 *
 *	Thus on failures, we have to REFRELE only ire and sire, if they
 *	are not NULL.
 */
static void
ip_newroute(queue_t *q, mblk_t *mp, ipaddr_t dst, ipc_t *ipc)
{
	areq_t	*areq;
	ipaddr_t gw = 0;
	ire_t	*ire;
	mblk_t	*res_mp;
	queue_t *stq;
	ipaddr_t *addrp;
	ipif_t  *src_ipif;
	ipha_t  *ipha;
	ire_t	*sire = NULL;
	mblk_t	*first_mp;
	ire_t	*save_ire;
	boolean_t	natural_if_route = B_FALSE;

	if (ip_debug > 2) {
		/* ip1dbg */
		pr_addr_dbg("ip_newroute: dst %s\n", AF_INET, &dst);
	}
	first_mp = mp;
	if (mp->b_datap->db_type == M_CTL) {
		mp = mp->b_cont;
	}
	ipha = (ipha_t *)mp->b_rptr;

	/* All multicast lookups come through ip_newroute_ipif() */
	if (CLASSD(dst)) {
		ip0dbg(("ip_newroute: CLASSD 0x%x (b_prev %p, b_next %p)\n",
		    ntohl(dst), (void *)mp->b_prev, (void *)mp->b_next));
		freemsg(first_mp);
		return;
	}
	/*
	 * Get what we can from ire_ftable_lookup which will follow an IRE
	 * chain until it gets the most specific information available.
	 * For example, we know that there is no IRE_CACHE for this dest,
	 * but there may be an IRE_OFFSUBNET which specifies a gateway.
	 * ire_ftable_lookup will look up the gateway, etc.
	 */
	ire = ire_ftable_lookup(dst, 0, 0, 0, NULL, &sire, NULL, 0,
	    (MATCH_IRE_RECURSIVE | MATCH_IRE_DEFAULT | MATCH_IRE_RJ_BHOLE));
	if (ire == NULL) {
		ip_rts_change(RTM_MISS, dst, 0, 0, 0, 0, 0, 0, RTA_DST);
		goto icmp_err_ret;
	}
	/*
	 * Verify that the returned IRE does not have either the RTF_REJECT or
	 * RTF_BLACKHOLE flags set and that the IRE is either an IRE_CACHE,
	 * IRE_IF_NORESOLVER or IRE_IF_RESOLVER.
	 */
	if ((ire->ire_flags & (RTF_REJECT | RTF_BLACKHOLE)) ||
	    (ire->ire_type & (IRE_CACHE | IRE_INTERFACE)) == 0) {
		goto icmp_err_ret;
	}


	/*
	 * Increment the ire_ob_pkt_count field for ire if it is an INTERFACE
	 * (IF_RESOLVER or IF_NORESOLVER) IRE type, and increment the same for
	 * the parent IRE, sire, if it is some sort of prefix IRE (which
	 * includes DEFAULT, PREFIX, HOST and HOST_REDIRECT).
	 */
	if ((ire->ire_type & IRE_INTERFACE) != 0)
		ire->ire_ob_pkt_count++;
	if (sire != NULL) {
		gw = sire->ire_gateway_addr;
		ASSERT((sire->ire_type & (IRE_CACHETABLE |
		    IRE_INTERFACE)) == 0);
		sire->ire_ob_pkt_count++;
	}

	if (ire->ire_type & IRE_INTERFACE) {
		ire_t 	*ipif_ire;

		ipif_ire = ipif_to_ire(ire->ire_ipif);
		if (ipif_ire != NULL) {
			if (ipif_ire == ire) {
				/*
				 * 'ire' is the natural inteface route
				 * associated with ire->ire_ipif. Record it.
				 */
				natural_if_route = B_TRUE;
			}
			ire_refrele(ipif_ire);
		}
	}
	if (ire->ire_type == IRE_CACHE || natural_if_route) {
		/*
		 * If the interface belongs to an interface group, make sure
		 * the next possible interface in the group is used.  This
		 * encourages load-balancing among peers in an interface group.
		 * Furthermore if a user has previously bound to a source
		 * address, try and use that interface if it makes good
		 * routing sense.
		 *
		 * Interface scheduling is only done in the IRE_CACHE case
		 * (where the IRE_CACHE is for the next-hop of the destination
		 * being looked up) or in the IRE_INTERFACE case when the IRE
		 * represents the interface route for ire->ire_ipif itself.  In
		 * general for manually added interface routes, the latter case
		 * does not hold true so interface scheduling is not done.
		 * This means that manually added interface routes can
		 * be used to selectively disable ifgrp scheduling.
		 *
		 * Note: While we pick a src_ipif we are really only interested
		 * in the ill for load balancing. The source ipif is
		 * determined by source address selection below.
		 *
		 * Remember kids, mp->b_prev is the indicator of local
		 * origination!
		 */
		src_ipif = ip_newroute_get_src_ipif(ire->ire_ipif,
		    (mp->b_prev == NULL), ipha->ipha_src);

	} else {
		src_ipif = ire->ire_ipif;
	}
	ASSERT(src_ipif != NULL);
	/*
	 * Pick a source address preferring non-deprecated ones.
	 */
	if (src_ipif->ipif_flags & IFF_DEPRECATED) {
		src_ipif = ipif_select_source(src_ipif->ipif_ill, dst);
		if (src_ipif == NULL) {
			if (ip_debug > 2) {
				/* ip1dbg */
				pr_addr_dbg("ip_newroute: no src for dst %s\n",
				    AF_INET, &dst);
				ip1dbg(("ip_newroute: no src for"
				    " dst through interface %s\n",
				    ire->ire_ipif->ipif_ill->ill_name));
			}
			goto icmp_err_ret;
		}
	}
	if (natural_if_route && (src_ipif != ire->ire_ipif)) {
		/*
		 * If 'ire' is a natural interface ire, and if we have
		 * chosen a new src_ipif, replace 'ire' with the natural
		 * interface ire associated with the new src_ipif, if it
		 * exists.
		 */
		ire_t	*ipif_ire;

		ipif_ire = ipif_to_ire(src_ipif);
		if (ipif_ire != NULL) {
			ire_refrele(ire);
			ire = ipif_ire;
		}
	}
	/*
	 * If ire_ftable_lookup() returned an interface ire, then at this
	 * point, 'ire' holds the right interface ire that will be tied
	 * to the new cache ire via the ihandle
	 *
	 * Assign a source address while we have the ipc.
	 * We can't have ip_wput_ire pick a source address when the
	 * packet returns from arp since ipc_unspec_src might be set
	 * and we loose the ipc when going through arp.
	 */
	if (ipha->ipha_src == INADDR_ANY &&
	    (ipc == NULL || !ipc->ipc_unspec_src)) {
		ipha->ipha_src = ire->ire_src_addr;
	}
	if (ip_debug > 3) {
		/* ip2dbg */
		pr_addr_dbg("ip_newroute: first hop %s\n", AF_INET, &gw);
	}
	ip2dbg(("\tire type %s (%d)\n",
		ip_nv_lookup(ire_nv_tbl, ire->ire_type), ire->ire_type));

	stq = ire->ire_stq;

	/*
	 * At this point in ip_newroute(), ire is either the IRE_CACHE of the
	 * next-hop gateway for an off-subnet destination or an IRE_INTERFACE
	 * type that should be used to resolve an on-subnet destination or
	 * an on-subnet next-hop gateway.
	 *
	 * In the IRE_CACHE case, src_ipif will point to the outgoing ipif to be
	 * used for this destination (as returned by ire->ire_ipif and possibly
	 * modified by interface group scheduling).  The IRE sire will point to
	 * the prefix that is the longest matching route for the destination.
	 * These prefix types include IRE_DEFAULT, IRE_PREFIX, IRE_HOST, and
	 * IRE_HOST_REDIRECT.  The newly created IRE_CACHE entry for the
	 * off-subnet destination is tied to both the prefix route and the
	 * interface route used to resolve the next-hop gateway via the
	 * ire_phandle and ire_ihandle fields, respectively.
	 *
	 * In the IRE_INTERFACE case, sire may or may not be NULL but the
	 * IRE_CACHE that is to be created will only be tied to the
	 * IRE_INTERFACE it was derived from via the ire_ihandle field.
	 * The IRE_INTERFACE used may vary if interface groups are enabled and
	 * if this IRE represents the interface itself (namely, if
	 * ipif_to_ire(ire->ipif) is the same as ire).
	 */
	save_ire = ire;
	switch (ire->ire_type) {
	case IRE_CACHE: {
		ire_t	*ipif_ire;
		mblk_t	*ire_fp_mp;

		ASSERT(sire != NULL);
		if (gw == 0)
			gw = ire->ire_gateway_addr;
		/*
		 * We need 3 ire's to create a new cache ire for an off-link
		 * destn. from the cache ire of the gateway.
		 *	1. The prefix ire 'sire'
		 *	2. The interface ire 'ipif_ire'
		 *	3. The cache ire of the gateway 'ire'
		 * The cache ire that is to be created needs be tied to
		 * an appropriate interface ire via the ihandle. The algorithm
		 * employed below is as follows.
		 * 1. Use the natural interface ire associated with the new
		 *    src_ipif, if it exists.
		 * 2. Otherwise use the natural interface ire associated with
		 *    ire->ire_ipif if it exists.
		 * 3. Otherwise use the interface ire, corresponding to
		 *    ire->ire_ihandle, if it exists. We can hit this case if
		 *    someone has manually deleted the natural interface route,
		 *    and added a host-specific interface route to the
		 *    gateway.
		 * 4. Otherwise, there is no interface route to the gateway.
		 *    This is a race condition, where we found the cache
		 *    but the inteface route has been deleted.
		 */
		ipif_ire = ipif_to_ire(src_ipif);
		if (ipif_ire == NULL) {
			ipif_ire = ipif_to_ire(ire->ire_ipif);
			if (ipif_ire == NULL) {
				ipif_ire =
				    ire_ihandle_lookup_offlink(ire, sire);
				if (ipif_ire == NULL) {
					ip0dbg(("ip_newroute: "
					    "ire_ihandle_lookup_offlink "
					    "failed\n"));
					goto icmp_err_ret;
				}
			}
		}
		/*
		 * If ifgrp scheduling has picked a new ill, zero out the
		 * fastpath mp which will have to be rebuilt.
		 */
		if (src_ipif->ipif_wq != ire->ire_stq)
			ire_fp_mp = NULL;
		else
			ire_fp_mp = ire->ire_fp_mp;
		/*
		 * Assume DL_UNITDATA_REQ is same for all physical interfaces
		 * in the ifgrp.  If it isn't, this code will
		 * have to be seriously rewhacked to allow the
		 * fastpath probing (such that I cache the link
		 * header in the IRE_CACHE) to work over ifgrps.
		 */
		ire = ire_create(
		    (uchar_t *)&dst,			 /* dest address */
		    (uchar_t *)&ip_g_all_ones,		 /* mask */
		    (uchar_t *)&src_ipif->ipif_src_addr, /* source address */
		    (uchar_t *)&gw,			 /* gateway address */
		    save_ire->ire_max_frag,
		    ire_fp_mp,				 /* Fast Path header */
		    src_ipif->ipif_rq,			 /* recv-from queue */
		    src_ipif->ipif_wq,			 /* send-to queue */
		    IRE_CACHE,				 /* IRE type */
		    save_ire->ire_dlureq_mp,
		    src_ipif,
		    sire->ire_mask,			/* Parent mask */
		    sire->ire_phandle,			/* Parent handle */
		    ipif_ire->ire_ihandle,		/* Interface handle */
		    0,					/* flags if any */
		    &(sire->ire_uinfo));

		ire_refrele(save_ire);
		if (ire == NULL) {
			ire_refrele(ipif_ire);
			break;
		}

		/*
		 * Prevent sire and ipif_ire from getting deleted. The
		 * newly created ire is tied to both of them via the phandle
		 * and ihandle respectively.
		 */
		IRB_REFHOLD(sire->ire_bucket);
		/* Has it been removed already ? */
		if (sire->ire_marks & IRE_MARK_CONDEMNED) {
			IRB_REFRELE(sire->ire_bucket);
			ire_refrele(ipif_ire);
			break;
		}

		IRB_REFHOLD(ipif_ire->ire_bucket);
		/* Has it been removed already ? */
		if (ipif_ire->ire_marks & IRE_MARK_CONDEMNED) {
			IRB_REFRELE(ipif_ire->ire_bucket);
			IRB_REFRELE(sire->ire_bucket);
			ire_refrele(ipif_ire);
			break;
		}
		/* Remember the packet we want to xmit */
		ire->ire_mp->b_cont = first_mp;
		ire_add_then_send(q, ire->ire_mp);

		/* Assert that sire is not deleted yet. */
		ASSERT(sire->ire_ptpn != NULL);
		IRB_REFRELE(sire->ire_bucket);
		ire_refrele(sire);

		/* Assert that ipif_ire is not deleted yet. */
		ASSERT(ipif_ire->ire_ptpn != NULL);
		IRB_REFRELE(ipif_ire->ire_bucket);
		ire_refrele(ipif_ire);
		return;
	}
	case IRE_IF_NORESOLVER: {
		/* We have what we need to build an IRE_CACHE. */
		ill_t	*ill;
		mblk_t	*dlureq_mp;

		/*
		 * Create a new dlureq_mp with the
		 * IP gateway address in destination address in the DLPI hdr
		 * if the physical length is exactly 4 bytes.
		 */
		ill = ire_to_ill(ire);
		if (ill == NULL) {
			ip0dbg(("ip_newroute: ire_to_ill failed\n"));
			break;
		}
		if (ill->ill_phys_addr_length == IP_ADDR_LEN) {
			uchar_t *addr;

			if (gw)
				addr = (uchar_t *)&gw;
			else
				addr = (uchar_t *)&dst;

			dlureq_mp = ill_dlur_gen(addr,
			    ill->ill_phys_addr_length, ill->ill_sap,
			    ill->ill_sap_length);
		} else {
			dlureq_mp = ire->ire_dlureq_mp;
		}

		if (dlureq_mp == NULL) {
			ip1dbg(("ip_newroute: dlureq_mp NULL\n"));
			break;
		}

		ire = ire_create(
		    (uchar_t *)&dst,			 /* dest address */
		    (uchar_t *)&ip_g_all_ones,		 /* mask */
		    (uchar_t *)&src_ipif->ipif_src_addr, /* source address */
		    (uchar_t *)&gw,			 /* gateway address */
		    save_ire->ire_max_frag,
		    NULL,				 /* Fast Path header */
		    save_ire->ire_rfq,			 /* recv-from queue */
		    stq,				 /* send-to queue */
		    IRE_CACHE,
		    dlureq_mp,
		    save_ire->ire_ipif,
		    save_ire->ire_mask,			 /* Parent mask */
		    0,
		    save_ire->ire_ihandle,		 /* Interface handle */
		    0,					 /* flags if any */
		    &(save_ire->ire_uinfo));

		if (ill->ill_phys_addr_length == IP_ADDR_LEN)
			freeb(dlureq_mp);

		if (ire == NULL) {
			ire_refrele(save_ire);
			break;
		}

		/* Prevent save_ire from getting deleted */
		IRB_REFHOLD(save_ire->ire_bucket);
		/* Has it been removed already ? */
		if (save_ire->ire_marks & IRE_MARK_CONDEMNED) {
			IRB_REFRELE(save_ire->ire_bucket);
			ire_refrele(save_ire);
			break;
		}

		/* Don't need sire anymore */
		if (sire != NULL)
			ire_refrele(sire);

		/* Remember the packet we want to xmit */
		ire->ire_mp->b_cont = first_mp;
		ire_add_then_send(q, ire->ire_mp);

		/* Assert that it is not deleted yet. */
		ASSERT(save_ire->ire_ptpn != NULL);
		IRB_REFRELE(save_ire->ire_bucket);
		ire_refrele(save_ire);
		return;
	}
	case IRE_IF_RESOLVER:
		/*
		 * We can't build an IRE_CACHE yet, but at least we found a
		 * resolver that can help.
		 */
		res_mp = ire->ire_dlureq_mp;
		if (!stq || !OK_RESOLVER_MP(res_mp)) {
			break;
		}
		/*
		 * To be at this point in the code with a non-zero gw means
		 * that dst is reachable through a gateway that we have never
		 * resolved.  By changing dst to the gw addr we resolve the
		 * gateway first.  When ire_add_then_send() tries to put the IP
		 * dg to dst, it will reenter ip_newroute() at which time we
		 * will find the IRE_CACHE for the gw and create another
		 * IRE_CACHE in case IRE_CACHE above.
		 */
		if (gw) {
			dst = gw;
			gw = 0;
		}
		/* NOTE: a resolvers rfq is NULL and its stq points upstream. */
		/*
		 * We obtain a partial IRE_CACHE which we will pass along with
		 * the resolver query.  When the response comes back it will be
		 * there ready for us to add.
		 */
		ire = ire_create(
		    (uchar_t *)&dst,			 /* dest address */
		    (uchar_t *)&ip_g_all_ones,		 /* mask */
		    (uchar_t *)&src_ipif->ipif_src_addr, /* source address */
		    (uchar_t *)&gw,			 /* gateway address */
		    save_ire->ire_max_frag,
		    NULL,				 /* Fast Path header */
		    stq,				 /* recv-from queue */
		    OTHERQ(stq),			 /* send-to queue */
		    IRE_CACHE,
		    res_mp,
		    save_ire->ire_ipif,
		    save_ire->ire_mask,			 /* Parent mask */
		    0,
		    save_ire->ire_ihandle,		 /* Interface handle */
		    0,					 /* flags if any */
		    &(save_ire->ire_uinfo));

		ire_refrele(save_ire);
		if (ire == NULL)
			break;
		if (sire != NULL)
			ire_refrele(sire);

		/*
		 * Construct message chain for the resolver of the form:
		 * 	ARP_REQ_MBLK-->IRE_MBLK-->Packet
		 * Packet could contain a IPSEC_OUT mp.
		 *
		 * NOTE : ire will be added later when the response comes
		 * back from ARP. If the response does not come back, ARP
		 * frees the packet. For this reason, we can't REFHOLD
		 * the bucket of save_ire to prevent deletions. We may not
		 * be able to REFRELE the bucket if the response never comes
		 * back. Thus, before adding the ire, ire_add_v4 will make
		 * sure that the interface route does not get deleted. This
		 * is the only case unlike ip_newroute_v6,
		 * ip_newroute_multi_v6 where we can always prevent deletions
		 * because of the synchronous nature of adding IRES i.e
		 * ire_add_then_send is called after creating the IRE.
		 */
		ire->ire_mp->b_cont = first_mp;
		mp = ire->ire_dlureq_mp;
		ASSERT(mp != NULL);
		ire->ire_dlureq_mp = NULL;
		linkb(mp, ire->ire_mp);

		/*
		 * Fill in the source and dest addrs for the resolver.
		 * NOTE: this depends on memory layouts imposed by ill_init().
		 */
		areq = (areq_t *)mp->b_rptr;
		addrp = (ipaddr_t *)((char *)areq +
		    areq->areq_sender_addr_offset);
		*addrp = ire->ire_src_addr;
		addrp = (ipaddr_t *)((char *)areq +
		    areq->areq_target_addr_offset);
		*addrp = dst;
		/* Up to the resolver. */
		putnext(stq, mp);
		/*
		 * The response will come back in ip_wput with db_type
		 * IRE_DB_TYPE.
		 */
		return;
	default:
		break;
	}

	ip1dbg(("ip_newroute: dropped\n"));
	/* Did this packet originate externally? */
	if (mp->b_prev) {
		mp->b_next = NULL;
		mp->b_prev = NULL;
		BUMP_MIB(ip_mib.ipInDiscards);
	} else {
		BUMP_MIB(ip_mib.ipOutDiscards);
	}
	freemsg(first_mp);
	if (ire != NULL)
		ire_refrele(ire);
	if (sire != NULL)
		ire_refrele(sire);
	return;

icmp_err_ret:
	ip1dbg(("ip_newroute: no route\n"));
	if (sire != NULL)
		ire_refrele(sire);
	/* Did this packet originate externally? */
	if (mp->b_prev) {
		mp->b_next = NULL;
		mp->b_prev = NULL;
		/* XXX ipInNoRoutes */
		q = WR(q);
	} else {
		/*
		 * Since ip_wput() isn't close to finished, we fill
		 * in enough of the header for credible error reporting.
		 */
		if (ip_hdr_complete(ipha)) {
			/* Failed */
			freemsg(first_mp);
			if (ire != NULL)
				ire_refrele(ire);
			return;
		}
	}
	BUMP_MIB(ip_mib.ipOutNoRoutes);

	/*
	 * At this point we will have ire only if RTF_BLACKHOLE
	 * or RTF_REJECT flags are set on the IRE. It will not
	 * generate ICMP_HOST_UNREACHABLE if RTF_BLACKHOLE is set.
	 */
	if (ire != NULL) {
		if (ire->ire_flags & RTF_BLACKHOLE) {
			ire_refrele(ire);
			freemsg(first_mp);
			return;
		}
		ire_refrele(ire);
	}
	if (ip_source_routed(ipha)) {
		icmp_unreachable(q, first_mp, ICMP_SOURCE_ROUTE_FAILED);
		return;
	}
	icmp_unreachable(q, first_mp, ICMP_HOST_UNREACHABLE);
}

/*
 * IPv4 -
 * ip_newroute_ipif is called by ip_wput_multicast and
 * ip_rput_forward_multicast whenever we need to send
 * out a packet to a destination address for which we do not have specific
 * routing information. It is used when the packet will be sent out
 * on a specific interface.
 */
static void
ip_newroute_ipif(queue_t *q, mblk_t *mp, ipif_t *ipif, ipaddr_t dst,
    ipc_t *ipc)
{
	ipha_t  *ipha;
	areq_t	*areq;
	ire_t	*ire = NULL;
	mblk_t	*res_mp;
	queue_t	*stq;
	ipaddr_t *addrp;
	mblk_t *first_mp;
	ire_t	*save_ire;

	ip1dbg(("ip_newroute_ipif: dst 0x%x, if %s\n", ntohl(dst),
	    ipif->ipif_ill->ill_name));

	first_mp = mp;
	if (mp->b_datap->db_type == M_CTL) {
		mp = mp->b_cont;
	}
	ipha = (ipha_t *)mp->b_rptr;

	/*
	 * If the interface is a pt-pt interface we look for an IRE_IF_RESOLVER
	 * or IRE_IF_NORESOLVER that matches both the local_address and the
	 * pt-pt destination address. Otherwise we just match the local address.
	 */
	if (!(ipif->ipif_flags & IFF_MULTICAST)) {
		goto err_ret;
	}

	/*
	 * Pick a source address preferring non-deprecated ones.
	 */
	if (ipif->ipif_flags & IFF_DEPRECATED) {
		ipif_t *src_ipif;

		src_ipif = ipif_select_source(ipif->ipif_ill, dst);
		if (src_ipif == NULL) {
			if (ip_debug > 2) {
				/* ip1dbg */
				pr_addr_dbg("ip_newroute_ipif: no src for"
				    " dst %s\n", AF_INET, &dst);
				ip1dbg(("ip_newroute_ipif: no src for dst"
				    " through interface %s\n",
				    ire->ire_ipif->ipif_ill->ill_name));
			}
			goto err_ret;
		}
		ipif = src_ipif;
	}

	/* ipif_to_ire returns an held ire */
	ire = ipif_to_ire(ipif);
	if (ire == NULL)
		goto err_ret;

	if (ire->ire_flags & (RTF_REJECT | RTF_BLACKHOLE))
		goto err_ret;

	ip1dbg(("ip_newroute_ipif: interface type %s (%d), address 0x%x\n",
	    ip_nv_lookup(ire_nv_tbl, ire->ire_type), ire->ire_type,
	    ntohl(ire->ire_src_addr)));

	/*
	 * Assign a source address while we have the ipc.
	 * We can't have ip_wput_ire pick a source address when the
	 * packet returns from arp since ipc_unspec_src might be set
	 * and we loose the ipc when going through arp.
	 */
	if (ipha->ipha_src == INADDR_ANY &&
	    (ipc == NULL || !ipc->ipc_unspec_src)) {
		ipha->ipha_src = ire->ire_src_addr;
	}

	save_ire = ire;
	stq = ire->ire_stq;
	switch (ire->ire_type) {
	case IRE_IF_NORESOLVER: {
		/* We have what we need to build an IRE_CACHE. */
		mblk_t	*dlureq_mp;
		ill_t	*ill;

		/*
		 * Create a new dlureq_mp with the
		 * IP gateway address in destination address in the DLPI hdr
		 * if the physical length is exactly 4 bytes.
		 */
		ill = ire_to_ill(ire);
		if (ill == NULL) {
			ip0dbg(("ip_newroute_ipif: ire_to_ill failed\n"));
			break;
		}
		if (ill->ill_phys_addr_length == IP_ADDR_LEN) {
			dlureq_mp = ill_dlur_gen((uchar_t *)&dst,
			    ill->ill_phys_addr_length, ill->ill_sap,
			    ill->ill_sap_length);
		} else {
			dlureq_mp = ire->ire_dlureq_mp;
		}

		if (dlureq_mp == NULL)
			break;

		ire = ire_create(
		    (uchar_t *)&dst,			/* dest address */
		    (uchar_t *)&ip_g_all_ones,		/* mask */
		    (uchar_t *)&ipif->ipif_src_addr,	/* source address */
		    NULL,				/* gateway address */
		    save_ire->ire_max_frag,
		    NULL,				/* Fast Path header */
		    save_ire->ire_rfq,			/* recv-from queue */
		    stq,				/* send-to queue */
		    IRE_CACHE,
		    dlureq_mp,
		    save_ire->ire_ipif,
		    0,
		    0,
		    save_ire->ire_ihandle,		/* Interface handle */
		    0,					/* flags if any */
		    &ire_uinfo_null);

		if (ill->ill_phys_addr_length == IP_ADDR_LEN)
			freeb(dlureq_mp);

		if (ire == NULL) {
			ire_refrele(save_ire);
			break;
		}

		/* Prevent save_ire from getting deleted */
		IRB_REFHOLD(save_ire->ire_bucket);
		/* Has it been removed already ? */
		if (save_ire->ire_marks & IRE_MARK_CONDEMNED) {
			IRB_REFRELE(save_ire->ire_bucket);
			ire_refrele(save_ire);
			break;
		}

		/* Remember the packet we want to xmit */
		ire->ire_mp->b_cont = first_mp;
		ire_add_then_send(q, ire->ire_mp);

		/* Assert that it is not deleted yet. */
		ASSERT(save_ire->ire_ptpn != NULL);
		IRB_REFRELE(save_ire->ire_bucket);
		ire_refrele(save_ire);
		return;
	}
	case IRE_IF_RESOLVER:
		/*
		 * We can't build an IRE_CACHE yet, but at least we found a
		 * resolver that can help.
		 */
		res_mp = ire->ire_dlureq_mp;
		if (!stq || !OK_RESOLVER_MP(res_mp))
			break;

		/*
		 * NOTE: a resolvers rfq is NULL and its stq points upstream.
		 *
		 * We obtain a partial IRE_CACHE which we will pass along with
		 * the resolver query.  When the response comes back it will be
		 * there ready for us to add.
		 */
		ire = ire_create(
		    (uchar_t *)&dst,			/* dest address */
		    (uchar_t *)&ip_g_all_ones,		/* mask */
		    (uchar_t *)&ipif->ipif_src_addr,	/* source address */
		    NULL,				/* gateway address */
		    save_ire->ire_max_frag,
		    NULL,				/* Fast Path header */
		    stq,				/* recv-from queue */
		    OTHERQ(stq),			/* send-to queue */
		    IRE_CACHE,
		    res_mp,
		    save_ire->ire_ipif,
		    0,
		    0,
		    save_ire->ire_ihandle,		/* Interface handle */
		    0,					/* flags if any */
		    &ire_uinfo_null);

		ire_refrele(save_ire);
		if (ire == NULL)
			break;

		/*
		 * Construct message chain for the resolver of the form:
		 *	ARP_REQ_MBLK-->IRE_MBLK-->Packet
		 *
		 * NOTE : ire will be added later when the response comes
		 * back from ARP. If the response does not come back, ARP
		 * frees the packet. For this reason, we can't REFHOLD
		 * the bucket of save_ire to prevent deletions. We may not
		 * be able to REFRELE the bucket if the response never comes
		 * back. Thus, before adding the ire, ire_add_v4 will make
		 * sure that the interface route does not get deleted. This
		 * is the only case unlike ip_newroute_v6,
		 * ip_newroute_multi_v6 where we can always prevent deletions
		 * because of the synchronous nature of adding IRES i.e
		 * ire_add_then_send is called after creating the IRE.
		 */
		ire->ire_mp->b_cont = first_mp;
		mp = ire->ire_dlureq_mp;
		ASSERT(mp != NULL);
		ire->ire_dlureq_mp = NULL;
		linkb(mp, ire->ire_mp);

		/*
		 * Fill in the source and dest addrs for the resolver.
		 * NOTE: this depends on memory layouts imposed by ill_init().
		 */
		areq = (areq_t *)mp->b_rptr;
		addrp = (ipaddr_t *)((char *)areq +
		    areq->areq_sender_addr_offset);
		*addrp = ire->ire_src_addr;
		addrp = (ipaddr_t *)((char *)areq +
		    areq->areq_target_addr_offset);
		*addrp = dst;
		/* Up to the resolver. */
		putnext(stq, mp);
		/*
		 * The response will come back in ip_wput with db_type
		 * IRE_DB_TYPE.
		 */
		return;
	default:
		break;
	}

err_ret:
	ip1dbg(("ip_newroute_ipif: dropped\n"));
	/* Did this packet originate externally? */
	if (mp->b_prev || mp->b_next) {
		mp->b_next = NULL;
		mp->b_prev = NULL;
	} else {
		/*
		 * Since ip_wput() isn't close to finished, we fill
		 * in enough of the header for credible error reporting.
		 */
		if (ip_hdr_complete((ipha_t *)mp->b_rptr)) {
			/* Failed */
			freemsg(first_mp);
			if (ire != NULL)
				ire_refrele(ire);
			return;
		}
	}
	/*
	 * At this point we will have ire only if RTF_BLACKHOLE
	 * or RTF_REJECT flags are set on the IRE. It will not
	 * generate ICMP_HOST_UNREACHABLE if RTF_BLACKHOLE is set.
	 */
	if (ire != NULL) {
		if (ire->ire_flags & RTF_BLACKHOLE) {
			ire_refrele(ire);
			freemsg(first_mp);
			return;
		}
		ire_refrele(ire);
	}
	icmp_unreachable(q, first_mp, ICMP_HOST_UNREACHABLE);
}

/* Name/Value Table Lookup Routine */
char *
ip_nv_lookup(nv_t *nv, int value)
{
	if (!nv)
		return (NULL);
	for (; nv->nv_name; nv++) {
		if (nv->nv_value == value)
			return (nv->nv_name);
	}
	return ("unknown");
}

/*
 * one day it can be patched to 1 from /etc/system for machines that have few
 * fast network interfaces feeding multiple cpus.
 */
int ill_stream_putlocks = 0;

/* IP Module open routine. */
int
ip_open(queue_t *q, dev_t *devp, int flag, int sflag, cred_t *credp)
{
	int	err;
	ipc_t	*ipc;
	ill_t	*ill;
	IDP	ptr;
	boolean_t	priv = B_FALSE;

	TRACE_1(TR_FAC_IP, TR_IP_OPEN, "ip_open: q %p", q);

	/* Allow reopen. */
	if (q->q_ptr)
		return (0);

	/*
	 * In first instance of opening (which HAS to be done if IP is
	 * used at all), spawn off the ipsec_loader thread.
	 */
	if (ipsec_loader_thread == NULL) {
		mutex_enter(&ipsec_loader_lock);
		if (ipsec_loader_thread == NULL) {
			ipsec_loader_thread = thread_create(NULL, DEFAULTSTKSZ,
			    ipsec_loader, NULL, 0, &p0, TS_RUN, MAXCLSYSPRI);
			if (ipsec_loader_thread == NULL) {
				cmn_err(CE_WARN,
				    "IPsec loader thread_create failed.\n");
				ipsec_loader_thread = IPSEC_LOADER_LOAD_FAILED;
			}
		}
		/* Else we lost the race, oh well. */
		mutex_exit(&ipsec_loader_lock);
	}

	/*
	 * We are either opening as a device or module.  In the device case,
	 * this is an IP client stream, and we allocate an ipc_t as the
	 * instance data.  If it is a module open, then this is a control
	 * stream for access to a DLPI device.  We allocate an ill_t as the
	 * instance data in this case.
	 *
	 * The open routine uses ill_close_flags to
	 * synchronize with ip_wsrv. These fields are only accessed
	 * by ip_open, ip_close, and ip_wsrv and the ordering of setting
	 * the flags ensures that the atomicify of 32 bit load/store
	 * is sufficient thus no locking is needed.
	 */
	if (credp && drv_priv(credp) == 0)
		priv = B_TRUE;

	if (sflag & MODOPEN) {
		/*
		 * Have to defer the ill_init to ip_wsrv since it requires
		 * single-threading of all of ip.
		 * ip_rput checks for a NULL ill_ipif just in case the DLPI
		 * driver will send up data after the qprocson but before
		 * the ill has been initialized.
		 */
		ill = (ill_t *)mi_open_alloc_sleep(sizeof (ill_t));
		ptr = (IDP)ill;
		/* Initialize the new ILL. */
		q->q_ptr = WR(q)->q_ptr = ill;

		qprocson(q);
		create_putlocks(q, ill_stream_putlocks);

		ill->ill_error = 0;
		ill->ill_close_flags |= IPCF_OPENING;
		while (!(ill->ill_close_flags & IPCF_OPEN_DONE)) {
			qenable(WR(q));
			qwait(WR(q));
		}
		ill->ill_close_flags &= ~IPCF_OPENING;
		if ((err = ill->ill_error) != 0) {
			/* ill_init failed */
			(void) ip_close(q);
			return (err);
		}

		/* Wait for the DL_INFO_ACK */
		while (ill->ill_ipif == NULL) {
			if (!qwait_sig(ill->ill_wq)) {
				(void) ip_close(q);
				return (EINTR);
			}
		}
		if ((err = ill->ill_error) != 0) {
			/* ill_init failed */
			(void) ip_close(q);
			return (err);
		}
		if (priv)
			ill->ill_priv_stream = 1;
	} else {
		ipc = (ipc_t *)mi_open_alloc_sleep(sizeof (ipc_t));
		ptr = (IDP)ipc;
		/* Initialize the new IPC. */
		q->q_ptr = WR(q)->q_ptr = ipc;
		ipc->ipc_rq = q;
		ipc->ipc_wq = WR(q);
		/* Non-zero default values */
		ipc->ipc_multicast_loop = IP_DEFAULT_MULTICAST_LOOP;
		if (priv)
			ipc->ipc_priv_stream = 1;
		mutex_init(&ipc->ipc_reflock, NULL,
		    MUTEX_DEFAULT, NULL);
		mutex_init(&ipc->ipc_irc_lock, NULL,
		    MUTEX_DEFAULT, NULL);
		cv_init(&ipc->ipc_refcv, NULL, CV_DEFAULT, NULL);
		/* Minor tells us which /dev entry was opened */
		if (geteminor(*devp) == IPV6_MINOR) {
			ipc->ipc_af_isv6 = B_TRUE;
			ip_setqinfo(q, B_TRUE, B_FALSE);
		} else {
			ipc->ipc_af_isv6 = B_FALSE;
			ipc->ipc_pkt_isv6 = B_FALSE;
		}
	}

	mutex_enter(&ip_mi_lock);
	err = mi_open_link(&ip_g_head, ptr, devp, flag, sflag, credp);
	mutex_exit(&ip_mi_lock);
	if (err) {
		if (sflag & MODOPEN) {
			(void) ip_close(q);
		} else {
			mi_close_free(ptr);
			q->q_ptr = WR(q)->q_ptr = NULL;
		}
		return (err);
	}

	/*
	 * Since D_MTOCSHARED is set the qprocson is deferred until
	 * we are ready to go for the ipc.
	 */
	if (!(sflag & MODOPEN)) {
		qprocson(q);
		create_putlocks(q, 0);
	}
	return (0);
}

/*
 * Change q_qinfo based on the value of isv6.
 * This can not called on an ill queue.
 * Note that there is no race since either q_qinfo works for ipc queues - it
 * is just an optimization to enter the best wput routine directly.
 */
void
ip_setqinfo(queue_t *q, boolean_t isv6, boolean_t bump_mib)
{
	ASSERT(q->q_flag & QREADR);
	ASSERT(WR(q)->q_next == NULL);
	ASSERT(q->q_ptr != NULL);
	if (isv6)  {
		if (bump_mib)
			BUMP_MIB(ip6_mib.ipv6OutSwitchIPv4);
		q->q_qinfo = &rinit_ipv6;
		WR(q)->q_qinfo = &winit_ipv6;
	} else {
		if (bump_mib)
			BUMP_MIB(ip_mib.ipOutSwitchIPv6);
		q->q_qinfo = &rinit;
		WR(q)->q_qinfo = &winit;
	}
	((ipc_t *)q->q_ptr)->ipc_pkt_isv6 = isv6;
}

/*
 * NOTE:  This function is entered w/o holding any STREAMS perimeters.
 */
/* ARGSUSED */
static void
ipsec_loader(void *ignoreme)
{
	extern int keysock_plumb_ipsec(void);
	callb_cpr_t cprinfo;

	CALLB_CPR_INIT(&cprinfo, &ipsec_loader_lock, callb_generic_cpr,
	    "ipsec_loader");
	mutex_enter(&ipsec_loader_lock);
	for (; ; ) {

		/*
		 * Wait for someone to tell me to continue.
		 */
		CALLB_CPR_SAFE_BEGIN(&cprinfo);
		while (ipsec_loader_sig == IPSEC_LOADER_UNLOADED)
			cv_wait(&ipsec_loader_sig_cv, &ipsec_loader_lock);
		CALLB_CPR_SAFE_END(&cprinfo, &ipsec_loader_lock);

		/* -1 implies signal by _fini(). */
		if (ipsec_loader_sig == IPSEC_LOADER_EXITNOW) {
			/*
			 * Let use of kadb set ipsec_loader_thread to
			 * (kthread_t *)0 to try again.
			 */
			ipsec_loader_sig = IPSEC_LOADER_EXITED;
			cv_signal(&ipsec_loader_exit_cv);

			/* ipsec_loader_lock is held at this point! */
			ASSERT(MUTEX_HELD(&ipsec_loader_lock));
			CALLB_CPR_EXIT(&cprinfo);
			thread_exit();
		}
		mutex_exit(&ipsec_loader_lock);

		/*
		 * Load IPsec, which is done by modloading keysock and calling
		 * keysock_plumb_ipsec().
		 */

		/* Pardon my hardcoding... */
		if (modload("drv", "keysock") == -1) {
			cmn_err(CE_WARN, "IP: Cannot load keysock.\n");
			/*
			 * Only this function can set ipsec_failure.  If the
			 * damage can be repaired, use adb to set this to
			 * B_FALSE and try again.
			 */
			ipsec_failure = B_TRUE;
		} else if (keysock_plumb_ipsec() != 0) {
			cmn_err(CE_WARN, "IP: Cannot plumb IPsec.\n");
			/*
			 * Only this function can set ipsec_failure.  If the
			 * damage can be repaired, use adb to set this to
			 * B_FALSE and try again.
			 */
			ipsec_failure = B_TRUE;
		}

		mutex_enter(&ipsec_loader_lock);
		if (ipsec_loader_q != NULL) {
			ASSERT(ipsec_loader_sig != IPSEC_LOADER_UNLOADED);
			qenable(ipsec_loader_q);
			ipsec_loader_q = NULL;
		}

		/*
		 * Tell any waiting close routines (actually any waiting wsrv
		 * close cleanups) that the IPsec module loader is now done.
		 */
		cv_signal(&ipsec_loader_cv);

		if (ipsec_failure) {
			ipsec_loader_sig = IPSEC_LOADER_UNLOADED;
		} else {
			/* Assume '2' is not a valid pointer. */
			ipsec_loader_thread = IPSEC_LOADER_LOAD_OK;
			CALLB_CPR_EXIT(&cprinfo);
			thread_exit();
		}
	}
}

static boolean_t
ipsec_needsloading(mblk_t *mp)
{
	uint8_t *optcp, *next_optcp, *opt_endcp;
	struct opthdr *opt;
	struct T_opthdr *topt;
	int opthdr_len;
	t_uscalar_t optname, optlevel;
	struct T_optmgmt_req *tor = (struct T_optmgmt_req *)mp->b_rptr;
	ipsec_req_t *ipsr;

	/*
	 * First check if IPsec is loaded.  I'm a writer (called from
	 * ip_optcom_req) at this point, so it should not be a problem to
	 * check.  AH should be sufficient to check for IPsec.
	 */
	if (ipc_proto_fanout_v6[IPPROTO_AH].icf_ipc != NULL || ipsec_failure) {
		return (B_FALSE);
	}

	/*
	 * Walk through the mess, and find IP_SEC_OPT.  If it's there,
	 * return TRUE.  My apologies for duplicating parts of
	 * ip_optmgmt_writer.  I really can't do these checks, though
	 * without first being called as a writer.
	 */

	optcp = mi_offset_param(mp, tor->OPT_offset, tor->OPT_length);
	opt_endcp = optcp + tor->OPT_length;
	if (tor->PRIM_type == T_OPTMGMT_REQ) {
		opthdr_len = sizeof (struct T_opthdr);
	} else {		/* O_OPTMGMT_REQ */
		ASSERT(tor->PRIM_type == T_SVR4_OPTMGMT_REQ);
		opthdr_len = sizeof (struct opthdr);
	}
	for (; optcp < opt_endcp; optcp = next_optcp) {
		if (optcp + opthdr_len > opt_endcp)
			return (B_FALSE);	/* Not enough option header. */
		if (tor->PRIM_type == T_OPTMGMT_REQ) {
			topt = (struct T_opthdr *)optcp;
			optlevel = topt->level;
			optname = topt->name;
			next_optcp = optcp + _TPI_ALIGN_TOPT(topt->len);
		} else {
			opt = (struct opthdr *)optcp;
			optlevel = opt->level;
			optname = opt->name;
			next_optcp = optcp + opthdr_len +
			    _TPI_ALIGN_OPT(opt->len);
		}
		if ((next_optcp < optcp) || /* wraparound pointer space */
		    ((next_optcp >= opt_endcp) && /* last option bad len */
			((next_optcp - opt_endcp) >= __TPI_ALIGN_SIZE)))
			return (B_FALSE); /* bad option buffer */
		if (optlevel == IPPROTO_IP && optname == IP_SEC_OPT) {
			/*
			 * Check to see if it's an all-bypass or all-zeroes
			 * IPsec request.  Don't bother loading IPsec if
			 * the socket doesn't want to use it.  (A good example
			 * is a bypass request.)
			 *
			 * Basically, if any of the non-NEVER bits are set,
			 * load IPsec.
			 */
			ipsr = (ipsec_req_t *)(optcp + opthdr_len);
			if ((ipsr->ipsr_ah_req & ~IPSEC_PREF_NEVER) != 0 ||
			    (ipsr->ipsr_esp_req & ~IPSEC_PREF_NEVER) != 0 ||
			    (ipsr->ipsr_self_encap_req & ~IPSEC_PREF_NEVER)
			    != 0)
				return (B_TRUE);
		}
	}

	return (B_FALSE);
}

static void
ipsec_loadnow(queue_t *q, mblk_t *mp)
{
	ipc_t *ipc = (ipc_t *)q->q_ptr;

	/*
	 * This mblk has an IPsec option, and IPsec isn't loaded yet.
	 * Load IPsec.
	 */

	mutex_enter(&ipsec_loader_lock);
	ASSERT(q == WR(q));
	if (ipsec_loader_sig == IPSEC_LOADER_UNLOADED) {
		/*
		 * If I've reached here, IPsec isn't loaded yet, and
		 * there's been no attempt to load IPsec done.  So make
		 * it happen here.
		 */

		ipsec_loader_q = q;
		ipsec_loader_sig = IPSEC_LOADER_LOADNOW;
		cv_signal(&ipsec_loader_sig_cv);
	}
	mutex_exit(&ipsec_loader_lock);

	if (ipc->ipc_draining) {
		ipc->ipc_did_putbq = 1;
		(void) putbq(ipc->ipc_wq, mp);
	} else {
		/* Flow control the queue until IPsec is loaded. */
		(void) putq(q, mp);
	}
}

static void
ip_optcom_req(queue_t *q, mblk_t *mp)
{
	t_scalar_t optreq_prim = ((union T_primitives *)mp->b_rptr)->type;

	/*
	 * Take IPsec requests and treat them special.
	 * Since we're a writer, we can just inspect ipsec_req_*.
	 */
	if (ipsec_needsloading(mp)) {
		ipsec_loadnow(q, mp);
		return;
	}

	if (optreq_prim == T_OPTMGMT_REQ) {
		/*
		 * Note: No snmpcom_req support through new
		 * T_OPTMGMT_REQ.
		 */
		tpi_optcom_req(q, mp, IS_PRIVILEGED_QUEUE(q), &ip_opt_obj);
	} else {
		ASSERT(optreq_prim == T_SVR4_OPTMGMT_REQ);
		if (!snmpcom_req(q, mp, ip_snmp_set, ip_snmp_get,
		    IS_PRIVILEGED_QUEUE(q)))
			svr4_optcom_req(q, mp, IS_PRIVILEGED_QUEUE(q),
			    &ip_opt_obj);
	}
}
/*
 * Set IPSEC policy. If req is not null and valid, all of them are copied
 * to the ipc_t. If req is NULL, policy is zeroed out. We keep only the
 * latest setting of the policy and thus policy setting is not
 * incremental/cumulative.
 *
 * XXX We need to think about supporting multiple matches i.e store
 * multiple requests and try matching one of them.
 */
static int
ipsec_set_options(boolean_t priv, ipc_t *ipc, ipsec_req_t *req)
{
	uint_t ah_req;
	uint_t esp_req;
	uint_t se_req;
	uint_t req_mask;

	/* If we couldn't load IPsec, fail with "protocol not supported". */
	if (ipsec_failure)
		return (EPROTONOSUPPORT);

	/*
	 * If we have already cached policies in ip_bind, don't
	 * let them change now. We cache policies for connections
	 * whose src,dst [addr, port] is known.
	 */
	if (ipc->ipc_policy_cached)
		return (EINVAL);

	if (req != NULL) {
		ah_req = req->ipsr_ah_req;
		esp_req = req->ipsr_esp_req;
		se_req = req->ipsr_self_encap_req;
		/*
		 * Test for valid requests. Invalid algorithms
		 * need to be tested by IPSEC code because new
		 * algorithms can be added dynamically.
		 */
		if ((ah_req & ~(IPSEC_PREF_NEVER|IPSEC_PREF_REQUIRED|
		    IPSEC_PREF_UNIQUE)) != 0) {
			return (EINVAL);
		}
		if ((esp_req & ~(IPSEC_PREF_NEVER|IPSEC_PREF_REQUIRED|
		    IPSEC_PREF_UNIQUE)) != 0) {
			return (EINVAL);
		}
		if ((se_req & ~(IPSEC_PREF_NEVER|IPSEC_PREF_REQUIRED|
		    IPSEC_PREF_UNIQUE)) != 0) {
			return (EINVAL);
		}

		if ((ah_req & IPSEC_PREF_NEVER) ||
		    (esp_req & IPSEC_PREF_NEVER) ||
		    (se_req & IPSEC_PREF_NEVER)) {
			/*
			 * Only super user can issue these
			 * requests.
			 */
			if (!priv)
				return (EPERM);
		}

		req_mask = (IPSEC_PREF_REQUIRED|IPSEC_PREF_NEVER);
		if (((ah_req & req_mask) == req_mask) ||
		    ((esp_req & req_mask) == req_mask) ||
		    ((se_req & req_mask) == req_mask)) {

			/* Both of them are set */
			return (EINVAL);
		}
		/*
		 * Enforce the same policy for both outbound and
		 * inbound.
		 */
		if (ipc->ipc_outbound_policy == NULL) {
			int ret;

			ASSERT(ipc->ipc_inbound_policy == NULL);
			if ((ret = ipsec_policy_alloc(ipc)) != 0) {
				return (ret);
			}
		}

		ASSERT(ipc->ipc_outbound_policy != NULL &&
		    ipc->ipc_inbound_policy != NULL);

		bcopy(req, ipc->ipc_inbound_policy, sizeof (ipsec_req_t));
		bcopy(req, ipc->ipc_outbound_policy, sizeof (ipsec_req_t));

		/*
		 * If the requests need security, set enforce_policy.
		 * If the requests are IPSEC_PREF_NEVER, one should
		 * still set ipc_out_enforce_policy so that an ipsec_out
		 * gets attached in ip_wput. This is needed so that
		 * for connections that we don't cache policy in ip_bind,
		 * if global policy matches in ip_wput_attach_policy, we
		 * don't wrongly inherit global policy. Similarly, we need
		 * to set ipc_in_enforce_policy also so that we don't verify
		 * policy wrongly.
		 */
		if ((ah_req & req_mask) != 0 ||
		    (esp_req & req_mask) != 0 ||
		    (se_req & req_mask) != 0) {
			ipc->ipc_in_enforce_policy = B_TRUE;
			ipc->ipc_out_enforce_policy = B_TRUE;
		}
	} else {
		/*
		 * Prevent somebody from resetting policy without
		 * setting them.
		 */
		if (ipc->ipc_outbound_policy == NULL) {
			ASSERT(ipc->ipc_inbound_policy == NULL);
			return (EINVAL);
		}
		bzero(ipc->ipc_outbound_policy, sizeof (ipsec_req_t));
		bzero(ipc->ipc_inbound_policy, sizeof (ipsec_req_t));

		ipc->ipc_in_enforce_policy = B_FALSE;
		ipc->ipc_out_enforce_policy = B_FALSE;
	}
	return (0);
}

/* This routine sets socket options. */
int
ip_opt_set(queue_t *q, uint_t optset_context, int level, int name, uint_t inlen,
    uchar_t *invalp, uint_t *outlenp, uchar_t *outvalp)
{
	int	*i1	= (int *)invalp;
	ipc_t	*ipc	= (ipc_t *)q->q_ptr;
	int	error	= 0;
	boolean_t priv	= IS_PRIVILEGED_QUEUE(q);
	boolean_t checkonly;
	ire_t *ire;

	switch (optset_context) {

	case SETFN_OPTCOM_CHECKONLY:
		checkonly = B_TRUE;
		/*
		 * Note: Implies T_CHECK semantics for T_OPTCOM_REQ
		 * inlen != 0 implies value supplied and
		 * 	we have to "pretend" to set it.
		 * inlen == 0 implies that there is no
		 * 	value part in T_CHECK request and just validation
		 * done elsewhere should be enough, we just return here.
		 */
		if (inlen == 0) {
			*outlenp = 0;
			return (0);
		}
		break;
	case SETFN_OPTCOM_NEGOTIATE:
	case SETFN_UD_NEGOTIATE:
	case SETFN_CONN_NEGOTIATE:
		checkonly = B_FALSE;
		break;
	default:
		/*
		 * We should never get here
		 */
		*outlenp = 0;
		return (EINVAL);
	}

	ASSERT((optset_context != SETFN_OPTCOM_CHECKONLY) ||
	    (optset_context == SETFN_OPTCOM_CHECKONLY && inlen != 0));

	/*
	 * For fixed length options, no sanity check
	 * of passed in length is done. It is assumed *_optcom_req()
	 * routines do the right thing.
	 */

	switch (level) {
	case SOL_SOCKET:
		/*
		 * XXX ipc_reflock is used to serialize the
		 * setting of socket options. The logic
		 * needs to be changed, when IP becomes MT hot.
		 */
		switch (name) {
		case SO_BROADCAST:
			if (!checkonly) {
				/* TODO: use value someplace? */
				mutex_enter(&ipc->ipc_reflock);
				ipc->ipc_broadcast = *i1 ? 1 : 0;
				mutex_exit(&ipc->ipc_reflock);
			}
			break;	/* goto sizeof (int) option return */
		case SO_USELOOPBACK:
			if (!checkonly) {
				/* TODO: use value someplace? */
				mutex_enter(&ipc->ipc_reflock);
				ipc->ipc_loopback = *i1 ? 1 : 0;
				mutex_exit(&ipc->ipc_reflock);
			}
			break;	/* goto sizeof (int) option return */
		case SO_DONTROUTE:
			if (!checkonly) {
				mutex_enter(&ipc->ipc_reflock);
				ipc->ipc_dontroute = *i1 ? 1 : 0;
				mutex_exit(&ipc->ipc_reflock);
			}
			break;	/* goto sizeof (int) option return */
		case SO_REUSEADDR:
			if (!checkonly) {
				mutex_enter(&ipc->ipc_reflock);
				ipc->ipc_reuseaddr = *i1 ? 1 : 0;
				mutex_exit(&ipc->ipc_reflock);
			}
			break;	/* goto sizeof (int) option return */
		case SO_PROTOTYPE:
			if (!checkonly) {
				mutex_enter(&ipc->ipc_reflock);
				ipc->ipc_proto = *i1;
				mutex_exit(&ipc->ipc_reflock);
			}
			break;	/* goto sizeof (int) option return */
		default:
			/*
			 * "soft" error (negative)
			 * option not handled at this level
			 * Note: Do not modify *outlenp
			 */
			return (-EINVAL);
		}
		break;
	case IPPROTO_IP:
		switch (name) {
		case IP_MULTICAST_IF: {
			ipaddr_t addr = *i1;

			ip2dbg(("ip_opt_set: MULTICAST IF\n"));
			if (checkonly) {
				/* T_CHECK case */
				if (ipif_lookup_addr(addr, NULL) == NULL) {
					*outlenp = 0;
					error = EHOSTUNREACH;
					return (error);
				}
				break; /* goto sizeof (int) option return */
			}
			if (addr == INADDR_ANY) {	/* Reset */
				ipc->ipc_multicast_ipif = NULL;
				ipc->ipc_multicast_ill = NULL;
				break; /* goto sizeof (int) option return */
			}
			ipc->ipc_multicast_ipif =
			    ipif_lookup_addr(addr, NULL);
			if (!ipc->ipc_multicast_ipif) {
				*outlenp = 0;
				error = EHOSTUNREACH;
				return (error);
			}
			ipc->ipc_multicast_ill =
			    ipc->ipc_multicast_ipif->ipif_ill;
			break;	/* goto sizeof (int) option return */
		}
		case IP_MULTICAST_TTL:
			/* Recorded in transport above IP */
			*outvalp = *invalp;
			*outlenp = sizeof (uchar_t);
			return (0);
		case IP_MULTICAST_LOOP:
			ip2dbg(("ip_opt_set: MULTICAST LOOP\n"));
			if (!checkonly)
				ipc->ipc_multicast_loop = *invalp ? 1 : 0;
			*outvalp = *invalp;
			*outlenp = sizeof (uchar_t);
			return (0);
		case IP_ADD_MEMBERSHIP: {
			struct ip_mreq *ip_mreqp = (struct ip_mreq *)i1;

			ip2dbg(("ip_opt_set: ADD MEMBER\n"));
			error = ip_opt_add_group(ipc, checkonly,
			    (ipaddr_t)ip_mreqp->imr_multiaddr.s_addr,
			    (ipaddr_t)ip_mreqp->imr_interface.s_addr);
			if (error) {
				*outlenp = 0;
				return (error);
			}
			/* OK return - copy input buffer into output buffer */
			if (invalp != outvalp) {
				/* don't trust bcopy for identical src/dst */
				bcopy(invalp, outvalp, inlen);
			}
			*outlenp = inlen;
			return (0);
		}
		case IP_DROP_MEMBERSHIP: {
			struct ip_mreq *ip_mreqp = (struct ip_mreq *)i1;

			ip2dbg(("ip_opt_set: DROP MEMBER\n"));
			error = ip_opt_delete_group(ipc, checkonly,
			    (ipaddr_t)ip_mreqp->imr_multiaddr.s_addr,
			    (ipaddr_t)ip_mreqp->imr_interface.s_addr);

			if (error) {
				*outlenp = 0;
				return (error);
			}
			/* OK return - copy input buffer into output buffer */
			if (invalp != outvalp) {
				/* don't trust bcopy for identical src/dst */
				bcopy(invalp, outvalp, inlen);
			}
			*outlenp = inlen;
			return (0);
		}
		case IP_SEC_OPT:
			error = ipsec_set_options(priv, ipc,
			    (ipsec_req_t *)invalp);
			if (error) {
				*outlenp = 0;
				return (EINVAL);
			}
			break;
		case IP_HDRINCL:
		case IP_OPTIONS:
		case T_IP_OPTIONS:
		case IP_TOS:
		case T_IP_TOS:
		case IP_TTL:
		case IP_RECVDSTADDR:
		case IP_RECVOPTS:
			/* OK return - copy input buffer into output buffer */
			if (invalp != outvalp) {
				/* don't trust bcopy for identical src/dst */
				bcopy(invalp, outvalp, inlen);
			}
			*outlenp = inlen;
			return (0);
		case IP_ADD_PROXY_ADDR: {
			proxy_addr_t *pa;
			proxy_addr_t pa1;
			ipaddr_t mask = 0;
			unsigned int i = ((in_prefix_t *)i1)->in_prefix_len;

			if (i > IP_ABITS) {
				*outlenp = 0;
				return (EINVAL);
			}
			pa1.pa_addr = (ipaddr_t)
			    ((in_prefix_t *)i1)->in_prefix_addr.s_addr;
			for (pa1.pa_mask = 0, mask |= 0x80000000; i != 0; i--) {
				pa1.pa_mask |= mask;
				mask = mask >> 1;
			}
			pa1.pa_mask = htonl(pa1.pa_mask);
			pa1.pa_addr &= pa1.pa_mask;
			for (pa = ipc->ipc_palist; pa != NULL;
			    pa = pa->pa_next) {
				if (ip_proxy_addr_compare(&pa1, pa) != 0) {
					/* Any overlap is forbidden */
					*outlenp = 0;
					return (EINVAL);
				}
			}
			pa = (proxy_addr_t *)kmem_alloc(sizeof (proxy_addr_t),
				KM_NOSLEEP);
			if (pa == NULL) {
				*outlenp = 0;
				return (ENOMEM);
			}
			*pa = pa1;
			pa->pa_next = ipc->ipc_palist;
			ipc->ipc_palist = pa;
			/* OK return - copy input buffer into output buffer */
			if (invalp != outvalp) {
				/* don't trust bcopy for identical src/dst */
				bcopy(invalp, outvalp, inlen);
			}
			*outlenp = inlen;
			return (0);
		}
		case MRT_INIT:
		case MRT_DONE:
		case MRT_ADD_VIF:
		case MRT_DEL_VIF:
		case MRT_ADD_MFC:
		case MRT_DEL_MFC:
		case MRT_ASSERT:
			if (!priv) {
				error = EPERM;
				*outlenp = 0;
				return (error);
			}
			error = ip_mrouter_set((int)name, q, checkonly,
			    (uchar_t *)invalp, inlen);
			if (error) {
				*outlenp = 0;
				return (error);
			}
			/* OK return - copy input buffer into output buffer */
			if (invalp != outvalp) {
				/* don't trust bcopy for identical src/dst */
				bcopy(invalp, outvalp, inlen);
			}
			*outlenp = inlen;
			return (0);
		case IP_BOUND_IF: {
			/*
			 * Limit all receipt to this ill and limit
			 * transmitted broadcast packets to this ill.
			 */
			int ifindex = *i1;
			ill_t *ill;

			ip2dbg(("ip_optmgt_req: IP_BOUND_IF %d\n", ifindex));
			if (ifindex == 0) {
				/* Reset */
				ipc->ipc_incoming_ill = NULL;
				ipc->ipc_outgoing_ill = NULL;
				break;	/* goto sizeof (int) option return */
			}
			ill = ill_lookup_on_ifindex(ifindex, 0);
			if (ill == NULL) {
				return (EINVAL);
			}
			if (!checkonly) {
				ipc->ipc_incoming_ill = ill;
				ipc->ipc_outgoing_ill = ill;
			}
			break;	/* goto sizeof (int) option return */
		}
		case IP_UNSPEC_SRC:
			/* Allow sending with a zero source address */
			if (!checkonly)
				ipc->ipc_unspec_src = *i1 ? 1 : 0;
			break;	/* goto sizeof (int) option return */
		default:
			/*
			 * "soft" error (negative)
			 * option not handled at this level
			 * Note: Do not modify *outlenp
			 */
			return (-EINVAL);
		}
		break;
	case IPPROTO_IPV6:
		switch (name) {
		case IPV6_BOUND_IF: {
			/*
			 * Limit all receipt and all transmit to this ill.
			 */
			int ifindex = *i1;
			ill_t *ill;

			ip2dbg(("ip_optmgt_req: IPV6_BOUND_IF %d\n", ifindex));
			if (ifindex == 0) {
				/* Reset */
				ipc->ipc_incoming_ill = NULL;
				ipc->ipc_outgoing_ill = NULL;
				break;	/* goto sizeof (int) option return */
			}
			ill = ill_lookup_on_ifindex(ifindex, 1);
			if (ill == NULL) {
				return (EINVAL);
			}
			if (!checkonly) {
				ipc->ipc_incoming_ill = ill;
				ipc->ipc_outgoing_ill = ill;
			}
			break;	/* goto sizeof (int) option return */
		}
		case IPV6_MULTICAST_IF: {
			uint_t   index = (uint_t)*i1;
			uint_t   dummy;

			ip2dbg(("ip_optmgt_req: IPV6_MULTICAST_IF\n"));
			if (index == 0) {
				/* Reset */
				ipc->ipc_multicast_ipif = NULL;
				ipc->ipc_multicast_ill = NULL;
				break;
			}
			ipc->ipc_multicast_ill =
			    ill_lookup_on_ifindex(index, 1);

			if (ipc->ipc_multicast_ill == NULL) {
				ipc->ipc_multicast_ipif = NULL;
				return (EINVAL);
			}
			/*
			 * Here we find the first ipif in the chain
			 * with a global scope or less.
			 * First look without IFF_DEPRECATED then
			 * use deprecated as a last resort.
			 */
			ipc->ipc_multicast_ipif =
			    ipif_lookup_scope_v6(ipc->ipc_multicast_ill,
				&ipv6_unspecified_group,
				IP6_SCOPE_GLOBAL, IP6_SCOPE_LINKLOCAL,
				IP6_SCOPE_GLOBAL, IFF_UP, IFF_DEPRECATED,
				&dummy);
			if (ipc->ipc_multicast_ipif != NULL)
				break;

			ipc->ipc_multicast_ipif =
			    ipif_lookup_scope_v6(ipc->ipc_multicast_ill,
				&ipv6_unspecified_group,
				IP6_SCOPE_GLOBAL, IP6_SCOPE_LINKLOCAL,
				IP6_SCOPE_GLOBAL, IFF_UP | IFF_DEPRECATED, 0,
				&dummy);
			if (ipc->ipc_multicast_ipif == NULL) {
				/* Shouldn't happen unless if is down */
				ipc->ipc_multicast_ill = NULL;
				return (EHOSTUNREACH);
			}
			break;
		}
		case IPV6_MULTICAST_HOPS:
			/* Recorded in transport above IP */
			break;	/* goto sizeof (int) option return */
		case IPV6_MULTICAST_LOOP:
			ip2dbg(("ip_optmgt_req: IPV6_MULTICAST_LOOP\n"));
			if (!checkonly)
				ipc->ipc_multicast_loop = *i1;
			break;	/* goto sizeof (int) option return */
		case IPV6_JOIN_GROUP: {
			struct ipv6_mreq *ip_mreqp = (struct ipv6_mreq *)i1;

			ip2dbg(("ip_optmgt_req: IPV6_JOIN_GROUP\n"));
			error = ip_opt_add_group_v6(ipc, checkonly,
			    (in6_addr_t *)&ip_mreqp->ipv6mr_multiaddr,
			    ip_mreqp->ipv6mr_interface);
			if (error) {
				*outlenp = 0;
				return (error);
			}
			/* OK return - copy input buffer into output buffer */
			if (invalp != outvalp) {
				/* don't trust bcopy for identical src/dst */
				bcopy(invalp, outvalp, inlen);
			}
			*outlenp = inlen;
			return (0);
		}
		case IPV6_LEAVE_GROUP: {
			struct ipv6_mreq *ip_mreqp = (struct ipv6_mreq *)i1;

			ip2dbg(("ip_optmgt_req: IPV6_LEAVE_GROUP\n"));
			error = ip_opt_delete_group_v6(ipc, checkonly,
			    (in6_addr_t *)&ip_mreqp->ipv6mr_multiaddr,
			    ip_mreqp->ipv6mr_interface);
			if (error) {
				*outlenp = 0;
				return (error);
			}
			/* OK return - copy input buffer into output buffer */
			if (invalp != outvalp) {
				/* don't trust bcopy for identical src/dst */
				bcopy(invalp, outvalp, inlen);
			}
			*outlenp = inlen;
			return (0);
		}
		case IPV6_UNICAST_HOPS:
			/* Recorded in transport above IP */
			break;	/* goto sizeof (int) option return */
		case IPV6_UNSPEC_SRC:
			/* Allow sending with a zero source address */
			if (!checkonly)
				ipc->ipc_unspec_src = *i1 ? 1 : 0;
			break;	/* goto sizeof (int) option return */
		case IPV6_RECVPKTINFO:
			if (!checkonly)
				ipc->ipc_ipv6_recvpktinfo = *i1 ? 1 : 0;
			break;	/* goto sizeof (int) option return */
		case IPV6_RECVHOPLIMIT:
			if (!checkonly)
				ipc->ipc_ipv6_recvhoplimit = *i1 ? 1 : 0;
			break;	/* goto sizeof (int) option return */
		case IPV6_RECVHOPOPTS:
			if (!checkonly)
				ipc->ipc_ipv6_recvhopopts = *i1 ? 1 : 0;
			break;	/* goto sizeof (int) option return */
		case IPV6_RECVDSTOPTS:
			if (!checkonly)
				ipc->ipc_ipv6_recvdstopts = *i1 ? 1 : 0;
			break;	/* goto sizeof (int) option return */
		case IPV6_RECVRTHDR:
			if (!checkonly)
				ipc->ipc_ipv6_recvrthdr = *i1 ? 1 : 0;
			break;	/* goto sizeof (int) option return */
		case IPV6_RECVRTHDRDSTOPTS:
			if (!checkonly)
				ipc->ipc_ipv6_recvrtdstopts = *i1 ? 1 : 0;
			break;	/* goto sizeof (int) option return */
		case IPV6_PKTINFO: {
			struct in6_pktinfo *pkti;
			/*
			 * Verify the source address and ifindex. Priviledged
			 * users can use any source address.
			 * For ancillary data the
			 * source address is checked in ip_wput_v6.
			 */
			if (inlen == 0)
				return (-EINVAL);	/* clearing option */

			pkti = (struct in6_pktinfo *)invalp;
			if (pkti->ipi6_ifindex != 0) {
				if (ill_lookup_on_ifindex(pkti->ipi6_ifindex,
				    B_TRUE) == NULL) {
					*outlenp = 0;
					return (ENXIO);
				}
			}
			if (!IN6_IS_ADDR_UNSPECIFIED(&pkti->ipi6_addr) &&
			    !priv) {
				ire = ire_route_lookup_v6(&pkti->ipi6_addr,
				    0, 0, (IRE_LOCAL|IRE_LOOPBACK), NULL,
				    NULL, NULL, MATCH_IRE_TYPE);
				if (ire == NULL) {
					*outlenp = 0;
					return (EADDRNOTAVAIL);
				}
				ire_refrele(ire);
			}
			return (-EINVAL);
		}
		case IPV6_NEXTHOP: {
			struct sockaddr_in6 *sin6;

			/* Verify that the nexthop is reachable */
			if (inlen == 0)
				return (-EINVAL);	/* clearing option */

			sin6 = (struct sockaddr_in6 *)invalp;
			ire = ire_route_lookup_v6(&sin6->sin6_addr,
			    0, 0, 0, NULL, NULL, NULL, MATCH_IRE_DEFAULT);

			if (ire == NULL) {
				*outlenp = 0;
				return (EHOSTUNREACH);
			}
			ire_refrele(ire);
			return (-EINVAL);
		}
		default:
			return (-EINVAL);
		}
		break;
	default:
		/*
		 * "soft" error (negative)
		 * option not handled at this level
		 * Note: Do not modify *outlenp
		 */
		return (-EINVAL);
	}
	/*
	 * Common case of return from an option that is sizeof (int)
	 */
	*(int *)outvalp = *i1;
	*outlenp = sizeof (int);
	return (0);
}

/*
 * This routine gets default values of certain options whose default
 * values are maintained by protocol specific code
 */
/* ARGSUSED */
int
ip_opt_default(queue_t *q, int level, int name, uchar_t *ptr)
{
	int *i1 = (int *)ptr;

	switch (level) {
	case IPPROTO_IP:
		switch (name) {
		case IP_MULTICAST_TTL:
			*ptr = (uchar_t)IP_DEFAULT_MULTICAST_TTL;
			return (sizeof (uchar_t));
		case IP_MULTICAST_LOOP:
			*ptr = (uchar_t)IP_DEFAULT_MULTICAST_LOOP;
			return (sizeof (uchar_t));
		default:
			return (-1);
		}
	case IPPROTO_IPV6:
		switch (name) {
		case IPV6_UNICAST_HOPS:
			*i1 = ipv6_def_hops;
			return (sizeof (int));
		case IPV6_MULTICAST_HOPS:
			*i1 = IP_DEFAULT_MULTICAST_TTL;
			return (sizeof (int));
		case IPV6_MULTICAST_LOOP:
			*i1 = IP_DEFAULT_MULTICAST_LOOP;
			return (sizeof (int));
		default:
			return (-1);
		}
	default:
		return (-1);
	}
	/* NOTREACHED */
}

/*
 * This routine gets socket options.  For MRT_VERSION and MRT_ASSERT, error
 * checking of IS_PRIVILEGED(q) and that ip_g_mrouter is set should be done and
 * isn't.  This doesn't matter as the error checking is done properly for the
 * other MRT options coming in through ip_opt_set.
 */
int
ip_opt_get(queue_t *q, int level, int name, uchar_t *ptr)
{
	ipc_t	*ipc = (ipc_t *)q->q_ptr;

	switch (level) {
	case IPPROTO_IP:
		switch (name) {
		case MRT_VERSION:
		case MRT_ASSERT:
			(void) ip_mrouter_get(name, q, ptr);
			return (sizeof (int));
		case IP_SEC_OPT:
			/*
			 * As outbound and inbound are the same with
			 * per-socket policy, copy it from one of them.
			 */
			if (ipc->ipc_outbound_policy != NULL) {
				bcopy((uchar_t *)ipc->ipc_outbound_policy, ptr,
				    sizeof (ipsec_req_t));
			} else {
				bzero(ptr, sizeof (ipsec_req_t));
			}
			return (sizeof (ipsec_req_t));
		default:
			break;
		}
		break;
	case IPPROTO_IPV6:
	default:
		break;
	}
	return (-1);
}

/*
 * Return 1 if there is something that requires the write lock in IP
 * Return 0 when the lock is not required. For a bad/invalid option
 * buffer also 0 is returned and the option processing routines will send
 * the appropriate error T_ERROR_ACK.
 */
static int
ip_optmgmt_writer(mblk_t *mp)
{
	uchar_t *optcp, *next_optcp, *opt_endcp;
	struct opthdr *opt;
	struct T_opthdr *topt;
	int opthdr_len;
	t_uscalar_t optname, optlevel;
	struct T_optmgmt_req *tor = (struct T_optmgmt_req *)mp->b_rptr;

	optcp = (uchar_t *)((uchar_t *)mi_offset_param(mp,
	    tor->OPT_offset, tor->OPT_length));
	/*
	 * XXX This code needs restructuring and enhanced to
	 * to use macros after we have something equivalent
	 * to _TPI_TOPT_VALID (used for 'struct T_opthdr') but for
	 * use with 'struct opthdr' option buffer in headers.
	 * Some other option macros may also be needed.
	 */
	if (! __TPI_SIZE_ISALIGNED(optcp))
		return (0);	/* misaligned buffer */
	opt_endcp = (uchar_t *)((uchar_t *)optcp + tor->OPT_length);
	if (tor->PRIM_type == T_OPTMGMT_REQ)
		opthdr_len = sizeof (struct T_opthdr);
	else {		/* O_OPTMGMT_REQ */
		ASSERT(tor->PRIM_type == T_SVR4_OPTMGMT_REQ);
		opthdr_len = sizeof (struct opthdr);
	};
	for (; optcp < opt_endcp; optcp = next_optcp) {
		if (optcp + opthdr_len > opt_endcp)
			return (0); /* not enough option header */
		if (tor->PRIM_type == T_OPTMGMT_REQ) {
			topt = (struct T_opthdr *)optcp;
			optlevel = topt->level;
			optname = topt->name;
			next_optcp = optcp + _TPI_ALIGN_TOPT(topt->len);
		} else {
			opt = (struct opthdr *)optcp;
			optlevel = opt->level;
			optname = opt->name;
			next_optcp = optcp + opthdr_len +
			    _TPI_ALIGN_OPT(opt->len);
		}
		/* should change to use _TPI_TOPT_VALID ? XXX */
		if ((next_optcp < optcp) || /* wraparound pointer space */
		    ((next_optcp >= opt_endcp) && /* last option bad len */
			((next_optcp - opt_endcp) >= __TPI_ALIGN_SIZE)))
			return (0); /* bad option buffer */
		switch (optlevel) {
		case SOL_SOCKET:
			/*
			 * we return 0 here as ip_opt_set serializes
			 * the setting of these options thru ipc_reflock.
			 *
			 * XXX NOTE : When IP becomes MT hot, this needs
			 * to be re-visited.
			 */
			switch (optname) {
			case SO_BROADCAST:
			case SO_USELOOPBACK:
			case SO_DONTROUTE:
			case SO_REUSEADDR:
			case SO_PROTOTYPE:
				return (0);
			default:
				break;
			}
			break;
		case IPPROTO_IP:
		case IPPROTO_IPV6:
			return (1);
		default:
			if ((optlevel >= MIB2_RANGE_START &&
			    optlevel <= MIB2_RANGE_END) ||
			    (optlevel >= EXPER_RANGE_START &&
				optlevel <= MIB2_RANGE_END)) {
				/* For snmpcom_req */
				return (0);
			}
			break;
		}
	}
	return (0);
}

/* Named Dispatch routine to get a current value out of our parameter table. */
/* ARGSUSED */
static int
ip_param_get(queue_t *q, mblk_t *mp, void *cp)
{
	ipparam_t *ippa = (ipparam_t *)cp;

	(void) mi_mpprintf(mp, "%d", ippa->ip_param_value);
	return (0);
}

/* ARGSUSED */
static int
ip_forward_get(queue_t *q, mblk_t *mp, void *cp)
{
	(void) mi_mpprintf(mp, "%d", ip_g_forward);
	return (0);
}

/* ARGSUSED */
static int
ip_forward_set(queue_t *q, mblk_t *mp, char *value, void *cp)
{
	char *end;
	int new_value;
	extern int ill_forward_set(queue_t *, mblk_t *, char *, void *);
	ill_t *walker;

	new_value = (int)mi_strtol(value, &end, 10);
	if (end == value || new_value < 0 || new_value > 1)
		return (EINVAL);

	ip_g_forward = new_value;

	/*
	 * Regardless of the current value of ip_forwarding, set all per-ill
	 * values of ip_forwarding to the value being set.
	 *
	 * Bring all the ill's up to date with the new global value.
	 */
	for (walker = ill_g_head; walker != NULL; walker = walker->ill_next)
		(void) ill_forward_set(q, NULL, value, (void *)walker);

	return (0);
}

/*
 * Walk through the param array specified registering each element with the
 * Named Dispatch handler.
 */
static boolean_t
ip_param_register(ipparam_t *ippa, size_t cnt)
{
	for (; cnt-- > 0; ippa++) {
		if (ippa->ip_param_name && ippa->ip_param_name[0]) {
			if (!nd_load(&ip_g_nd, ippa->ip_param_name,
			    ip_param_get, ip_param_set, (caddr_t)ippa)) {
				nd_free(&ip_g_nd);
				return (B_FALSE);
			}
		}
	}

	if (!nd_load(&ip_g_nd, "ip_forwarding", ip_forward_get, ip_forward_set,
	    NULL)) {
		nd_free(&ip_g_nd);
		return (B_FALSE);
	}

	if (!nd_load(&ip_g_nd, "ip_ill_status", ip_ill_report, NULL,
	    NULL)) {
		nd_free(&ip_g_nd);
		return (B_FALSE);
	}

	if (!nd_load(&ip_g_nd, "ip_ipif_status", ip_ipif_report, NULL,
	    NULL)) {
		nd_free(&ip_g_nd);
		return (B_FALSE);
	}

	if (!nd_load(&ip_g_nd, "ipv4_ire_status", ip_ire_report, NULL,
	    NULL)) {
		nd_free(&ip_g_nd);
		return (B_FALSE);
	}

	if (!nd_load(&ip_g_nd, "ipv6_ire_status", ip_ire_report_v6, NULL,
	    NULL)) {
		nd_free(&ip_g_nd);
		return (B_FALSE);
	}

	if (!nd_load(&ip_g_nd, "ip_ipc_status", ip_ipc_report, NULL,
	    NULL)) {
		nd_free(&ip_g_nd);
		return (B_FALSE);
	}

	if (!nd_load(&ip_g_nd, "ip_rput_pullups", nd_get_long, nd_set_long,
	    (caddr_t)&ip_rput_pullups)) {
		nd_free(&ip_g_nd);
		return (B_FALSE);
	}

	if (!nd_load(&ip_g_nd, "ip_enable_group_ifs", ifgrp_get, ifgrp_set,
	    NULL)) {
		nd_free(&ip_g_nd);
		return (B_FALSE);
	}
	if (!nd_load(&ip_g_nd, "ifgrp_status", ifgrp_report, NULL,
	    NULL)) {
		nd_free(&ip_g_nd);
		return (B_FALSE);
	}

	if (!nd_load(&ip_g_nd, "ip_ndp_cache_report", ndp_report,
	    NULL, NULL)) {
		nd_free(&ip_g_nd);
		return (B_FALSE);
	}
	if (!nd_load(&ip_g_nd, "ip_proxy_status", ip_proxy_addr_report,
	    NULL, NULL)) {
		nd_free(&ip_g_nd);
		return (B_FALSE);
	}
	if (!nd_load(&ip_g_nd, "ip_srcid_status", ip_srcid_report,
	    NULL, NULL)) {
		nd_free(&ip_g_nd);
		return (B_FALSE);
	}
	return (B_TRUE);
}

/* Named Dispatch routine to negotiate a new value for one of our parameters. */
/* ARGSUSED */
static int
ip_param_set(queue_t *q, mblk_t *mp, char *value, void *cp)
{
	char	*end;
	int	new_value;
	ipparam_t	*ippa = (ipparam_t *)cp;

	new_value = (int)mi_strtol(value, &end, 10);
	if (end == value || new_value < ippa->ip_param_min ||
	    new_value > ippa->ip_param_max)
		return (EINVAL);
	ippa->ip_param_value = new_value;
	return (0);
}

/*
 * Handles both IPv4 and IPv6 reassembly - doing the out-of-order cases,
 * When an ipf is passed here for the first time, if
 * we already have in-order fragments on the queue, we convert from the fast-
 * path reassembly scheme to the hard-case scheme.  From then on, additional
 * fragments are reassembled here.  We keep track of the start and end offsets
 * of each piece, and the number of holes in the chain.  When the hole count
 * goes to zero, we are done!
 *
 * The ipf_count will be updated to account for any mblk(s) added (pointed to
 * by mp) or subtracted (freeb()ed dups), upon return the caller must update
 * ipfb_count and ill_frag_count by the difference of ipf_count before and
 * after the call to ip_reassemble().
 */
boolean_t
ip_reassemble(mblk_t *mp, ipf_t *ipf, uint_t start, boolean_t more,
    uint_t stripped_hdr_len, ill_t *ill)
{
	uint_t	end;
	mblk_t	*next_mp;
	mblk_t	*mp1;
	uint_t	offset;

	if (ipf->ipf_end) {
		/*
		 * We were part way through in-order reassembly, but now there
		 * is a hole.  We walk through messages already queued, and
		 * mark them for hard case reassembly.  We know that up till
		 * now they were in order starting from offset zero.
		 */
		offset = 0;
		for (mp1 = ipf->ipf_mp->b_cont; mp1; mp1 = mp1->b_cont) {
			IP_REASS_SET_START(mp1, offset);
			if (offset == 0) {
				ASSERT(ipf->ipf_nf_hdr_len != 0);
				offset = -ipf->ipf_nf_hdr_len;
			}
			offset += mp1->b_wptr - mp1->b_rptr;
			IP_REASS_SET_END(mp1, offset);
		}
		/* One hole at the end. */
		ipf->ipf_hole_cnt = 1;
		/* Brand it as a hard case, forever. */
		ipf->ipf_end = 0;
	}
	/* Walk through all the new pieces. */
	do {
		end = start + (mp->b_wptr - mp->b_rptr);
		if (start == 0) {
			/* First segment */
			ASSERT(ipf->ipf_nf_hdr_len != 0);
			end -= ipf->ipf_nf_hdr_len;
		}
		next_mp = mp->b_cont;
		if (start == end) {
			/* Empty.  Blast it. */
			IP_REASS_SET_START(mp, 0);
			IP_REASS_SET_END(mp, 0);
			/*
			 * If the ipf points to the mblk we are about to free,
			 * update ipf to point to the next mblk (or NULL
			 * if none).
			 */
			if (ipf->ipf_mp->b_cont == mp)
				ipf->ipf_mp->b_cont = next_mp;
			freeb(mp);
			continue;
		}
		/* Add in byte count */
		ipf->ipf_count += mp->b_datap->db_lim - mp->b_datap->db_base;
		mp->b_cont = NULL;
		IP_REASS_SET_START(mp, start);
		IP_REASS_SET_END(mp, end);
		if (!ipf->ipf_tail_mp) {
			ipf->ipf_tail_mp = mp;
			ipf->ipf_mp->b_cont = mp;
			if (start == 0 || !more) {
				ipf->ipf_hole_cnt = 1;
				/*
				 * if the first fragment comes in more than one
				 * mblk, this loop will be executed for each
				 * mblk. Need to adjust hole count so exiting
				 * this routine will leave hole count at 1.
				 */
				if (next_mp)
					ipf->ipf_hole_cnt++;
			} else
				ipf->ipf_hole_cnt = 2;
			ipf->ipf_stripped_hdr_len = stripped_hdr_len;
			continue;
		}
		/* New stuff at or beyond tail? */
		offset = IP_REASS_END(ipf->ipf_tail_mp);
		if (start >= offset) {
			/* Link it on end. */
			ipf->ipf_tail_mp->b_cont = mp;
			ipf->ipf_tail_mp = mp;
			if (more) {
				if (start != offset)
					ipf->ipf_hole_cnt++;
			} else if (start == offset && next_mp == NULL)
					ipf->ipf_hole_cnt--;
			continue;
		}
		mp1 = ipf->ipf_mp->b_cont;
		offset = IP_REASS_START(mp1);
		/* New stuff at the front? */
		if (start < offset) {
			ipf->ipf_stripped_hdr_len = stripped_hdr_len;
			if (start == 0) {
				if (end >= offset) {
					/* Nailed the hole at the begining. */
					ipf->ipf_hole_cnt--;
				}
			} else if (end < offset) {
				/*
				 * A hole, stuff, and a hole where there used
				 * to be just a hole.
				 */
				ipf->ipf_hole_cnt++;
			}
			mp->b_cont = mp1;
			/* Check for overlap. */
			while (end > offset) {
				if (end < IP_REASS_END(mp1)) {
					mp->b_wptr -= end - offset;
					IP_REASS_SET_END(mp, offset);
					if (ill->ill_isv6) {
						BUMP_MIB(ill->ill_ip6_mib->
						    ipv6ReasmPartDups);
					} else {
						BUMP_MIB(ip_mib.
						    ipReasmPartDups);
					}
					break;
				}
				/* Did we cover another hole? */
				if ((mp1->b_cont &&
				    IP_REASS_END(mp1) !=
				    IP_REASS_START(mp1->b_cont) &&
				    end >= IP_REASS_START(mp1->b_cont)) ||
				    !more) {
					ipf->ipf_hole_cnt--;
				}
				/* Clip out mp1. */
				if ((mp->b_cont = mp1->b_cont) == NULL) {
					/*
					 * After clipping out mp1, this guy
					 * is now hanging off the end.
					 */
					ipf->ipf_tail_mp = mp;
				}
				IP_REASS_SET_START(mp1, 0);
				IP_REASS_SET_END(mp1, 0);
				/* Subtract byte count */
				ipf->ipf_count -= mp1->b_datap->db_lim -
				    mp1->b_datap->db_base;
				freeb(mp1);
				if (ill->ill_isv6) {
					BUMP_MIB(ill->ill_ip6_mib->
					    ipv6ReasmPartDups);
				} else {
					BUMP_MIB(ip_mib.ipReasmPartDups);
				}
				mp1 = mp->b_cont;
				if (!mp1)
					break;
				offset = IP_REASS_START(mp1);
			}
			ipf->ipf_mp->b_cont = mp;
			continue;
		}
		/*
		 * The new piece starts somewhere between the start of the head
		 * and before the end of the tail.
		 */
		for (; mp1; mp1 = mp1->b_cont) {
			offset = IP_REASS_END(mp1);
			if (start < offset) {
				if (end <= offset) {
					/* Nothing new. */
					IP_REASS_SET_START(mp, 0);
					IP_REASS_SET_END(mp, 0);
					/* Subtract byte count */
					ipf->ipf_count -= mp->b_datap->db_lim -
					    mp->b_datap->db_base;
					freeb(mp);
					if (ill->ill_isv6) {
						BUMP_MIB(ill->ill_ip6_mib->
						    ipv6ReasmDuplicates);
					} else {
						BUMP_MIB(ip_mib.
						    ipReasmDuplicates);
					}
					break;
				}
				/*
				 * Trim redundant stuff off beginning of new
				 * piece.
				 */
				IP_REASS_SET_START(mp, offset);
				mp->b_rptr += offset - start;
				if (ill->ill_isv6) {
					BUMP_MIB(ill->ill_ip6_mib->
					    ipv6ReasmPartDups);
				} else {
					BUMP_MIB(ip_mib.ipReasmPartDups);
				}
				start = offset;
				if (!mp1->b_cont) {
					/*
					 * After trimming, this guy is now
					 * hanging off the end.
					 */
					mp1->b_cont = mp;
					ipf->ipf_tail_mp = mp;
					if (!more) {
						ipf->ipf_hole_cnt--;
					}
					break;
				}
			}
			if (start >= IP_REASS_START(mp1->b_cont))
				continue;
			/* Fill a hole */
			if (start > offset)
				ipf->ipf_hole_cnt++;
			mp->b_cont = mp1->b_cont;
			mp1->b_cont = mp;
			mp1 = mp->b_cont;
			offset = IP_REASS_START(mp1);
			if (end >= offset) {
				ipf->ipf_hole_cnt--;
				/* Check for overlap. */
				while (end > offset) {
					if (end < IP_REASS_END(mp1)) {
						mp->b_wptr -= end - offset;
						IP_REASS_SET_END(mp, offset);
						/*
						 * TODO we might bump
						 * this up twice if there is
						 * overlap at both ends.
						 */
						if (ill->ill_isv6) {
							BUMP_MIB(
							    ill->ill_ip6_mib->
							    ipv6ReasmPartDups);
						} else {
							BUMP_MIB(ip_mib.
							    ipReasmPartDups);
						}
						break;
					}
					/* Did we cover another hole? */
					if ((mp1->b_cont &&
					    IP_REASS_END(mp1)
					    != IP_REASS_START(mp1->b_cont) &&
					    end >=
					    IP_REASS_START(mp1->b_cont)) ||
					    !more) {
						ipf->ipf_hole_cnt--;
					}
					/* Clip out mp1. */
					if ((mp->b_cont = mp1->b_cont) ==
					    NULL) {
						/*
						 * After clipping out mp1,
						 * this guy is now hanging
						 * off the end.
						 */
						ipf->ipf_tail_mp = mp;
					}
					IP_REASS_SET_START(mp1, 0);
					IP_REASS_SET_END(mp1, 0);
					/* Subtract byte count */
					ipf->ipf_count -=
					    mp1->b_datap->db_lim -
					    mp1->b_datap->db_base;
					freeb(mp1);
					if (ill->ill_isv6) {
						BUMP_MIB(ill->ill_ip6_mib->
						    ipv6ReasmPartDups);
					} else {
						BUMP_MIB(ip_mib.
						    ipReasmPartDups);
					}
					mp1 = mp->b_cont;
					if (!mp1)
						break;
					offset = IP_REASS_START(mp1);
				}
			}
			break;
		}
	} while (start = end, mp = next_mp);
	/* Still got holes? */
	if (ipf->ipf_hole_cnt)
		return (B_FALSE);
	/* Clean up overloaded fields to avoid upstream disasters. */
	for (mp1 = ipf->ipf_mp->b_cont; mp1; mp1 = mp1->b_cont) {
		IP_REASS_SET_START(mp1, 0);
		IP_REASS_SET_END(mp1, 0);
	}
	return (B_TRUE);
}

/* Read side put procedure.  Packets coming from the wire arrive here. */
void
ip_rput(queue_t *q, mblk_t *mp)
{
	ipaddr_t	dst;
	ire_t	*ire = NULL;
	mblk_t	*mp1;
	ipha_t	*ipha;
	ill_t	*ill;
	uint_t	pkt_len;
	ssize_t	len;
	uint_t	opt_len;
	int	ll_multicast;
	struct	iocblk *iocp;

	TRACE_1(TR_FAC_IP, TR_IP_RPUT_START, "ip_rput_start: q %p", q);

#define	rptr	((uchar_t *)ipha)

	ill = (ill_t *)q->q_ptr;
	if (ill->ill_ipif == NULL) {
		/*
		 * Things are opening or closing - only accept DLPI
		 * ack messages
		 */
		if (mp->b_datap->db_type != M_PCPROTO) {
			/* Clear b_next - used in M_BREAK messages */
			mp->b_next = NULL;
			/* clear b_prev - used by ip_mroute_decap */
			mp->b_prev = NULL;
			freemsg(mp);
			TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
			    "ip_rput_end: q %p (%S)", q, "uninit");
			return;
		}
	}

	/*
	 * ip_rput fast path
	 */

	/* mblk type is not M_DATA */
	if (mp->b_datap->db_type != M_DATA)
		goto notdata;

	/*
	 * if db_ref > 1 then copymsg and free original. Packet may be
	 * changed and do not want other entity who has a reference to this
	 * message to trip over the changes. This is a blind change because
	 * trying to catch all places that might change packet is too
	 * difficult (since it may be a module above this one).
	 *
	 * This corresponds to the fast path case, where we have a chain of
	 * M_DATA mblks.  We check the db_ref count of only the 1st data block
	 * in the mblk chain. There doesn't seem to be a reason why a device
	 * driver would send up data with varying db_ref counts in the mblk
	 * chain. In any case the Fast path is a private interface, and our
	 * drivers don't do such a thing. Given the above assumption, there is
	 * no need to walk down the entire mblk chain (which could have a
	 * potential performance problem)
	 */
	if (mp->b_datap->db_ref > 1) {
		mblk_t  *mp1;

		mp1 = copymsg(mp);
		if (mp1 == NULL) {
			/* Clear b_next - used in M_BREAK messages */
			mp->b_next = NULL;
			/* clear b_prev - used by ip_mroute_decap */
			mp->b_prev = NULL;
			freemsg(mp);
			BUMP_MIB(ip_mib.ipInDiscards);
			TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
			    "ip_rput_end: q %p (%S)", q, "copymsg");
			return;
		}
		/* Copy b_next - used in M_BREAK messages */
		mp1->b_next = mp->b_next;
		mp->b_next = NULL;
		/* Copy b_prev - used by ip_mroute_decap */
		mp1->b_prev = mp->b_prev;
		mp->b_prev = NULL;
		freemsg(mp);
		mp = mp1;
	}

	ipha = (ipha_t *)mp->b_rptr;
	ll_multicast = 0;
	BUMP_MIB(ip_mib.ipInReceives);
	len = mp->b_wptr - rptr;

	/* IP header ptr not aligned? */
	if (!OK_32PTR(rptr))
		goto notaligned;

	/* IP header not complete in first mblk */
	if (len < IP_SIMPLE_HDR_LENGTH)
		goto notaligned;

	/* multiple mblk or too short */
	if (len - ntohs(ipha->ipha_length))
		goto multimblk;

	/* IP version bad or there are IP options */
	if (ipha->ipha_version_and_hdr_length - (uchar_t)IP_SIMPLE_HDR_VERSION)
		goto ipoptions;

	dst = ipha->ipha_dst;

	/*
	 * If rsvpd is running, let RSVP daemon handle its processing
	 * and forwarding of RSVP multicast/unicast packets.
	 * If rsvpd is not running but mrouted is running, RSVP
	 * multicast packets are forwarded as multicast traffic
	 * and RSVP unicast packets are forwarded by unicast router.
	 * If neither rsvpd nor mrouted is running, RSVP multicast
	 * packets are not forwarded, but the unicast packets are
	 * forwarded like unicast traffic.
	 */

	if (ipha->ipha_protocol == IPPROTO_RSVP &&
	    ipc_proto_fanout[IPPROTO_RSVP].icf_ipc != NULL) {
		/* RSVP packet and rsvpd running. Treat as ours */
		goto ours;
	}

	/* packet is multicast */
	if (CLASSD(dst))
		goto multicast;

	ire = ire_cache_lookup(dst);

	if (!ire)
		goto noirefound;

	/* broadcast? */
	if (ire->ire_type == IRE_BROADCAST)
		goto broadcast;

	/* fowarding? */
	if (ire->ire_stq != 0)
		goto forward;

	/* packet not for us */
	if (ire->ire_rfq != q)
		goto notforus;

	TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
	    "ip_rput_end: q %p (%S)", q, "end");

	ip_rput_local(q, mp, ipha, ire, 0);
	IRE_REFRELE(ire);
	return;

notdata:
	/*
	 * if db_ref > 1 then copymsg and free original. Packet may be
	 * changed and do not want other entity who has a reference to this
	 * message to trip over the changes. This is a blind change because
	 * trying to catch all places that might change packet is too
	 * difficult (since it may be a module above this one)
	 *
	 * This corresponds to the non-fast path case. We walk down the full
	 * chain in this case, and check the db_ref count of all the dblks,
	 * and do a copymsg if required. It is possible that the db_ref counts
	 * of the data blocks in the mblk chain can be different.
	 * For Example, we can get a DL_UNITDATA_IND(M_PROTO) with a db_ref
	 * count of 1, followed by a M_DATA block with a ref count of 2, if
	 * 'snoop' is running.
	 */
	{
		mblk_t *mp1, *from_mp, *to_mp;
		boolean_t must_copy = B_FALSE;

		for (mp1 = mp; mp1 != NULL; mp1 = mp1->b_cont) {
			if (mp1->b_datap->db_ref > 1) {
				must_copy = B_TRUE;
				break;
			}
		}

		if (must_copy) {
			mp1 = copymsg(mp);
			if (mp1 == NULL) {
				for (mp1 = mp; mp1 != NULL;
				    mp1 = mp1->b_cont) {
					mp1->b_next = NULL;
					mp1->b_prev = NULL;
				}
				freemsg(mp);
				BUMP_MIB(ip_mib.ipInDiscards);
				TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
				    "ip_rput_end: q %p (%S)", q, "copymsg");
				return;
			}
			for (from_mp = mp, to_mp = mp1; from_mp != NULL;
			    from_mp = from_mp->b_cont, to_mp = to_mp->b_cont) {
				/* Copy b_next - used in M_BREAK messages */
				to_mp->b_next = from_mp->b_next;
				from_mp->b_next = NULL;
				/* Copy b_prev - used by ip_mroute_decap */
				to_mp->b_prev = from_mp->b_prev;
				from_mp->b_prev = NULL;
			}
			freemsg(mp);
			mp = mp1;
		}
	}
	ipha = (ipha_t *)mp->b_rptr;
	switch (mp->b_datap->db_type) {
	case M_DATA:
		/*
		 * A fastpath device may send us M_DATA.  Fastpath messages
		 * start with the network header and are never used for packets
		 * that were broadcast or multicast by the link layer.
		 *
		 * M_DATA are also used to pass back decapsulated packets
		 * from ip_mroute_decap. For those packets we will force
		 * ll_multicast to one below.
		 */
		ll_multicast = 0;
		break;
	case M_PROTO:
	case M_PCPROTO:
		if (((dl_unitdata_ind_t *)rptr)->dl_primitive !=
			DL_UNITDATA_IND) {
			/* Go handle anything other than data elsewhere. */
			ip_rput_dlpi(q, mp);
			TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
			    "ip_rput_end: q %p (%S)", q, "proto");
			return;
		}
		ll_multicast = ((dl_unitdata_ind_t *)rptr)->dl_group_address;
		/* Ditch the DLPI header. */
		mp1 = mp;
		mp = mp->b_cont;
		freeb(mp1);
		ipha = (ipha_t *)mp->b_rptr;
		break;
	case M_BREAK:
		/*
		 * A packet arrives as M_BREAK following a cycle through
		 * ip_rput, ip_newroute, ... and finally ire_add_then_send.
		 * This is an IP datagram sans lower level header.
		 * M_BREAK are also used to pass back in multicast packets
		 * that are encapsulated with a source route.
		 */
		/* Ditch the M_BREAK mblk */
		mp1 = mp->b_cont;
		freeb(mp);
		mp = mp1;
		ipha = (ipha_t *)mp->b_rptr;
		/* Get the number of words of IP options in the IP header. */
		opt_len = ipha->ipha_version_and_hdr_length -
		    IP_SIMPLE_HDR_VERSION;
		dst = (ipaddr_t)mp->b_next;
		mp->b_next = NULL;
		ll_multicast = 0;
		goto hdrs_ok;
	case M_IOCACK:
		iocp = (struct iocblk *)mp->b_rptr;
		switch (iocp->ioc_cmd) {
		case DL_IOC_HDR_INFO:
			ill = (ill_t *)q->q_ptr;
			ill_fastpath_ack(ill, mp);
			return;
		case SIOCSTUNPARAM:
		case SIOCGTUNPARAM:
			/* Go through become_exlusive */
			break;
		default:
			putnext(q, mp);
			TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
			    "ip_rput_end: q %p (%S)", q, "iocack");
			return;
		}
		/* FALLTHRU */
	case M_ERROR:
	case M_HANGUP:
		become_exclusive(q, mp, ip_rput_other);
		TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
		    "ip_rput_end: q %p (%S)", q, "ip_rput_other");
		return;
	case M_CTL: {
		inetcksum_t *ick = (inetcksum_t *)mp->b_rptr;

		if ((mp->b_wptr - mp->b_rptr) == sizeof (*ick) &&
		    ick->ick_magic == ICK_M_CTL_MAGIC) {
			ill = (ill_t *)q->q_ptr;
			ill->ill_ick = *ick;
			freemsg(mp);
			return;
		} else {
			putnext(q, mp);
			TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
			    "ip_rput_end: q %p (%S)", q, "default");
			return;
		}
	}
	case M_IOCNAK:
		iocp = (struct iocblk *)mp->b_rptr;
		switch (iocp->ioc_cmd) {
		case DL_IOC_HDR_INFO:
		case SIOCSTUNPARAM:
		case SIOCGTUNPARAM:
			become_exclusive(q, mp, ip_rput_other);
			TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
			    "ip_rput_end: q %p (%S)", q, "ip_rput_other");
			return;
		default:
			break;
		}
		/* FALLTHRU */
	default:
		putnext(q, mp);
		TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
		    "ip_rput_end: q %p (%S)", q, "default");
		return;
	}
	/*
	 * Make sure that we at least have a simple IP header accessible and
	 * that we have at least long-word alignment.  If not, try a pullup.
	 */
	BUMP_MIB(ip_mib.ipInReceives);
	len = mp->b_wptr - rptr;
	if (!OK_32PTR(rptr) || len < IP_SIMPLE_HDR_LENGTH) {
notaligned:
		/* Guard against bogus device drivers */
		if (len < 0) {
			/* clear b_prev - used by ip_mroute_decap */
			mp->b_prev = NULL;
			goto in_hdr_errors;
		}
		if (ip_rput_pullups++ == 0) {
			ill = (ill_t *)q->q_ptr;
			(void) mi_strlog(q, 1, SL_ERROR|SL_TRACE,
			    "ip_rput: %s forced us to pullup pkt,"
			    " hdr len %ld, hdr addr %p",
			    ill->ill_name, len, ipha);
		}
		if (!pullupmsg(mp, IP_SIMPLE_HDR_LENGTH)) {
			/* clear b_prev - used by ip_mroute_decap */
			mp->b_prev = NULL;
			BUMP_MIB(ip_mib.ipInDiscards);
			goto drop_pkt;
		}
		ipha = (ipha_t *)mp->b_rptr;
		len = mp->b_wptr - rptr;
	}
multimblk:
ipoptions:
	/* Assume no IPv6 packets arrive over the IPv4 queue */
	if ((ipha->ipha_version_and_hdr_length & 0xf0) == 0x60) {
		BUMP_MIB(ip_mib.ipInIPv6);
		goto drop_pkt;
	}
	pkt_len = ntohs(ipha->ipha_length);
	len -= pkt_len;
	if (len) {
		/*
		 * Make sure we have data length consistent with the IP header.
		 */
		if (!mp->b_cont) {
			if (len < 0 || pkt_len < IP_SIMPLE_HDR_LENGTH)
				goto in_hdr_errors;
			mp->b_wptr = rptr + pkt_len;
		} else if (len += msgdsize(mp->b_cont)) {
			if (len < 0 || pkt_len < IP_SIMPLE_HDR_LENGTH)
				goto in_hdr_errors;
			(void) adjmsg(mp, -len);
		}
	}
	/* Get the number of words of IP options in the IP header. */
	opt_len = ipha->ipha_version_and_hdr_length - IP_SIMPLE_HDR_VERSION;
	if (opt_len) {
		/* IP Options present!  Validate and process. */
		if (opt_len > (15 - IP_SIMPLE_HDR_LENGTH_IN_WORDS)) {
			/* clear b_prev - used by ip_mroute_decap */
			mp->b_prev = NULL;
			goto in_hdr_errors;
		}
		/*
		 * Recompute complete header length and make sure we
		 * have access to all of it.
		 */
		len = ((size_t)opt_len + IP_SIMPLE_HDR_LENGTH_IN_WORDS) << 2;
		if (len > (mp->b_wptr - rptr)) {
			if (len > pkt_len) {
				/* clear b_prev - used by ip_mroute_decap */
				mp->b_prev = NULL;
				goto in_hdr_errors;
			}
			if (!pullupmsg(mp, len)) {
				BUMP_MIB(ip_mib.ipInDiscards);
				/* clear b_prev - used by ip_mroute_decap */
				mp->b_prev = NULL;
				goto drop_pkt;
			}
			ipha = (ipha_t *)mp->b_rptr;
		}
		/*
		 * Go off to ip_rput_options which returns the next hop
		 * destination address, which may have been affected
		 * by source routing.
		 */
		if (ip_rput_options(q, mp, ipha, &dst) == -1) {
			TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
			    "ip_rput_end: q %p (%S)", q, "baddst");
			return;
		}
	} else {
		/* No options.  We know we have alignment so grap the dst. */
		dst = ipha->ipha_dst;
	}
hdrs_ok:;
	/*
	* If rsvpd is running, let RSVP daemon handle its processing
	* and forwarding of RSVP multicast/unicast packets.
	* If rsvpd is not running but mrouted is running, RSVP
	* multicast packets are forwarded as multicast traffic
	* and RSVP unicast packets are forwarded by unicast router.
	* If neither rsvpd nor mrouted is running, RSVP multicast
	* packets are not forwarded, but the unicast packets are
	* forwarded like unicast traffic.
	*/
	if (ipha->ipha_protocol == IPPROTO_RSVP &&
	    ipc_proto_fanout[IPPROTO_RSVP].icf_ipc != NULL) {
		/* RSVP packet and rsvpd running. Treat as ours */
		goto ours;
	}
	if (CLASSD(dst)) {
multicast:
		ill = (ill_t *)q->q_ptr;
		if (ip_g_mrouter) {
			int retval;

			/*
			 * Clear the indication that this may have a hardware
			 * checksum as we are not using it.
			 */
			mp->b_ick_flag &= ~ICK_VALID;
			retval = ip_mforward(ill, ipha, mp);
			/* ip_mforward updates mib variables if needed */
			/* clear b_prev - used by ip_mroute_decap */
			mp->b_prev = NULL;

			switch (retval) {
			case 0:
				/*
				 * pkt is okay and arrived on phyint.
				 *
				 * If we are running as a multicast router
				 * we need to see all IGMP and/or PIM packets.
				 */
				if ((ipha->ipha_protocol == IPPROTO_IGMP) ||
				    (ipha->ipha_protocol == IPPROTO_PIM))
					goto ours;
				break;
			case -1:
				/* pkt is mal-formed, toss it */
				goto drop_pkt;
			case 1:
				/* pkt is okay and arrived on a tunnel */
				/*
				 * If we are running a multicast router
				 *  we need to see all igmp packets.
				 */
				if (ipha->ipha_protocol == IPPROTO_IGMP)
					goto ours;

				goto drop_pkt;
			}
		}

		if (ilm_lookup_ill(ill, dst) == NULL) {
			/*
			 * This might just be caused by the fact that
			 * multiple IP Multicast addresses map to the same
			 * link layer multicast - no need to increment counter!
			 */
			freemsg(mp);
			TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
			    "ip_rput_end: q %p (%S)", q, "badilm");
			return;
		}
	ours:
		ip2dbg(("ip_rput: multicast for us: 0x%x\n", ntohl(dst)));
		/*
		 * This assume the we deliver to all streams for multicast
		 * and broadcast packets.
		 * We have to force ll_multicast to 1 to handle the
		 * M_DATA messages passed in from ip_mroute_decap.
		 */
		dst = INADDR_BROADCAST;
		ll_multicast = 1;
	}

	ire = ire_cache_lookup(dst);
	if (!ire) {
noirefound:
		/*
		 * No IRE for this destination, so it can't be for us.
		 * Unless we are forwarding, drop the packet.
		 * We have to let source routed packets through
		 * since we don't yet know if they are 'ping -l'
		 * packets i.e. if they will go out over the
		 * same interface as they came in on.
		 */
		if (ll_multicast)
			goto drop_pkt;
		if ((!ill->ill_forwarding) && !ip_source_routed(ipha)) {
			BUMP_MIB(ip_mib.ipForwProhibits);
			goto drop_pkt;
		}

		/* Mark this packet as having originated externally */
		mp->b_prev = (mblk_t *)q;

		/*
		 * Remember the dst, we may have gotten it from
		 * ip_rput_options.
		 */
		mp->b_next = (mblk_t *)dst;

		/*
		 * Clear the indication that this may have a hardware checksum
		 * as we are not using it.
		 */
		mp->b_ick_flag &= ~ICK_VALID;

		/*
		 * Now hand the packet to ip_newroute.
		 * It may come back.
		 */
		ip_newroute(q, mp, dst, NULL);
		TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
		    "ip_rput_end: q %p (%S)", q, "no ire");
		return;
	}
	/*
	 * We now have a final IRE for the destination address.  If ire_stq
	 * is non-NULL, (and it is not a broadcast IRE), then this packet
	 * needs to be forwarded, if allowable.
	 */
	/*
	 * Directed broadcast forwarding: if the packet came in over a
	 * different interface then it is routed out over we can forward it.
	 */
	if (ire->ire_type == IRE_BROADCAST) {
broadcast:
		if (ip_g_forward_directed_bcast && !ll_multicast) {
			/*
			 * Verify that there are not more then one
			 * IRE_BROADCAST with this broadcast address which
			 * has ire_stq set.
			 * TODO: simplify, loop over all IRE's
			 */
			ire_t	*ire1;
			int	num_stq = 0;
			mblk_t	*mp1;

			/* Find the first one with ire_stq set */
			rw_enter(&ire->ire_bucket->irb_lock, RW_READER);
			for (ire1 = ire; ire1 &&
			    !ire1->ire_stq && ire1->ire_addr == ire->ire_addr;
			    ire1 = ire1->ire_next)
				;
			if (ire1) {
				ire_refrele(ire);
				ire = ire1;
				IRE_REFHOLD(ire);
			}

			/* Check if there are additional ones with stq set */
			for (ire1 = ire; ire1; ire1 = ire1->ire_next) {
				if (ire->ire_addr != ire1->ire_addr)
					break;
				if (ire1->ire_stq) {
					num_stq++;
					break;
				}
			}
			rw_exit(&ire->ire_bucket->irb_lock);
			if (num_stq == 1 && ire->ire_stq) {
				ip1dbg(("ip_rput: directed broadcast to 0x%x\n",
				    ntohl(ire->ire_addr)));
				mp1 = copymsg(mp);
				if (mp1) {
					/*
					 * ip_rput_local does not REFRELE the
					 * ire. So, we still have a hold when
					 * the function returns.
					 */
					ip_rput_local(q, mp1,
					    (ipha_t *)mp1->b_rptr, ire, 0);
				}
				/*
				 * Adjust ttl to 2 (1+1 - the forward engine
				 * will decrement it by one.
				 */
				if (ip_csum_hdr(ipha)) {
					BUMP_MIB(ip_mib.ipInCksumErrs);
					goto drop_pkt;
				}
				ipha->ipha_ttl = ip_broadcast_ttl + 1;
				ipha->ipha_hdr_checksum = 0;
				ipha->ipha_hdr_checksum = ip_csum_hdr(ipha);
				goto forward;
			}
			ip1dbg(("ip_rput: NO directed broadcast to 0x%x\n",
			    ntohl(ire->ire_addr)));
		}
	} else if (ire->ire_stq) {
		queue_t	*dev_q;

		if (ll_multicast)
			goto drop_pkt;
	forward:
		/*
		 * Check if we want to forward this one at this time.
		 * We allow source routed packets on a host provided that
		 * the go out the same interface as they came in on.
		 *
		 * XXX To be quicker, we may wish to not chase
		 * pointers (ire->ire_ipif->ipif_ill...) and instead store
		 * the ill_forwarding value in the ire.  An unfortunate
		 * side-effect of this is requiring an ire flush whenever
		 * the ill_forwarding values changes.
		 */
		if ((!ill->ill_forwarding ||
		    !ire->ire_ipif->ipif_ill->ill_forwarding) &&
		    !(ip_source_routed(ipha) && ire->ire_rfq == q)) {
			BUMP_MIB(ip_mib.ipForwProhibits);
			if (ip_source_routed(ipha)) {
				q = WR(q);
				/*
				 * Clear the indication that this may have a
				 * hardware checksum as we are not using it.
				 */
				mp->b_ick_flag &= ~ICK_VALID;
				icmp_unreachable(q, mp,
				    ICMP_SOURCE_ROUTE_FAILED);
				TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
				    "ip_rput_end: q %p (%S)", q,
				    "route failed");
				ire_refrele(ire);
				return;
			}
			goto drop_pkt;
		}

		/*
		 * If proxy listeners are present, determine if there is
		 * a match. ip_rput_local() will find the right client to
		 * send it up to.
		 * Skip source routed packets unless we're at the end of source
		 * route list. This is because our proxying function applies
		 * to only end nodes, not any intermediate router along the
		 * path.
		 */
		if ((ip_proxy_listeners != NULL) &&
		    (ipha->ipha_protocol == IPPROTO_TCP)) {
			uint16_t	dstport;
			uint32_t	iphdrlen;
			uint32_t	ipoptlen;
			ipc_t		*ipc;
			boolean_t	is_frag;

			iphdrlen = ((ipha->ipha_version_and_hdr_length -
			    (IP_VERSION << 4)) << 2);
			ipoptlen = iphdrlen - IP_SIMPLE_HDR_LENGTH;
			/*
			 * If the source route option is present, and there
			 * are more hops to go, continue to forward.
			 */
			if (ipoptlen != 0 &&
			    ip_source_route_more_hops(ipha, ipoptlen))
				goto exit_proxy;

			/*
			 * Fragments other than the first one pose a problem
			 * here. We need the dstport in ULP header to match
			 * proxy listeners. But frags after the first one
			 * don't carry the ULP header.
			 *
			 * This won't be a problem if an all-port listener
			 * exists, since it doesn't care about dstport and
			 * only match dst IP addr. Otherwise we'll let go all
			 * the frags except the first one. Alternatively, one
			 * can set an internal variable
			 * "ip_proxy_take_all_frags" to non-zero, which will
			 * cause dstport to be skipped when matching frags
			 * against proxy listeners.
			 */
			is_frag =
			    ((ntohs(ipha->ipha_fragment_offset_and_flags)
			    & IPH_OFFSET) != 0);
			if (!is_frag && mp->b_wptr - mp->b_rptr < (iphdrlen +
			    TCP_MIN_HEADER_LENGTH)) {
				if (!pullupmsg(mp, iphdrlen +
				    TCP_MIN_HEADER_LENGTH)) {
					BUMP_MIB(ip_mib.ipInDiscards);
					freemsg(mp);
					ire_refrele(ire);
					return;
				}
				ipha = (ipha_t *)mp->b_rptr;
			}
			if (is_frag) {
				/* make fragments match wild-card port */
				dstport = 0;
			} else {
				dstport = *(uint16_t *)(mp->b_rptr + 2 +
				    IPH_HDR_LENGTH(ipha));
			}
			ipc = ip_proxy_match_listener(ipha->ipha_dst,
			    dstport, is_frag);
			if (ipc != NULL) {
				ipc->ipc_proxy_ib_pkt_count++;
				if (is_frag)
					ip_proxy_ib_frag_count++;
				else
					ip_proxy_ib_pkt_count++;
				IPC_REFRELE(ipc);
				ip_rput_local(q, mp, ipha, ire,
				    IP_FF_PROXY_ONLY);
				IRE_REFRELE(ire);
				return;
			}
		}
	exit_proxy:
		/* Packet is being forwarded. Turning off hwcksum flag. */
		mp->b_ick_flag &= ~ICK_VALID;
		if (ire->ire_rfq == q && ip_g_send_redirects) {
			/*
			 * It wants to go out the same way it came in.
			 * Check the source address to see if it originated
			 * on the same logical subnet it is going back out on.
			 * If so, we should be able to send it a redirect.
			 * Avoid sending a redirect if the destination
			 * is directly connected (gw_addr == 0),
			 * or if the packet was source routed out this
			 * interface.
			 */
			ipaddr_t src;
			mblk_t	*mp1;
			ire_t	*src_ire = NULL;

			src = ipha->ipha_src;
			if ((ire->ire_gateway_addr != 0) &&
			    !ip_source_routed(ipha)) {

				src_ire = ire_ftable_lookup(src, 0, 0,
				    IRE_INTERFACE, ire->ire_ipif, NULL, NULL,
				    0, MATCH_IRE_IPIF | MATCH_IRE_TYPE);

				if (src_ire != NULL) {
					/*
					 * The source is directly connected.
					 * Just copy the ip header (which is
					 * in the first mblk)
					 */
					mp1 = copyb(mp);
					if (mp1 != NULL) {
						icmp_send_redirect(WR(q), mp1,
						    ire->ire_gateway_addr);
					}
					ire_refrele(src_ire);
				}
			}
		}

		dev_q = ire->ire_stq->q_next;
		if ((dev_q->q_next || dev_q->q_first) && !canput(dev_q)) {
			BUMP_MIB(ip_mib.ipInDiscards);
			freemsg(mp);
			q = WR(q);
			TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
			    "ip_rput_end: q %p (%S)", q, "flow control");
			ire_refrele(ire);
			return;
		}
		TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
		    "ip_rput_end: q %p (%S)", q, "forward");
		ip_rput_forward(ire, ipha, mp);
		IRE_REFRELE(ire);
		return;
	}
	/*
	 * It's for us.  We need to check to make sure the packet came in
	 * on the queue associated with the destination IRE.
	 * Note that for multicast packets and broadcast packets sent to
	 * a broadcast address which is shared between multiple interfaces
	 * we should not do this since we just got a random broadcast ire.
	 */
	if (ire->ire_rfq != q) {
notforus:
		if (ire->ire_rfq && ire->ire_type != IRE_BROADCAST) {
			/*
			 * This packet came in on an interface other than the
			 * one associated with the destination address.
			 * "Gateway" it to the appropriate interface here.
			 */
			if (ip_strict_dst_multihoming &&
			    (!ill->ill_forwarding ||
				!ire->ire_ipif->ipif_ill->ill_forwarding)) {
				/* Drop packet */
				BUMP_MIB(ip_mib.ipForwProhibits);
				freemsg(mp);
				TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
				    "ip_rput_end: q %p (%S)", q,
				    "strict_dst_multihoming");
				ire_refrele(ire);
				return;
			}

			/*
			 * Change the queue and ip_rput_local will be
			 * called with the right queue. We can do this
			 * because IP is D_MTPERMOD and close is exclusive
			 * for module instances pushed above the driver.
			 * Hence the ire_rfq->q_stream can't disappear while
			 * we are in q->q_stream.
			 */
			q = ire->ire_rfq;
		}
		/* Must be broadcast.  We'll take it. */
	}

	TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
	    "ip_rput_end: q %p (%S)", q, "end");
	ip_rput_local(q, mp, ipha, ire, 0);
	IRE_REFRELE(ire);
	return;

in_hdr_errors:;
	BUMP_MIB(ip_mib.ipInHdrErrors);
	/* FALLTHRU */
drop_pkt:;
	if (ire != NULL)
		ire_refrele(ire);
	ip2dbg(("ip_rput: drop pkt\n"));
	freemsg(mp);
	TRACE_2(TR_FAC_IP, TR_IP_RPUT_END,
	    "ip_rput_end: q %p (%S)", q, "drop");
#undef	rptr
}

/*
 * ip_rput_dlpi is called by ip_rput to handle all DLPI messages other
 * than DL_UNITDATA_IND messages.
 */
void
ip_rput_dlpi(queue_t *q, mblk_t *mp)
{
	dl_ok_ack_t	*dloa = (dl_ok_ack_t *)mp->b_rptr;
	dl_error_ack_t	*dlea = (dl_error_ack_t *)dloa;
	ill_t		*ill;

	ill = (ill_t *)q->q_ptr;
	switch (dloa->dl_primitive) {
	case DL_ERROR_ACK:
		ip0dbg(("ip_rput_dlpi(%s): DL_ERROR_ACK "
		    "for %s(%d), errno %d, unix %d\n",
		    ill->ill_name,
		    dlpi_prim_str((int)dlea->dl_error_primitive),
		    (int)dlea->dl_error_primitive,
		    (int)dlea->dl_errno,
		    (int)dlea->dl_unix_errno));
		switch (dlea->dl_error_primitive) {
		case DL_UNBIND_REQ:
		case DL_ATTACH_REQ:
		case DL_DETACH_REQ:
		case DL_INFO_REQ:
		case DL_BIND_REQ:
		case DL_ENABMULTI_REQ:
		case DL_PHYS_ADDR_REQ:
			become_exclusive(q, mp, ip_rput_dlpi_writer);
			return;
		case DL_DISABMULTI_REQ:
			ip1dbg(("DL_ERROR_ACK to disabmulti\n"));
			freemsg(mp);	/* Don't want to pass this up */
			return;
		default:
			break;
		}
		/*
		 * There is no IOCTL hanging out on these.  Log the
		 * problem and hope someone notices.
		 */
		(void) mi_strlog(q, 1, SL_ERROR|SL_TRACE,
		    "ip_rput_dlpi: %s failed dl_errno %d dl_unix_errno %d",
		    dlpi_prim_str((int)dlea->dl_error_primitive),
		    dlea->dl_errno, dlea->dl_unix_errno);
		freemsg(mp);
		return;
	case DL_INFO_ACK:
	case DL_BIND_ACK:
	case DL_PHYS_ADDR_ACK:
		become_exclusive(q, mp, ip_rput_dlpi_writer);
		return;
	case DL_OK_ACK:
		ip1dbg(("ip_rput: DL_OK_ACK for %s\n",
		    dlpi_prim_str((int)dloa->dl_correct_primitive)));
		switch (dloa->dl_correct_primitive) {
		case DL_UNBIND_REQ:
		case DL_ATTACH_REQ:
		case DL_DETACH_REQ:
			become_exclusive(q, mp, ip_rput_dlpi_writer);
			return;
		case DL_ENABMULTI_REQ:
			if (ill->ill_dlpi_multicast_state == IDMS_INPROGRESS) {
				/* Note: no locking needed. Set to OK once. */
				ill->ill_dlpi_multicast_state = IDMS_OK;
			}
			break;
		}
		break;
	default:
		break;
	}
	freemsg(mp);
}

/*
 * Handling of DLPI messages that require exclusive access to IP
 */
static void
ip_rput_dlpi_writer(queue_t *q, mblk_t	*mp)
{
	dl_ok_ack_t	*dloa = (dl_ok_ack_t *)mp->b_rptr;
	dl_error_ack_t	*dlea = (dl_error_ack_t *)dloa;
	int		err = 0;
	char		*err_str = NULL;
	ill_t		*ill;
	ipif_t		*ipif;
	mblk_t		*mp1 = NULL;

	ill = (ill_t *)q->q_ptr;
	ipif = ill->ill_pending_ipif;
	/* ill_pending_mp and ill_pending_ipif consistency check. */
	ASSERT((ill->ill_pending_mp == NULL &&
	    ill->ill_pending_ipif == NULL) ||
	    (ill->ill_pending_mp != NULL && ill->ill_pending_ipif != NULL));

	switch (dloa->dl_primitive) {
	case DL_ERROR_ACK:
		switch (dlea->dl_error_primitive) {
		case DL_UNBIND_REQ:
		case DL_ATTACH_REQ:
		case DL_DETACH_REQ:
		case DL_INFO_REQ:
			ill_dlpi_done(ill);
			break;
		case DL_PHYS_ADDR_REQ:
			ill_dlpi_done(ill);
			/*
			 * Something went wrong with the DL_PHYS_ADDR_REQ.
			 * We presumably have an IOCTL hanging out waiting
			 * for completion. Find it and complete the IOCTL
			 * with the error noted.
			 * However, ill_dl_phys was called on an ill queue
			 * (from SIOCSLIFNAME) thus ill_pending_q and
			 * ipc_pending_ill are not set. But the ioctl is
			 * known to be pending on ill_wq.
			 */
			err_str = "DL_PHYS_ADDR";
			if (!ill->ill_ifname_pending)
				break;
			ill->ill_ifname_pending = 0;
			mp1 = ill->ill_pending_mp;
			if (mp1) {
				ASSERT(ipif->ipif_ill == ill);
				ill->ill_pending_ipif = NULL;
				ill->ill_pending_mp = NULL;
				ASSERT(ill->ill_pending_q == NULL);
				q = ill->ill_wq;
			}
			break;
		case DL_BIND_REQ:
			ill_dlpi_done(ill);
			if (ill->ill_ifname_pending)
				break;
			/*
			 * Something went wrong with the bind.  We presumably
			 * have an IOCTL hanging out waiting for completion.
			 * Find it, take down the interface that was coming
			 * up, and complete the IOCTL with the error noted.
			 */
			mp1 = ill->ill_pending_mp;
			if (mp1) {
				ASSERT(ipif->ipif_ill == ill);
				ill->ill_pending_ipif = NULL;

				ipif_down(ipif);

				ill->ill_pending_mp = NULL;
				q = ill->ill_pending_q;
				/*
				 * If the client stream is no longer around,
				 * then do not try and send a reply.
				 */
				if (q == NULL) {
					mblk_t	*bp = mp1;

					for (; bp; bp = bp->b_cont) {
						bp->b_prev = NULL;
						bp->b_next = NULL;
					}
					freemsg(mp1);
					freemsg(mp);
					return;
				}
				/*
				 * Clear fields here. Since we are holding
				 * the writers lock we can do it here even
				 * though we do the qreply below.
				 */
				ASSERT(((ipc_t *)q->q_ptr)->ipc_pending_ill
				    == ill);
				ill->ill_pending_q = NULL;
				((ipc_t *)q->q_ptr)->ipc_pending_ill = NULL;
			}
			break;
		case DL_ENABMULTI_REQ:
			ip1dbg(("DL_ERROR_ACK to enabmulti\n"));
			if (ill->ill_dlpi_multicast_state == IDMS_INPROGRESS)
				ill->ill_dlpi_multicast_state = IDMS_FAILED;
			if (ill->ill_dlpi_multicast_state == IDMS_FAILED) {
				ipif_t *ipif;

				printf("ip: joining multicasts failed (%d)"
				    " on %s - will use link layer "
				    "broadcasts for multicast\n",
				    dlea->dl_errno, ill->ill_name);
				for (ipif = ill->ill_ipif;
				    ipif; ipif = ipif->ipif_next) {
					ipif->ipif_flags |= IFF_MULTI_BCAST;
					if (!ipif->ipif_isv6) {
						(void) ipif_arp_up(ipif,
						    ipif->ipif_lcl_addr);
					} else {
						(void) ipif_ndp_up(ipif,
						    &ipif->ipif_v6lcl_addr);
					}
				}
			}
			freemsg(mp);	/* Don't want to pass this up */
			return;
		}
		/* Note the error for IOCTL completion. */
		err = dlea->dl_unix_errno ? dlea->dl_unix_errno : ENXIO;
		if (err_str) {
			/*
			 * Log problem and hope someone notices.
			 */
			(void) mi_strlog(q, 1, SL_ERROR|SL_TRACE,
			"ip_rput_dlpi: %s failed dl_errno %d dl_unix_errno %d",
			    err_str, dlea->dl_errno, dlea->dl_unix_errno);
		}
		break;
	case DL_INFO_ACK:
		/* Call a routine to handle this one. */
		ill_dlpi_done(ill);
		ip_ll_subnet_defaults(ill, mp);
		return;
	case DL_BIND_ACK:
		/*
		 * We should have an IOCTL waiting on this unless
		 * sent by ill_dl_phys.
		 */
		ill_dlpi_done(ill);
		if (ill->ill_ifname_pending)
			break;

		mp1 = ill->ill_pending_mp;
		if (mp1) {
			ASSERT(ipif->ipif_ill == ill);
			q = ill->ill_pending_q;

			ip1dbg(("ip_rput_dlpi: bind_ack %s\n",
			    ill->ill_name));

			ill->ill_pending_mp = NULL;
			/*
			 * Now bring up the resolver, when that is
			 * done we'll create IREs and we are done.
			 */
			if (ill->ill_isv6) {
				/*
				 * Unlike ARP which has to do another bind
				 * and attach  once we get here we are
				 * done with NDP.
				 */
				if ((err = ipif_ndp_up(ipif,
				    &ipif->ipif_v6lcl_addr)) != 0) {
					break;
				}
				/*
				 * This one is complete.
				 * reply to pending ioctl
				 */
				err = ipif_up_done_v6(ipif);
			} else if (ill->ill_net_type == IRE_IF_RESOLVER &&
			    ipif->ipif_lcl_addr != 0 &&
			    (ismod_next(ill->ill_rq, "arp") ||
			    ismod_next(ill->ill_rq, "atmip"))) {
				/*
				 * Need to check for "arp" and some other
				 * resolvers (e.g. "atmip") that may use an
				 * older version of this private interface and
				 * don't know about the AR_DLPIOP_DONE message.
				 *
				 * XXX The check for "atmip" can be removed when
				 * XXX the unbundled Solaris ATM product uses
				 * XXX the new IFF_XRESOLV flag interface. It is
				 * XXX being inserted for Solaris 8 beta only to
				 * XXX make that unbundled product work
				 * XXX temporarily as it is too late for that
				 * XXX product team to do the new plumbing logic
				 * XXX design needed in a timely manner, and we
				 * XXX want it to work at Beta sites.
				 * XXX (atmip impersonates ARP and could
				 * XXX no longer be plumbed)
				 *
				 * Restore the pending mblk so that
				 * the ioctl completes in ip_rput.
				 */
				ill->ill_pending_mp = mp1;
				if (!ipif_arp_up(ipif, ipif->ipif_lcl_addr)) {
					ill->ill_pending_mp = NULL;
					err = ENOMEM;
				} else {
					freemsg(mp);
					return;
				}
			} else {
				/*
				 * This one is complete. Reply to pending ioctl.
				 */
				err = ipif_up_done(ipif);
			}

			/*
			 * This has to be done here - after ill_pending_mp
			 * was reused above.
			 */
			ill->ill_pending_ipif = NULL;
			/*
			 * If the client stream is no longer around,
			 * then do not try and send a reply.
			 */
			if (q == NULL) {
				mblk_t	*bp = mp1;

				for (; bp; bp = bp->b_cont) {
					bp->b_prev = NULL;
					bp->b_next = NULL;
				}
				freemsg(mp1);
				return;
			}
			/*
			 * Clear fields here. Since we are holding
			 * the writers lock we can do it here even
			 * though we do the qreply below.
			 */
			ASSERT(((ipc_t *)q->q_ptr)->ipc_pending_ill == ill);
			ill->ill_pending_q = NULL;
			((ipc_t *)q->q_ptr)->ipc_pending_ill = NULL;
		}
		break;
	case DL_PHYS_ADDR_ACK: {
		/*
		 * We should have an IOCTL waiting on this when request
		 * sent by ill_dl_phys.
		 * However, ill_dl_phys was called on an ill queue (from
		 * SIOCSLIFNAME) thus ill_pending_q and ipc_pending_ill
		 * are not set. But the ioctl is known to be pending
		 * on ill_wq.
		 */
		union DL_primitives *dlp;
		mblk_t  *mp_hw;

		ill_dlpi_done(ill);
		if (!ill->ill_ifname_pending)
			break;
		ill->ill_ifname_pending = 0;
		mp1 = ill->ill_pending_mp;
		if (mp1 != NULL) {
			ASSERT(ill->ill_pending_ipif->ipif_ill == ill);
			ill->ill_pending_mp = NULL;
			ASSERT(ill->ill_pending_q == NULL);
			q = ill->ill_wq;
			ill->ill_pending_ipif = NULL;
		}

		/*
		 * Get the interface token.  If the zeroth interface
		 * address is zero then set the address to the link local
		 * address
		 */
		mp_hw = copyb(mp);
		if (mp_hw == NULL) {
			err = ENOMEM;
			break;
		}
		dlp = (union DL_primitives *)mp_hw->b_rptr;

		ill->ill_hw_mp = mp_hw;
		ill->ill_hw_addr = (uchar_t *)mp_hw->b_rptr +
				dlp->physaddr_ack.dl_addr_offset;
		if (dlp->physaddr_ack.dl_addr_length == 0 ||
		    ill->ill_phys_addr_length == 0 ||
		    ill->ill_phys_addr_length == IP_ADDR_LEN) {
			/*
			 * XXX Compatbility: atun driver returns a length of 0.
			 * ipdptp has an ill_phys_addr_length of zero (from
			 * DL_BIND_ACK) but a non-zero length here.
			 * ipd has an ill_phys_addr_length of 4 (from
			 * DL_BIND_ACK) but a non-zero length here.
			 */
			ill->ill_hw_addr = NULL;
		} else if (dlp->physaddr_ack.dl_addr_length !=
		    ill->ill_phys_addr_length) {
			ip0dbg(("DL_PHYS_ADDR_ACK: "
			    "Address length mismatch %d %d\n",
			    dlp->physaddr_ack.dl_addr_length,
			    ill->ill_phys_addr_length));
			err = EINVAL;
			break;
		}
		if (IN6_IS_ADDR_UNSPECIFIED(&ill->ill_token))
			(void) ill_setdefaulttoken(ill);

		/*
		 * If the ill zero interface has a zero address assign
		 * it the proper link local address.
		 */
		ASSERT(ill->ill_ipif->ipif_id == 0);
		if (ipif && IN6_IS_ADDR_UNSPECIFIED(&ipif->ipif_v6lcl_addr))
			(void) ipif_setlinklocal(ipif);
		break;
	}
	case DL_OK_ACK:
		ip2dbg(("DL_OK_ACK %s (0x%x)\n",
		    dlpi_prim_str((int)dloa->dl_correct_primitive),
		    dloa->dl_correct_primitive));
		switch (dloa->dl_correct_primitive) {
		case DL_UNBIND_REQ:
		case DL_ATTACH_REQ:
		case DL_DETACH_REQ:
			ill_dlpi_done(ill);
			break;
		}
		break;
	default:
		break;
	}
	/* ill_pending_mp and ill_pending_ipif consistency check. */
	ASSERT((ill->ill_pending_mp == NULL &&
	    ill->ill_pending_ipif == NULL) ||
	    (ill->ill_pending_mp != NULL && ill->ill_pending_ipif != NULL));

	freemsg(mp);
	if (mp1) {
		struct iocblk *iocp;

		/*
		 * Complete the waiting IOCTL. For SIOCLIFADDIF or
		 * SIOCSLIFNAME do a copyout.
		 */
		iocp = (struct iocblk *)mp1->b_rptr;
		if (err == 0 && (iocp->ioc_cmd == SIOCLIFADDIF ||
		    iocp->ioc_cmd == SIOCSLIFNAME)) {
			mi_copyout(q, mp1);
		} else {
			mi_copy_done(q, mp1, err);
		}
	}
}

/*
 * ip_rput_other is called by ip_rput to handle messages modifying the global
 * state in IP.  Always called as a writer.
 */
void
ip_rput_other(queue_t *q, mblk_t *mp)
{
	ill_t		*ill;
	struct iocblk	*iocp;
	mblk_t		*mp1;

	ill = (ill_t *)q->q_ptr;
	/* ill_pending_mp and ill_pending_ipif consistency check. */
	ASSERT((ill->ill_pending_mp == NULL &&
	    ill->ill_pending_ipif == NULL) ||
	    (ill->ill_pending_mp != NULL && ill->ill_pending_ipif != NULL));

	switch (mp->b_datap->db_type) {
	case M_ERROR:
	case M_HANGUP:
		/*
		 * The device has a problem.  We force the ILL down.  It can
		 * be brought up again manually using SIOCSIFFLAGS (via
		 * ifconfig or equivalent).
		 */
		if (mp->b_rptr < mp->b_wptr)
			ill->ill_error = (int)(*mp->b_rptr & 0xFF);
		if (ill->ill_error == 0)
			ill->ill_error = ENXIO;
		ill_down(ill);
		freemsg(mp);
		break;
	case M_IOCACK:
		iocp = (struct iocblk *)mp->b_rptr;
		ASSERT(iocp->ioc_cmd != DL_IOC_HDR_INFO);
		switch (iocp->ioc_cmd) {
		case SIOCSTUNPARAM:
			/*
			 * Finish socket ioctl passed through to tun.
			 * We should have an IOCTL waiting on this.
			 */
			mp1 = ill->ill_pending_mp;
			if (mp1) {
				ASSERT(ill->ill_pending_ipif->ipif_ill == ill);
				ill->ill_pending_ipif = NULL;
				ill->ill_pending_mp = NULL;
				q = ill->ill_pending_q;
				/*
				 * If the client stream is no longer around,
				 * then do not try and send a reply.
				 */
				if (q == NULL) {
					mblk_t	*bp = mp1;

					for (; bp; bp = bp->b_cont) {
						bp->b_prev = NULL;
						bp->b_next = NULL;
					}
					freemsg(mp1);
					freemsg(mp);
					break;
				}
				/*
				 * Clear fields here. Since we are holding
				 * the writers lock we can do it here even
				 * though we do the qreply below.
				 */
				ASSERT(((ipc_t *)q->q_ptr)->ipc_pending_ill
				    == ill);
				ill->ill_pending_q = NULL;
				((ipc_t *)q->q_ptr)->ipc_pending_ill = NULL;
			}
			if (ill->ill_isv6) {
				struct iftun_req *ta;

				/*
				 * if a source or destination is
				 * being set, try and set the link
				 * local address for the tunnel
				 */
				ta = (struct iftun_req *)mp->b_cont->
				    b_cont->b_rptr;
				if (ta->ifta_flags & (IFTUN_SRC | IFTUN_DST)) {
					ipif_set_tun_llink(ill, ta);
				}

			}
			if (mp1 != NULL) {
				freemsg(mp1);
				mi_copy_done(q, mp, iocp->ioc_error);
			} else {
				putnext(q, mp);
			}
			break;
		case SIOCGTUNPARAM:
			/*
			 * This is really M_IOCDATA from the tunnel driver.
			 * convert back and complete the ioctl.
			 * We should have an IOCTL waiting on this.
			 */
			mp1 = ill->ill_pending_mp;
			if (mp1) {
				ASSERT(ill->ill_pending_ipif->ipif_ill == ill);
				ill->ill_pending_ipif = NULL;
				ill->ill_pending_mp = NULL;
				q = ill->ill_pending_q;
				/*
				 * If the client stream is no longer around,
				 * then do not try and send a reply.
				 */
				if (q == NULL) {
					mblk_t	*bp = mp1;

					for (; bp; bp = bp->b_cont) {
						bp->b_prev = NULL;
						bp->b_next = NULL;
					}
					freemsg(mp1);
					freemsg(mp);
					break;
				}
				/*
				 * Clear fields here. Since we are holding
				 * the writers lock we can do it here even
				 * though we do the qreply below.
				 */
				ASSERT(((ipc_t *)q->q_ptr)->ipc_pending_ill
				    == ill);
				ill->ill_pending_q = NULL;
				((ipc_t *)q->q_ptr)->ipc_pending_ill = NULL;

				freemsg(mp1);
				if (iocp->ioc_error != 0) {
					mi_copy_done(q, mp, iocp->ioc_error);
				} else {
					mp->b_datap->db_type = M_IOCDATA;
					mi_copyout(q, mp);
				}
			} else {
				putnext(q, mp);
			}
			break;
		default:
			break;
		}
		break;
	case M_IOCNAK:
		iocp = (struct iocblk *)mp->b_rptr;
		switch (iocp->ioc_cmd) {
		case DL_IOC_HDR_INFO:
			/*
			 * If this was the first attempt turn of the
			 * fastpath probing.
			 */
			if (ill->ill_dlpi_fastpath_state == IDMS_INPROGRESS) {
				ill->ill_dlpi_fastpath_state = IDMS_FAILED;
				ip1dbg(("ip_rput: DLPI fastpath off on "
				    "interface %s\n",
				    ill->ill_name));
			}
			freemsg(mp);
			break;
		case SIOCSTUNPARAM:
			/*
			 * Finish socket ioctl passed through to tun
			 * We should have an IOCTL waiting on this.
			 */
			mp1 = ill->ill_pending_mp;
			if (mp1) {
				ASSERT(ill->ill_pending_ipif->ipif_ill == ill);
				ill->ill_pending_ipif = NULL;
				ill->ill_pending_mp = NULL;
				q = ill->ill_pending_q;
				/*
				 * If the client stream is no longer around,
				 * then do not try and send a reply.
				 */
				if (q == NULL) {
					mblk_t	*bp = mp1;

					for (; bp; bp = bp->b_cont) {
						bp->b_prev = NULL;
						bp->b_next = NULL;
					}
					freemsg(mp1);
					freemsg(mp);
					break;
				}
				/*
				 * Clear fields here. Since we are holding
				 * the writers lock we can do it here even
				 * though we do the qreply below.
				 */
				ASSERT(((ipc_t *)q->q_ptr)->ipc_pending_ill
				    == ill);
				ill->ill_pending_q = NULL;
				((ipc_t *)q->q_ptr)->ipc_pending_ill = NULL;
				if (iocp->ioc_error == 0) {
					iocp->ioc_error = EINVAL;
				}
				freemsg(mp1);
				mi_copy_done(q, mp, iocp->ioc_error);
			} else {
				putnext(q, mp);
			}
			break;
		case SIOCGTUNPARAM:
			/*
			 * This is really M_IOCDATA from the tunnel driver.
			 * convert back and complete the ioctl.
			 * We should have an IOCTL waiting on this.
			 */
			mp1 = ill->ill_pending_mp;
			if (mp1) {
				ASSERT(ill->ill_pending_ipif->ipif_ill == ill);
				ill->ill_pending_ipif = NULL;
				ill->ill_pending_mp = NULL;
				q = ill->ill_pending_q;
				/*
				 * If the client stream is no longer around,
				 * then do not try and send a reply.
				 */
				if (q == NULL) {
					mblk_t	*bp = mp1;

					for (; bp; bp = bp->b_cont) {
						bp->b_prev = NULL;
						bp->b_next = NULL;
					}
					freemsg(mp1);
					freemsg(mp);
					break;
				}
				/*
				 * Clear fields here. Since we are holding
				 * the writers lock we can do it here even
				 * though we do the qreply below.
				 */
				ASSERT(((ipc_t *)q->q_ptr)->ipc_pending_ill
				    == ill);
				ill->ill_pending_q = NULL;
				((ipc_t *)q->q_ptr)->ipc_pending_ill = NULL;

				if (iocp->ioc_error == 0) {
					iocp->ioc_error = EINVAL;
				}
				freemsg(mp1);
				mi_copy_done(q, mp, iocp->ioc_error);
			} else {
				putnext(q, mp);
			}
			break;
		default:
			break;
		}
	default:
		break;
	}
	/* ill_pending_mp and ill_pending_ipif consistency check. */
	ASSERT((ill->ill_pending_mp == NULL &&
	    ill->ill_pending_ipif == NULL) ||
	    (ill->ill_pending_mp != NULL && ill->ill_pending_ipif != NULL));
}

/*
 * NOTE : This function does not ire_refrele the ire argument passed in.
 */
void
ip_rput_forward(ire_t *ire, ipha_t *ipha, mblk_t *mp)
{
	mblk_t	*mp1;
	uint32_t	pkt_len;
	queue_t	*q;
	uint32_t	sum;
	uint32_t	ll_hdr_len;
#define	rptr	((uchar_t *)ipha)
	uint32_t	max_frag;

	pkt_len = ntohs(ipha->ipha_length);

	/* Adjust the checksum to reflect the ttl decrement. */
	sum = (int)ipha->ipha_hdr_checksum + IP_HDR_CSUM_TTL_ADJUST;
	ipha->ipha_hdr_checksum = (uint16_t)(sum + (sum >> 16));

	if (ipha->ipha_ttl-- <= 1) {
		if (ip_csum_hdr(ipha)) {
			BUMP_MIB(ip_mib.ipInCksumErrs);
			goto drop_pkt;
		}
		/*
		 * Note: ire_stq this will be NULL for multicast
		 * datagrams using the long path through arp (the IRE
		 * is not an IRE_CACHE). This should not cause
		 * problems since we don't generate ICMP errors for
		 * multicast packets.
		 */
		q = ire->ire_stq;
		if (q)
			icmp_time_exceeded(q, mp, ICMP_TTL_EXCEEDED);
		else
			freemsg(mp);
		return;
	}

	/* Check if there are options to update */
	if (!IS_SIMPLE_IPH(ipha)) {
		if (ip_csum_hdr(ipha)) {
			BUMP_MIB(ip_mib.ipInCksumErrs);
			goto drop_pkt;
		}
		if (ip_rput_forward_options(mp, ipha, ire)) {
			return;
		}

		ipha->ipha_hdr_checksum = 0;
		ipha->ipha_hdr_checksum = ip_csum_hdr(ipha);
	}
	max_frag = ire->ire_max_frag;
	if (pkt_len > max_frag) {
		/*
		 * It needs fragging on its way out.  We haven't
		 * verified the header checksum yet.  Since we
		 * are going to put a surely good checksum in the
		 * outgoing header, we have to make sure that it
		 * was good coming in.
		 */
		if (ip_csum_hdr(ipha)) {
			BUMP_MIB(ip_mib.ipInCksumErrs);
			goto drop_pkt;
		}
		ip_wput_frag(ire, mp, &ire->ire_ib_pkt_count, max_frag, 0);
		return;
	}
	ll_hdr_len = 0;
	mp1 = ire->ire_fp_mp;
	if (mp1 != NULL) {
		ASSERT(mp1->b_datap->db_type == M_DATA);
		ll_hdr_len = mp1->b_wptr - mp1->b_rptr;
	} else {
		mp1 = ire->ire_dlureq_mp;
	}
	/*
	 * If the driver accepts M_DATA prepends
	 * and we have enough room to lay it in ...
	 */
	if (ll_hdr_len != 0 && (rptr - mp->b_datap->db_base) >= ll_hdr_len) {
		/* Note: ipha is only used as an alias for rptr */
		ipha = (ipha_t *)(rptr - ll_hdr_len);
		mp->b_rptr = rptr;
		/* TODO: inline this small copy */
		bcopy(mp1->b_rptr, rptr, ll_hdr_len);
	} else {
		mp1 = copyb(mp1);
		if (!mp1) {
			BUMP_MIB(ip_mib.ipInDiscards);
			goto drop_pkt;
		}
		mp1->b_cont = mp;
		mp = mp1;
	}
	q = ire->ire_stq;
	/* TODO: make this atomic */
	ire->ire_ib_pkt_count++;
	BUMP_MIB(ip_mib.ipForwDatagrams);
	putnext(q, mp);
	return;

drop_pkt:;
	ip1dbg(("ip_rput_forward: drop pkt\n"));
	freemsg(mp);
#undef	rptr
}

void
ip_rput_forward_multicast(ipaddr_t dst, mblk_t *mp, ipif_t *ipif)
{
	ire_t	*ire;

	ASSERT(!ipif->ipif_isv6);
	/*
	 * Find an IRE which matches the destination and the outgoing
	 * queue (i.e. the outgoing logical interface) in the cache table.
	 */
	if (ipif->ipif_flags & IFF_POINTOPOINT)
		dst = ipif->ipif_pp_dst_addr;
	ire = ire_ctable_lookup(dst, 0, 0, ipif, NULL, MATCH_IRE_IPIF);
	if (!ire) {
		/*
		 * Mark this packet to make it be delivered to
		 * ip_rput_forward after the new ire has been
		 * created.
		 */
		mp->b_prev = NULL;
		mp->b_next = mp;
		ip_newroute_ipif(ipif->ipif_ill->ill_wq, mp, ipif, dst, NULL);
	} else {
		ip_rput_forward(ire, (ipha_t *)mp->b_rptr, mp);
		IRE_REFRELE(ire);
	}
}

/* Update any source route, record route or timestamp options */
static int
ip_rput_forward_options(mblk_t *mp, ipha_t *ipha, ire_t *ire)
{
	uint32_t	totallen;
	uchar_t	*opt;
	uint32_t	optval;
	uint32_t	optlen;
	ipaddr_t dst;
	uint32_t	ts;
	ire_t		*dst_ire = NULL;
	ire_t		*tmp_ire = NULL;

	ip2dbg(("ip_rput_forward_options\n"));
	dst = ipha->ipha_dst;
	totallen = ipha->ipha_version_and_hdr_length -
		(uint8_t)((IP_VERSION << 4) + IP_SIMPLE_HDR_LENGTH_IN_WORDS);
	opt = (uchar_t *)&ipha[1];
	totallen <<= 2;
	while (totallen != 0) {
		switch (optval = opt[IPOPT_POS_VAL]) {
		case IPOPT_EOL:
			return (0);
		case IPOPT_NOP:
			optlen = 1;
			break;
		default:
			optlen = opt[IPOPT_POS_LEN];
		}
		ip2dbg(("ip_rput_forward_options: opt %d, len %d\n",
		    optval, optlen));

		if (optlen == 0 || optlen > totallen)
			break;

		switch (optval) {
			uint32_t off;
		case IPOPT_SSRR:
		case IPOPT_LSRR:
			/* Check if adminstratively disabled */
			if (!ip_forward_src_routed) {
				BUMP_MIB(ip_mib.ipForwProhibits);
				if (ire->ire_stq)
					icmp_unreachable(ire->ire_stq, mp,
					    ICMP_SOURCE_ROUTE_FAILED);
				else {
					ip0dbg(("ip_rput_fw_options: "
					    "unable to send unreach\n"));
					freemsg(mp);
				}
				return (-1);
			}

			dst_ire = ire_ctable_lookup(dst, 0, IRE_LOCAL,
			    NULL, NULL, MATCH_IRE_TYPE);
			if (dst_ire == NULL) {
				/*
				 * Must be partial since ip_rput_options
				 * checked for strict.
				 */
				break;
			}
			off = opt[IPOPT_POS_OFF];
			off--;
		redo_srr:
			if (optlen < IP_ADDR_LEN ||
			    off > optlen - IP_ADDR_LEN) {
				/* End of source route */
				ip1dbg((
				    "ip_rput_forward_options: end of SR\n"));
				ire_refrele(dst_ire);
				break;
			}
			bcopy((char *)opt + off, &dst, IP_ADDR_LEN);
			bcopy(&ire->ire_src_addr, (char *)opt + off,
			    IP_ADDR_LEN);
			ip1dbg(("ip_rput_forward_options: next hop 0x%x\n",
			    ntohl(dst)));

			/*
			 * Check if our address is present more than
			 * once as consecutive hops in source route.
			 */
			tmp_ire = ire_ctable_lookup(dst, 0, IRE_LOCAL,
			    NULL, NULL, MATCH_IRE_TYPE);
			if (tmp_ire != NULL) {
				ire_refrele(tmp_ire);
				off += IP_ADDR_LEN;
				opt[IPOPT_POS_OFF] += IP_ADDR_LEN;
				goto redo_srr;
			}
			ipha->ipha_dst = dst;
			opt[IPOPT_POS_OFF] += IP_ADDR_LEN;
			ire_refrele(dst_ire);
			break;
		case IPOPT_RR:
			off = opt[IPOPT_POS_OFF];
			off--;
			if (optlen < IP_ADDR_LEN ||
			    off > optlen - IP_ADDR_LEN) {
				/* No more room - ignore */
				ip1dbg((
				    "ip_rput_forward_options: end of RR\n"));
				break;
			}
			bcopy(&ire->ire_src_addr, (char *)opt + off,
			    IP_ADDR_LEN);
			opt[IPOPT_POS_OFF] += IP_ADDR_LEN;
			break;
		case IPOPT_IT:
			/* Insert timestamp if there is room */
			switch (opt[IPOPT_POS_OV_FLG] & 0x0F) {
			case IPOPT_IT_TIME:
				off = IPOPT_IT_TIMELEN;
				break;
			case IPOPT_IT_SPEC:
#ifdef IPOPT_IT_SPEC_BSD
			case IPOPT_IT_SPEC_BSD:
#endif
				/* Verify that the address matched */
				off = opt[IPOPT_POS_OFF] - 1;
				bcopy((char *)opt + off, &dst, IP_ADDR_LEN);
				dst_ire = ire_ctable_lookup(dst, 0,
				    IRE_LOCAL, NULL, NULL, MATCH_IRE_TYPE);
				if (dst_ire == NULL) {
					/* Not for us */
					break;
				}
				ire_refrele(dst_ire);
				/* FALLTHRU */
			case IPOPT_IT_TIME_ADDR:
				off = IP_ADDR_LEN + IPOPT_IT_TIMELEN;
				break;
			default:
				/*
				 * ip_*put_options should have already
				 * dropped this packet.
				 */
				cmn_err(CE_PANIC, "ip_rput_forward_options: "
				    "unknown IT - bug in ip_rput_options?\n");
				return (0);	/* Keep "lint" happy */
			}
			if (opt[IPOPT_POS_OFF] - 1 + off > optlen) {
				/* Increase overflow counter */
				off = (opt[IPOPT_POS_OV_FLG] >> 4) + 1;
				opt[IPOPT_POS_OV_FLG] =
				    (uint8_t)((opt[IPOPT_POS_OV_FLG] & 0x0F) |
				    (off << 4));
				break;
			}
			off = opt[IPOPT_POS_OFF] - 1;
			switch (opt[IPOPT_POS_OV_FLG] & 0x0F) {
			case IPOPT_IT_SPEC:
#ifdef IPOPT_IT_SPEC_BSD
			case IPOPT_IT_SPEC_BSD:
#endif
			case IPOPT_IT_TIME_ADDR:
				bcopy(&ire->ire_src_addr,
				    (char *)opt + off, IP_ADDR_LEN);
				opt[IPOPT_POS_OFF] += IP_ADDR_LEN;
				/* FALLTHRU */
			case IPOPT_IT_TIME:
				off = opt[IPOPT_POS_OFF] - 1;
				/* Compute # of milliseconds since midnight */
				ts = (hrestime.tv_sec % (24 * 60 * 60)) * 1000 +
					hrestime.tv_nsec / (NANOSEC / MILLISEC);
				bcopy(&ts, (char *)opt + off, IPOPT_IT_TIMELEN);
				opt[IPOPT_POS_OFF] += IPOPT_IT_TIMELEN;
				break;
			}
			break;
		}
		totallen -= optlen;
		opt += optlen;
	}
	return (0);
}

mblk_t *
ipsec_in_alloc()
{
	mblk_t *ipsec_in;
	ipsec_in_t *ii;

	ipsec_in = allocb(sizeof (ipsec_info_t), BPRI_HI);
	if (ipsec_in == NULL) {
		ip1dbg(("ipsec_in_alloc: IPSEC_IN allocation failure.\n"));
		return (NULL);
	}

	ii = (ipsec_in_t *)ipsec_in->b_rptr;
	ipsec_in->b_datap->db_type = M_CTL;
	ipsec_in->b_wptr += sizeof (ipsec_info_t);
	bzero(ii, sizeof (ipsec_info_t));
	ii->ipsec_in_type = IPSEC_IN;
	ii->ipsec_in_v4 = B_TRUE;
	ii->ipsec_in_len = sizeof (ipsec_in_t);
	ii->ipsec_in_secure = B_TRUE;

	return (ipsec_in);
}

/*
 * This is called after processing at least one of AH/ESP headers.
 */
static void
ip_fanout_proto_again(queue_t *q, mblk_t *ipsec_mp)
{
	mblk_t *mp;
	ipaddr_t dst;
	ipha_t *ipha;
	ire_t *ire;
	ipsec_in_t *ii;
	ill_t *ill;

	ii = (ipsec_in_t *)ipsec_mp->b_rptr;
	ASSERT(ii->ipsec_in_ill_index != 0);

	mp = ipsec_mp->b_cont;
	ASSERT(mp != NULL);

	/*
	 * We need to get the original queue on which ip_rput_local
	 * was called.
	 */
	for (ill = ill_g_head; ill; ill = ill->ill_next) {
		if (ill->ill_index == ii->ipsec_in_ill_index)
			break;
	}

	if (ill == NULL) {
		ip0dbg(("ip_fanout_proto_again: interface disappeared\n"));
		freemsg(ipsec_mp);
		return;
	}

	if (mp->b_datap->db_type == M_CTL) {
		/*
		 * AH/ESP is returning the ICMP message after
		 * removing their headers. Fanout again till
		 * it gets to the right protocol.
		 */
		icmph_t *icmph;
		int iph_hdr_length;
		int hdr_length;

		ipha = (ipha_t *)mp->b_rptr;
		iph_hdr_length = IPH_HDR_LENGTH(ipha);
		icmph = (icmph_t *)&mp->b_rptr[iph_hdr_length];
		ipha = (ipha_t *)&icmph[1];
		hdr_length = IPH_HDR_LENGTH(ipha);
		/*
		 * icmp_inbound_error_fanout may need to do pullupmsg.
		 * Reset the type to M_DATA.
		 */
		mp->b_datap->db_type = M_DATA;
		icmp_inbound_error_fanout(q, ill, ipsec_mp, icmph, ipha,
		    iph_hdr_length, hdr_length, B_TRUE);
		return;
	}

	ipha = (ipha_t *)mp->b_rptr;
	dst = ipha->ipha_dst;

	if (CLASSD(dst)) {
		/*
		 * Multicast has to be delivered to all streams.
		 */
		dst = INADDR_BROADCAST;
	}
	ire = ire_cache_lookup(dst);
	if (ire == NULL) {
		ip1dbg(("ip_fanout_proto_again : IRE not found"));
		freemsg(ipsec_mp);
		return;
	}
	ip_rput_local(ill->ill_rq, ipsec_mp, ipha, ire, 0);
	ire_refrele(ire);
}

/*
 * Call ill_frag_timeout to do garbage collection. ill_frag_timeout
 * returns 'true' if there are still fragments left on the queue, in
 * which case we restart the timer.
 */
void
ill_frag_timer(void *arg)
{
	ill_t *ill = (ill_t *)arg;

	ill->ill_frag_timer_id = 0;
	if (ill_frag_timeout(ill, ip_g_frag_timeout)) {
		ill->ill_frag_timer_id = qtimeout(ill->ill_rq,
		    ill_frag_timer, ill, MSEC_TO_TICK(ip_g_frag_timo_ms >> 1));
	}
}

/*
 * This routine is needed for loopback when forwarding multicasts.
 *
 * NOTE : The callers of this function need to do the ire_refrele for the
 *	  ire that is being passed in.
 */
void
ip_rput_local(queue_t *q, mblk_t *mp, ipha_t *ipha, ire_t *ire, uint_t flags)
{
	dblk_t	*dp;
	ipaddr_t	dst;
	ill_t	*ill = (ill_t *)q->q_ptr;
	icf_t	*icf;
	ipc_t	*ipc;
	mblk_t	*mp1;
	uint32_t	sum;
	uint32_t	u1;
	uint32_t	u2;
	int	sum_valid, hdr_length;
	uint16_t	*up;
	int	offset;
	ssize_t	len;
	uint32_t	ports;
	boolean_t	mctl_present;
	mblk_t		*first_mp;
	ipha_t		*inner_ipha;
	queue_t		*rq;

	TRACE_1(TR_FAC_IP, TR_IP_RPUT_LOCL_START,
	    "ip_rput_locl_start: q %p", q);

	ASSERT(ire->ire_ipversion == IPV4_VERSION);

#define	rptr	((uchar_t *)ipha)
#define	UDPH_SIZE 8

	EXTRACT_PKT_MP(mp, first_mp, mctl_present);
	/*
	 * IF M_CTL is not present, then ipsec_in_is_secure
	 * should return B_TRUE. There is a case where loopback
	 * packets has an M_CTL in the front with all the
	 * IPSEC options set to IPSEC_PREF_NEVER - which means
	 * ipsec_in_is_secure will return B_FALSE. As loopback
	 * packets never comes here, it is safe to ASSERT the
	 * following.
	 */
	ASSERT(!mctl_present || ipsec_in_is_secure(first_mp));

	/*
	 * FAST PATH for udp packets
	 */

	/* u1 is # words of IP options */
	u1 = ipha->ipha_version_and_hdr_length - (uchar_t)((IP_VERSION << 4)
	    + IP_SIMPLE_HDR_LENGTH_IN_WORDS);

	/* Check the IP header checksum.  */
#define	uph	((uint16_t *)ipha)
	sum = uph[0] + uph[1] + uph[2] + uph[3] + uph[4] + uph[5] + uph[6] +
		uph[7] + uph[8] + uph[9];
#undef  uph

	/* set dst before fragment handling - it needs dst as well */
	dst = ipha->ipha_dst;
	/* IP options present (udppullup uses u1) */
	if (u1)
		goto ipoptions;

	/* finish doing IP checksum */
	sum = (sum & 0xFFFF) + (sum >> 16);
	sum = ~(sum + (sum >> 16)) & 0xFFFF;

	/*
	 * Don't verify header checksum if this packet is coming
	 * back from AH/ESP as we already did it.
	 */
	if (!mctl_present && (sum && sum != 0xFFFF)) goto cksumerror;

	/*
	 * Count for SNMP of inbound packets for ire. As ip_rput_local
	 * might be called more than once for secure packets, count only
	 * the first time.
	 *
	 * XXX - For those packets that have been redirected here from
	 * the forwarding path due to a proxy listener match, the forwarding
	 * count continue to get incremented.
	 */
	if (!mctl_present)
		ire->ire_ib_pkt_count++;

	/* packet part of fragmented IP packet? */
	u2 = ntohs(ipha->ipha_fragment_offset_and_flags);
	sum_valid = 0;
	u1 = u2 & (IPH_MF | IPH_OFFSET);
	if (u1)
		goto fragmented;

	/* u1 = IP header length (20 bytes) */
	u1 = IP_SIMPLE_HDR_LENGTH;

	/* protocol type not UDP (ports used as temporary) */
	if ((ports = ipha->ipha_protocol) != IPPROTO_UDP)
		goto notudp;

	/* clear the ICK_MAGIC bit as it is not used by UDP */
	mp->b_ick_flag &= ~ICK_VALID;

	/* packet does not contain complete IP & UDP headers */
	if ((mp->b_wptr - rptr) < (IP_SIMPLE_HDR_LENGTH + UDPH_SIZE))
		goto udppullup;

	/* up points to UDP header */
	up = (uint16_t *)((uchar_t *)ipha + IP_SIMPLE_HDR_LENGTH);

#define	iphs	((uint16_t *)ipha)
	/* if udp hdr cksum != 0, then need to checksum udp packet */
	if (up[3] &&
	    (sum = IP_CSUM(mp, (int32_t)((uchar_t *)up - (uchar_t *)ipha),
	    IP_UDP_CSUM_COMP + iphs[6] + iphs[7] + iphs[8] +
	    iphs[9] + up[2])) != 0) {
		goto badchecksum;
	}

	/* u1 = UDP destination port */
	u1 = up[1];	/* Destination port in net byte order */
	/* u2 = UDP src port */
	u2 = up[0];

	/* broadcast IP packet? */
	if (ire->ire_type == IRE_BROADCAST)
		goto udpslowpath;

	/* look for matching stream based on UDP addresses */
	icf = &ipc_udp_fanout[IP_UDP_HASH(u1)];
	mutex_enter(&icf->icf_lock);
	ipc = (ipc_t *)&icf->icf_ipc;	/* ipc_hash_next will get first */
	do {
		ipc = ipc->ipc_hash_next;
		if (ipc == NULL) {
			/* Check IPv6 fanout in the slow path */
			mutex_exit(&icf->icf_lock);
			goto udpslowpath;
		}

	} while (!IP_UDP_MATCH(ipc, u1, dst, u2, ipha->ipha_src));

	IPC_REFHOLD(ipc);
	mutex_exit(&icf->icf_lock);
	rq = ipc->ipc_rq;

	/* stream blocked? */
	if (!canputnext(rq)) {
		IPC_REFRELE(ipc);
		goto udpslowpath;
	}

	/*
	 * Statistics. Do it only once as ip_rput_local might be called
	 * multiple times for secure packets.
	 */
	if (!mctl_present)
		BUMP_MIB(ip_mib.ipInDelivers);

	TRACE_2(TR_FAC_IP, TR_IP_RPUT_LOCL_END,
	    "ip_rput_locl_end: q %p (%S)", rq, "xx-putnext up");

	/* pass packet up to the transport */
	if (IPC_INBOUND_POLICY_PRESENT(ipc)) {
		if (!ipsec_check_policy(first_mp, ipc, IPSEC_INBOUND,
		    mctl_present)) {
			/*
			 * POLICY FAILURE : Loggable Event.
			 */
			IPC_REFRELE(ipc);
			ipsec_log_policy_failure(q, IPSEC_POLICY_MISMATCH,
			    "ip_rput_local(udp)", ipha, mctl_present);
			freemsg(first_mp);
			return;
		}
	} else if (mctl_present) {
		/*
		 * Don't allow this. Check IPSEC NOTE above
		 * ip_fanout_proto().
		 */
		IPC_REFRELE(ipc);
		ipsec_log_policy_failure(q, IPSEC_POLICY_NOT_NEEDED,
		    "ip_rput_local(udp)", ipha, mctl_present);
		freemsg(first_mp);
		return;
	}
	putnext(rq, mp);
	IPC_REFRELE(ipc);
	if (mctl_present)
		freeb(first_mp);
	return;

	/*
	 * FAST PATH for tcp packets
	 */
notudp:
	/* if not TCP, then just use default code */
	if (ports != IPPROTO_TCP) {
		/* clear the ICK_MAGIC bit as its currently only used by TCP */
		mp->b_ick_flag &= ~ICK_VALID;
		goto nottcp;
	}

	/* does packet contain IP+TCP headers? */
	len = mp->b_wptr - rptr;
	if (len < (IP_SIMPLE_HDR_LENGTH + TCP_MIN_HEADER_LENGTH))
		goto tcppullup;

	/* TCP options present? */
	offset = ((uchar_t *)ipha)[IP_SIMPLE_HDR_LENGTH + 12] >> 4;
	if (offset != 5)
		goto tcpoptions;

	/* multiple mblks of tcp data? */
	if ((mp1 = mp->b_cont) != NULL) {
		/* more then two? */
		if (mp1->b_cont)
			goto multipkttcp;
		len += mp1->b_wptr - mp1->b_rptr;
	}
	up = (uint16_t *)(rptr + IP_SIMPLE_HDR_LENGTH + TCP_PORTS_OFFSET);
	ports = *(uint32_t *)up;

	/* part of pseudo checksum */
	u1 = len - IP_SIMPLE_HDR_LENGTH;
#ifdef  _BIG_ENDIAN
	u1 += IPPROTO_TCP;
#else
	u1 = ((u1 >> 8) & 0xFF) + (((u1 & 0xFF) + IPPROTO_TCP) << 8);
#endif
	u1 += iphs[6] + iphs[7] + iphs[8] + iphs[9];
	/*
	 * If the packet has gone through AH/ESP, do the checksum here
	 * itself.
	 *
	 * If it has not gone through IPSEC processing and not a duped
	 * mblk, then look for driver checksummed mblk. We validate or
	 * postpone the checksum to TCP for single copy checksum.
	 *
	 * Note that we only honor HW cksum in the fastpath.
	 */
	dp = mp->b_datap;
	if (!mctl_present && dp->db_ref == 1) {
		if ((mp->b_ick_flag & ICK_VALID) && dohwcksum &&
		    ((len = (uchar_t *)up - mp->b_ick_start) & 1) == 0) {
			/*
			 * A driver checksummed mblk and prepended extraneous
			 * data (if any) is a multiple of 16 bits in size.
			 * First calculate the cksum for any extraneous data.
			 * Then clear inetcksum flag (now a normal mblk).
			 *
			 * Note: we know from above that this is either a
			 *	 single mblk or a 2 mblk chain and that
			 *	 any extraneous data can only be prepended
			 *	 to the first mblk or the end of the last.
			 */
#ifdef ZC_TEST
			zckstat->zc_hwcksum_r.value.ul++;
#endif
			/* Consuming hwcksum so clear the flag now */
			mp->b_ick_flag &= ~ICK_VALID;
			u1 += mp->b_ick_value;
			if (!mp1)
				mp1 = mp;
			if (len > 0)
				u2 = IP_BCSUM_PARTIAL(mp->b_ick_start,
				    (int32_t)len, 0);
			else
				u2 = 0;
			if ((len = mp1->b_ick_end - mp1->b_wptr) > 0) {
				uint32_t	u3;

				u3 = IP_BCSUM_PARTIAL(mp1->b_wptr,
				    (int32_t)len, 0);
				if ((uintptr_t)mp1->b_wptr & 1)
					/*
					 * Postpended extraneous data was
					 * odd byte aligned, so swap the
					 * resulting cksum bytes.
					 */
					u2 += ((u3 << 8) & 0xffff) | (u3 >> 8);
				else
					u2 += u3;
				u2 = (u2 & 0xFFFF) + ((int)u2 >> 16);
			}
			/* One's complement subtract extraneous checksum */
			if (u2 >= u1)
				u1 = ~(u2 - u1) & 0xFFFF;
			else
				u1 -= u2;
			u1 = (u1 & 0xFFFF) + ((int)u1 >> 16);
			u1 += (u1 >> 16);
			if (~u1 & 0xFFFF) {
				ipcsumdbg("hwcksumerr\n", mp);
				goto tcpcksumerr;
			}
#if 0
			if (syncstream) {
				/*
				 * XXX - have to also set db_struioptr,
				 * db_struioflag... on every mblk.
				 */
				dp->db_struioflag |= STRUIO_SPEC;
				mp1->b_datap->db_struioflag |= STRUIO_SPEC;
			}
#endif
#ifdef ZC_TEST
		} else if (syncstream) {
#else
		} else {
#endif
			/* Not using hwcksum so clear the flag now */
			mp->b_ick_flag &= ~ICK_VALID;
			u1 = (u1 >> 16) + (u1 & 0xffff);
			u1 += (u1 >> 16);
			*(uint16_t *)dp->db_struioun.data = (uint16_t)u1;
			dp->db_struiobase = (uchar_t *)up;
			dp->db_struioptr = (uchar_t *)up;
			dp->db_struiolim = mp->b_wptr;
			dp->db_struioflag |= STRUIO_SPEC|STRUIO_IP;
#ifdef ZC_TEST
			zckstat->zc_swcksum_r.value.ul++;
#endif
			if (mp1) {
				*(uint16_t *)mp1->b_datap->db_struioun.data = 0;
				mp1->b_datap->db_struiobase = mp1->b_rptr;
				mp1->b_datap->db_struioptr = mp1->b_rptr;
				mp1->b_datap->db_struiolim = mp1->b_wptr;
				mp1->b_datap->db_struioflag |=
				    STRUIO_SPEC | STRUIO_IP;
			}
		}
#ifdef ZC_TEST
		else if (IP_CSUM(mp, (int32_t)((uchar_t *)up - rptr), u1)) {
			zckstat->zc_swcksum_r.value.ul++;
			ipcsumdbg("swcksumerr2\n", mp);
			goto tcpcksumerr;
		}
#endif
	} else {
#ifdef ZC_TEST
		zckstat->zc_swcksum_r.value.ul++;
#endif
		/* Not using hwcksum so clear the flag now */
		mp->b_ick_flag &= ~ICK_VALID;
		if (IP_CSUM(mp, (int32_t)((uchar_t *)up - rptr), u1)) {
			ipcsumdbg("swcksumerr\n", mp);
			goto tcpcksumerr;
		}
	}

	/* Find a TCP client stream for this packet. */
	icf = &ipc_tcp_conn_fanout[IP_TCP_CONN_HASH(ipha->ipha_src, ports)];
	mutex_enter(&icf->icf_lock);
	ipc = (ipc_t *)&icf->icf_ipc;	/* ipc_hash_next will get first */
	do {
		ipc = ipc->ipc_hash_next;
		if (ipc == NULL) {
			mutex_exit(&icf->icf_lock);
			goto notcpport;
		}
	} while (!IP_TCP_CONN_MATCH(ipc, ipha, ports));

	/* Got a client */
	IPC_REFHOLD(ipc);
	mutex_exit(&icf->icf_lock);
	rq = ipc->ipc_rq;
	/*
	 * Ip module statistics. Do it only once as ip_rput_local
	 * might be called multiple times for mctl_present data.
	 */
	if (!mctl_present)
		BUMP_MIB(ip_mib.ipInDelivers);

	TRACE_2(TR_FAC_IP, TR_IP_RPUT_LOCL_END,
	    "ip_rput_locl_end: q %p (%S)", rq, "tcp");

	if (IPC_INBOUND_POLICY_PRESENT(ipc)) {
		if (!ipsec_check_policy(first_mp, ipc, IPSEC_INBOUND,
		    mctl_present)) {
			/*
			 * POLICY FAILURE : Loggable Event.
			 */
			IPC_REFRELE(ipc);
			ipsec_log_policy_failure(q, IPSEC_POLICY_MISMATCH,
			    "ip_rput_local(tcp)", ipha, mctl_present);
			freemsg(first_mp);
			return;
		}
	} else if (mctl_present) {
		/*
		 * Don't allow this. Check IPSEC NOTE above
		 * ip_fanout_proto().
		 */
		IPC_REFRELE(ipc);
		ipsec_log_policy_failure(q, IPSEC_POLICY_NOT_NEEDED,
		    "ip_rput_local(tcp)", ipha, mctl_present);
		freemsg(first_mp);
		return;
	}
	putnext(rq, mp);
	IPC_REFRELE(ipc);
	if (mctl_present)
		freeb(first_mp);
	return;

ipoptions:
	{
		/* Add in IP options. */
		uint16_t	*uph = &((uint16_t *)ipha)[10];
		do {
			sum += uph[0];
			sum += uph[1];
			uph += 2;
		} while (--u1);
		if (ip_rput_local_options(q, mp, ipha, ire)) {
			TRACE_2(TR_FAC_IP, TR_IP_RPUT_LOCL_END,
			    "ip_rput_locl_end: q %p (%S)", q, "badopts");
			if (mctl_present)
				freeb(first_mp);
			return;
		}
	}
	sum = (sum & 0xFFFF) + (sum >> 16);
	sum = ~(sum + (sum >> 16)) & 0xFFFF;
	/*
	 * Don't do the checksum if it has gone through AH/ESP
	 * processing.
	 */
	if (!mctl_present && (sum && sum != 0xFFFF)) {
cksumerror:
		BUMP_MIB(ip_mib.ipInCksumErrs);
		goto drop_pkt;
	}

	/* TODO: make this atomic */
	ire->ire_ib_pkt_count++;
	/* Check for fragmentation offset. */
	u2 = ntohs(ipha->ipha_fragment_offset_and_flags);
	sum_valid = 0;
	u1 = u2 & (IPH_MF | IPH_OFFSET);
	if (u1) {
		ipf_t		*ipf;
		ipf_t		**ipfp;
		ipfb_t		*ipfb;
		uint16_t	ident;
		uint32_t	offset;
		ipaddr_t	src;
		uint_t		hdr_length;
		uint32_t	end;
		uint8_t		proto;
		mblk_t		*mp1;
		size_t		count;

fragmented:
		/*
		 * We re-assemble fragments before we do the AH/ESP
		 * processing. Thus, M_CTL should not be present
		 * while we are re-assembling.
		 */
		ASSERT(!mctl_present);
		ASSERT(first_mp == mp);

		ident = ipha->ipha_ident;
		offset = (u1 << 3) & 0xFFFF;
		src = ipha->ipha_src;
		hdr_length = IPH_HDR_LENGTH(ipha);
		end = ntohs(ipha->ipha_length) - hdr_length;

		/*
		 * if end == 0 then we have a packet with no data, so just
		 * free it.
		 */
		if (end == 0) {
			freemsg(mp);
			return;
		}
		proto = ipha->ipha_protocol;

		/*
		 * Fragmentation reassembly.  Each ILL has a hash table for
		 * queuing packets undergoing reassembly for all IPIFs
		 * associated with the ILL.  The hash is based on the packet
		 * IP ident field.  The ILL frag hash table was allocated
		 * as a timer block at the time the ILL was created.  Whenever
		 * there is anything on the reassembly queue, the timer will
		 * be running.
		 *
		 * For fragments that got here because of a proxy address
		 * match, the q is the interface through which the packet
		 * comes in, which may not be unique per IP datagram due to
		 * route changes. In order to ensure fragments from the same
		 * IP datagram get reassembled correctly, we always use the
		 * reassembly table from the same ill (proxy_frag_ill).
		 */
		if ((flags & IP_FF_PROXY_ONLY) != 0) {
			if ((ill = proxy_frag_ill) == NULL) {
				BUMP_MIB(ip_mib.ipInDiscards);
				freemsg(mp);
				return;
			}
			ASSERT(ill->ill_frag_hash_tbl != NULL);
		} else {
			/* EMPTY */
			ASSERT(ill != NULL);
		}
		/*
		 * Compute a partial checksum before acquiring the lock
		 */
		sum = IP_CSUM_PARTIAL(mp, hdr_length, 0);

		if (offset) {
			/*
			 * If this isn't the first piece, strip the header, and
			 * add the offset to the end value.
			 */
			mp->b_rptr += hdr_length;
			end += offset;
		}
		/*
		 * If the reassembly list for this ILL is to big, prune it.
		 */
		if (ill->ill_frag_count > ip_reass_queue_bytes)
			ill_frag_prune(ill, ip_reass_queue_bytes);
		ipfb = &ill->ill_frag_hash_tbl[ILL_FRAG_HASH(src, ident)];
		mutex_enter(&ipfb->ipfb_lock);
		ipfp = &ipfb->ipfb_ipf;
		/* Try to find an existing fragment queue for this packet. */
		for (;;) {
			ipf = ipfp[0];
			if (ipf) {
				/*
				 * It has to match on ident and src/dst address.
				 */
				if (ipf->ipf_ident == ident &&
				    ipf->ipf_src == src &&
				    ipf->ipf_dst == dst &&
				    ipf->ipf_protocol == proto) {
					/* Found it. */
					break;
				}
				ipfp = &ipf->ipf_hash_next;
				continue;
			}
			/* New guy.  Allocate a frag message. */
			mp1 = allocb(sizeof (*ipf), BPRI_MED);
			if (!mp1) {
				BUMP_MIB(ip_mib.ipInDiscards);
				freemsg(mp);
				TRACE_2(TR_FAC_IP, TR_IP_RPUT_LOCL_ERR,
				    "ip_rput_locl_err: q %p (%S)", q,
				    "ALLOCBFAIL");
reass_done:;
				mutex_exit(&ipfb->ipfb_lock);
				TRACE_2(TR_FAC_IP, TR_IP_RPUT_LOCL_END,
				    "ip_rput_locl_end: q %p (%S)", q,
				    "reass_done");
				return;
			}
			BUMP_MIB(ip_mib.ipReasmReqds);
			mp1->b_cont = mp;

			/* Initialize the fragment header. */
			ipf = (ipf_t *)mp1->b_rptr;
			ipf->ipf_checksum = 0;
			ipf->ipf_checksum_valid = 1;
			ipf->ipf_mp = mp1;
			ipf->ipf_ptphn = ipfp;
			ipfp[0] = ipf;
			ipf->ipf_hash_next = NULL;
			ipf->ipf_ident = ident;
			ipf->ipf_protocol = proto;
			ipf->ipf_src = src;
			ipf->ipf_dst = dst;
			ipf->ipf_nf_hdr_len = 0;
			/* Record reassembly start time. */
			ipf->ipf_timestamp = hrestime.tv_sec;
			/* Record ipf generation and account for frag header */
			ipf->ipf_gen = ill->ill_ipf_gen++;
			ipf->ipf_count = mp1->b_datap->db_lim -
			    mp1->b_datap->db_base;
			/*
			 * We handle reassembly two ways.  In the easy case,
			 * where all the fragments show up in order, we do
			 * minimal bookkeeping, and just clip new pieces on
			 * the end.  If we ever see a hole, then we go off
			 * to ip_reassemble which has to mark the pieces and
			 * keep track of the number of holes, etc.  Obviously,
			 * the point of having both mechanisms is so we can
			 * handle the easy case as efficiently as possible.
			 */
			if (offset == 0) {
				/* Easy case, in-order reassembly so far. */
				/* Compute a partial checksum and byte count */
				sum += ipf->ipf_checksum;
				sum = (sum & 0xFFFF) + (sum >> 16);
				sum = (sum & 0xFFFF) + (sum >> 16);
				ipf->ipf_checksum = (uint16_t)sum;
				ipf->ipf_count += mp->b_datap->db_lim -
				    mp->b_datap->db_base;
				while (mp->b_cont) {
					mp = mp->b_cont;
					ipf->ipf_count += mp->b_datap->db_lim -
					    mp->b_datap->db_base;
				}
				ipf->ipf_tail_mp = mp;
				/*
				 * Keep track of next expected offset in
				 * ipf_end.
				 */
				ipf->ipf_end = end;
				ipf->ipf_stripped_hdr_len = 0;
				ipf->ipf_nf_hdr_len = hdr_length;
			} else {
				/* Hard case, hole at the beginning. */
				ipf->ipf_tail_mp = NULL;
				/*
				 * ipf_end == 0 means that we have given up
				 * on easy reassembly.
				 */
				ipf->ipf_end = 0;
				/*
				 * Toss the partial checksum since it is to
				 * hard to compute it with potentially
				 * overlapping fragments.
				 */
				ipf->ipf_checksum_valid = 0;
				/*
				 * ipf_hole_cnt and ipf_stripped_hdr_len are
				 * set by ip_reassemble.
				 *
				 * ipf_count is updated by ip_reassemble.
				 */
				(void) ip_reassemble(mp, ipf,
				    (u1 & IPH_OFFSET) << 3,
				    (u1 & IPH_MF),
				    hdr_length, ill);
			}
			/* Update per ipfb and ill byte counts */
			ipfb->ipfb_count += ipf->ipf_count;
			ASSERT(ipfb->ipfb_count > 0);	/* Wraparound */
			ill->ill_frag_count += ipf->ipf_count;
			ASSERT(ill->ill_frag_count > 0); /* Wraparound */
			/* If the frag timer wasn't already going, start it. */
			if (ill->ill_frag_timer_id == 0) {
				ill->ill_frag_timer_id = qtimeout(ill->ill_rq,
				    ill_frag_timer, ill,
				    MSEC_TO_TICK(ip_g_frag_timo_ms));
			}
			goto reass_done;
		}

		/*
		 * We have a new piece of a datagram which is already being
		 * reassembled.
		 */
		if (offset && ipf->ipf_end == offset) {
			/* The new fragment fits at the end */
			ipf->ipf_tail_mp->b_cont = mp;
			/* Update the partial checksum and byte count */
			sum += ipf->ipf_checksum;
			sum = (sum & 0xFFFF) + (sum >> 16);
			sum = (sum & 0xFFFF) + (sum >> 16);
			ipf->ipf_checksum = (uint16_t)sum;
			count = mp->b_datap->db_lim -
				mp->b_datap->db_base;
			while (mp->b_cont) {
				mp = mp->b_cont;
				count += mp->b_datap->db_lim -
				    mp->b_datap->db_base;
			}
			ipf->ipf_count += count;
			/* Update per ipfb and ill byte counts */
			ipfb->ipfb_count += count;
			ASSERT(ipfb->ipfb_count > 0);	/* Wraparound */
			ill->ill_frag_count += count;
			ASSERT(ill->ill_frag_count > 0); /* Wraparound */
			if (u1 & IPH_MF) {
				/* More to come. */
				ipf->ipf_end = end;
				ipf->ipf_tail_mp = mp;
				goto reass_done;
			}
		} else {
			/* Go do the hard cases. */
			/*
			 * Toss the partial checksum since it is to hard to
			 * compute it with potentially overlapping fragments.
			 * Call ip_reassable().
			 */
			boolean_t ret;

			if (offset == 0)
				ipf->ipf_nf_hdr_len = hdr_length;

			/* Save current byte count */
			count = ipf->ipf_count;
			ipf->ipf_checksum_valid = 0;
			ret = ip_reassemble(mp, ipf, (u1  & IPH_OFFSET) << 3,
			    (u1 & IPH_MF),
			    offset ? hdr_length : 0, ill);
			/* Count of bytes added and subtracted (freeb()ed) */
			count = ipf->ipf_count - count;
			if (count) {
				/* Update per ipfb and ill byte counts */
				ipfb->ipfb_count += count;
				ASSERT(ipfb->ipfb_count > 0); /* Wraparound */
				ill->ill_frag_count += count;
				ASSERT(ill->ill_frag_count > 0);
			}
			if (! ret)
				goto reass_done;
			/* Return value of 'true' means mp is complete. */
		}
		/*
		 * We have completed reassembly.  Unhook the frag header from
		 * the reassembly list.
		 */
		BUMP_MIB(ip_mib.ipReasmOKs);
		ipfp = ipf->ipf_ptphn;
		mp1 = ipf->ipf_mp;
		sum_valid = ipf->ipf_checksum_valid;
		sum = ipf->ipf_checksum;
		count = ipf->ipf_count;
		ipf = ipf->ipf_hash_next;
		if (ipf)
			ipf->ipf_ptphn = ipfp;
		ipfp[0] = ipf;
		ill->ill_frag_count -= count;
		ASSERT(ipfb->ipfb_count >= count);
		ipfb->ipfb_count -= count;
		mutex_exit(&ipfb->ipfb_lock);
		/* Ditch the frag header. */
		mp = mp1->b_cont;

		freeb(mp1);

		/* Restore original IP length in header. */
		u2 = (uint32_t)msgdsize(mp);
		if (u2 > IP_MAXPACKET) {
			freemsg(mp);
			BUMP_MIB(ip_mib.ipInHdrErrors);
			TRACE_2(TR_FAC_IP, TR_IP_RPUT_LOCL_END,
			    "ip_rput_locl_end: q %p (%S)", q, "ipReasmFails");
			return;
		}

		if (mp->b_datap->db_ref > 1) {
			mblk_t *mp2;

			mp2 = copymsg(mp);
			freemsg(mp);
			if (!mp2) {
				BUMP_MIB(ip_mib.ipInDiscards);
				return;
			}
			mp = mp2;
		}
		ipha = (ipha_t *)mp->b_rptr;

		ipha->ipha_length = htons((uint16_t)u2);
		/* We're now complete, zip the frag state */
		ipha->ipha_fragment_offset_and_flags = 0;
		/*
		 * Make sure that first_mp points back to mp as
		 * the mp we came in with could have changed.
		 */
		ASSERT(!mctl_present);
		first_mp = mp;
	}

	/* Now we have a complete datagram, destined for this machine. */
	u1 = IPH_HDR_LENGTH(ipha);
	dst = ipha->ipha_dst;
nottcp:
	switch (ipha->ipha_protocol) {
	case IPPROTO_ICMP:
		icmp_inbound(q, first_mp, ire->ire_type, ill, sum_valid, sum,
		    mctl_present);
		TRACE_2(TR_FAC_IP, TR_IP_RPUT_LOCL_END,
		    "ip_rput_locl_end: q %p (%S)", q, "icmp");
		return;
	case IPPROTO_IGMP:
		/*
		 * If we are not willing to accept IGMP packets in clear,
		 * then check with global policy.
		 */
		if (igmp_accept_clear_messages == 0) {
			if (!ipsec_check_global_policy(first_mp, NULL)) {
				freemsg(first_mp);
				return;
			}
		}
		if (igmp_input(q, mp, ill)) {
			/* Bad packet - discarded by igmp_input */
			TRACE_2(TR_FAC_IP, TR_IP_RPUT_LOCL_END,
			    "ip_rput_locl_end: q %p (%S)", q, "igmp");
			if (mctl_present)
				freeb(first_mp);
			return;
		}
		/*
		 * igmp_input() may have pulled up the message so ipha needs to
		 * be reinitialized.
		 */
		ipha = (ipha_t *)mp->b_rptr;
		if (ipc_proto_fanout[ipha->ipha_protocol].icf_ipc == NULL) {
			/* No user-level listener for IGMP packets */
			goto drop_pkt;
		}
		/* deliver to local raw users */
		break;
	case IPPROTO_PIM:
		/*
		 * If we are not willing to accept PIM packets in clear,
		 * then check with global policy.
		 */
		if (pim_accept_clear_messages == 0) {
			if (!ipsec_check_global_policy(first_mp, NULL)) {
				freemsg(first_mp);
				return;
			}
		}
		if (pim_input(q, mp) != 0) {
			/* Bad packet - discarded by pim_input */
			TRACE_2(TR_FAC_IP, TR_IP_RPUT_LOCL_END,
			    "ip_rput_locl_end: q %p (%S)", q, "pim");
			if (mctl_present)
				freeb(first_mp);
			return;
		}

		/*
		 * pim_input() may have pulled up the message so ipha needs to
		 * be reinitialized.
		 */
		ipha = (ipha_t *)mp->b_rptr;
		if (ipc_proto_fanout[ipha->ipha_protocol].icf_ipc == NULL) {
			/* No user-level listener for PIM packets */
			goto drop_pkt;
		}
		/* deliver to local raw users */
		break;
	case IPPROTO_ENCAP:
		/*
		 * Handle self-encapsulated packets (IP-in-IP where
		 * the inner addresses == the outer addresses).
		 */
		hdr_length = IPH_HDR_LENGTH(ipha);
		if ((uchar_t *)ipha + hdr_length + sizeof (ipha_t) >
		    mp->b_wptr) {
			if (!pullupmsg(mp, (uchar_t *)ipha + hdr_length +
			    sizeof (ipha_t) - mp->b_rptr)) {
				BUMP_MIB(ip_mib.ipInDiscards);
				freemsg(first_mp);
				return;
			}
			ipha = (ipha_t *)mp->b_rptr;
		}
		inner_ipha = (ipha_t *)((uchar_t *)ipha + hdr_length);
		if (inner_ipha->ipha_src == ipha->ipha_src &&
		    inner_ipha->ipha_dst == ipha->ipha_dst) {
			ipsec_in_t *ii;

			/*
			 * Self-encapsulated tunnel packet. Remove
			 * the outer IP header and fanout again.
			 * We also need to make sure that the inner
			 * header is pulled up until options.
			 */
			mp->b_rptr = (uchar_t *)inner_ipha;
			ipha = inner_ipha;
			hdr_length = IPH_HDR_LENGTH(ipha);
			if ((uchar_t *)ipha + hdr_length > mp->b_wptr) {
				if (!pullupmsg(mp, (uchar_t *)ipha +
				    + hdr_length - mp->b_rptr)) {
					freemsg(first_mp);
					return;
				}
				ipha = (ipha_t *)mp->b_rptr;
			}
			if (!mctl_present) {
				ASSERT(first_mp == mp);
				/*
				 * This means that somebody is sending
				 * Self-encapsualted packets without AH/ESP.
				 * If AH/ESP was present, we would have already
				 * allocated the first_mp.
				 */
				if ((first_mp = ipsec_in_alloc()) == NULL) {
					ip1dbg(("ip_rput_local: IPSEC_IN "
					    "allocation failure.\n"));
					BUMP_MIB(ip_mib.ipInDiscards);
					freemsg(mp);
					return;
				}
				first_mp->b_cont = mp;
			}
			/*
			 * We generally store the ill_index if we need to
			 * do IPSEC processing as we lose the ill queue when
			 * we come back. But in this case, we never should
			 * have to store the ill_index here as it should have
			 * been stored previously when we processed the
			 * AH/ESP header in this routine or for non-ipsec
			 * cases, we still have the queue. But for some bad
			 * packets from the wire, we can get to IPSEC after
			 * this and we better store the index for that case.
			 */
			ill = (ill_t *)q->q_ptr;
			ii = (ipsec_in_t *)first_mp->b_rptr;
			ii->ipsec_in_ill_index = ill->ill_index;
			ii->ipsec_in_decaps = B_TRUE;
			ip_rput_local(q, first_mp, ipha, ire, flags);
			return;
		}
		break;
	case IPPROTO_UDP:
		{
		/* Pull up the UDP header, if necessary. */
		if ((mp->b_wptr - mp->b_rptr) < (u1 + 8)) {
udppullup:
			if (!pullupmsg(mp, u1 + 8)) {
				BUMP_MIB(ip_mib.ipInDiscards);
				freemsg(first_mp);
				TRACE_2(TR_FAC_IP, TR_IP_RPUT_LOCL_END,
				    "ip_rput_locl_end: q %p (%S)", q, "udp");
				return;
			}
			ipha = (ipha_t *)mp->b_rptr;
		}
		/*
		 * Validate the checksum.  This code is a bit funny looking
		 * but may help out the compiler in this crucial spot.
		 */
		up = (uint16_t *)((uchar_t *)ipha + u1 + UDP_PORTS_OFFSET);
		if (up[3]) {
			if (sum_valid) {
				sum += IP_UDP_CSUM_COMP + iphs[6] + iphs[7] +
				    iphs[8] + iphs[9] + up[2];
				sum = (sum & 0xFFFF) + (sum >> 16);
				sum = ~(sum + (sum >> 16)) & 0xFFFF;
				if (sum && sum != 0xFFFF) {
					ip1dbg(("ip_rput_local: "
					    "bad udp checksum 0x%x\n", sum));
					BUMP_MIB(ip_mib.udpInCksumErrs);
					goto drop_pkt;
				}
			} else {
				sum = IP_CSUM(mp,
				    (int32_t)((uchar_t *)up - (uchar_t *)ipha),
				    IP_UDP_CSUM_COMP + iphs[6] +
				    iphs[7] + iphs[8] +
				    iphs[9] + up[2]);
				if (sum != 0) {
badchecksum:
					ip1dbg(("ip_rput_local: "
					    "bad udp checksum 0x%x\n", sum));
					BUMP_MIB(ip_mib.udpInCksumErrs);
					goto drop_pkt;
				}
			}
		}
udpslowpath:
		ports = *(uint32_t *)up;
		ip_fanout_udp(q, first_mp, ill, ipha, ports, ire->ire_type,
		    IP_FF_SEND_ICMP | IP_FF_CKSUM, mctl_present);
		TRACE_2(TR_FAC_IP, TR_IP_RPUT_LOCL_END,
		    "ip_rput_locl_end: q %p (%S)", q, "ip_fanout_udp");
		return;
	}

	case IPPROTO_TCP:
		{
		len = mp->b_wptr - mp->b_rptr;

		/* Pull up a minimal TCP header, if necessary. */
		if (len < (u1 + 20)) {
tcppullup:
			if (!pullupmsg(mp, u1 + 20)) {
				BUMP_MIB(ip_mib.ipInDiscards);
				freemsg(first_mp);
				TRACE_2(TR_FAC_IP, TR_IP_RPUT_LOCL_END,
				    "ip_rput_locl_end: q %p (%S)", q,
				    "tcp:badpullup");
				return;
			}
			ipha = (ipha_t *)mp->b_rptr;
			len = mp->b_wptr - mp->b_rptr;
		}

		/*
		 * Extract the offset field from the TCP header.  As usual, we
		 * try to help the compiler more than the reader.
		 */
		offset = ((uchar_t *)ipha)[u1 + 12] >> 4;
		if (offset != 5) {
tcpoptions:
			if (offset < 5) {
				freemsg(first_mp);
				TRACE_2(TR_FAC_IP, TR_IP_RPUT_LOCL_END,
				    "ip_rput_locl_end: q %p (%S)", q,
				    "tcp_badoff");
				return;
			}
			/*
			 * There must be TCP options.
			 * Make sure we can grab them.
			 */
			offset <<= 2;
			offset += u1;
			if (len < offset) {
				if (!pullupmsg(mp, offset)) {
					BUMP_MIB(ip_mib.ipInDiscards);
					freemsg(first_mp);
					TRACE_2(TR_FAC_IP, TR_IP_RPUT_LOCL_END,
					    "ip_rput_locl_end: q %p (%S)", q,
					    "tcp:pullupfailed");
					return;
				}
				ipha = (ipha_t *)mp->b_rptr;
				len = mp->b_wptr - rptr;
			}
		}

		/* Get the total packet length in len, including headers. */
		if (mp->b_cont) {
multipkttcp:
			len = msgdsize(mp);
		}
#ifdef ZC_TEST
		zckstat->zc_slowpath_r.value.ul++;
#endif
		/*
		 * Check the TCP checksum by pulling together the pseudo-
		 * header checksum, and passing it to ip_csum to be added in
		 * with the TCP datagram.
		 *
		 * Since we are not using the hwcksum if available we must
		 * clear the flag. We may come here via tcppullup or tcpoptions.
		 * If either of these fails along the way the mblk is freed.
		 * If this logic ever changes and mblk is reused to say send
		 * ICMP's back, then this flag may need to be cleared in
		 * other places as well.
		 */
		mp->b_ick_flag &= ~ICK_VALID;
		up = (uint16_t *)(rptr + u1 + TCP_PORTS_OFFSET);
		ports = *(uint32_t *)up;
		u1 = (uint32_t)(len - u1);	/* TCP datagram length. */
#ifdef	_BIG_ENDIAN
		u1 += IPPROTO_TCP;
#else
		u1 = ((u1 >> 8) & 0xFF) + (((u1 & 0xFF) + IPPROTO_TCP) << 8);
#endif
		u1 += iphs[6] + iphs[7] + iphs[8] + iphs[9];
		dp = mp->b_datap;
		if (dp->db_type != M_DATA || dp->db_ref > 1 || mctl_present
		    /* BEGIN CSTYLED */
#ifdef ZC_TEST
		    || !syncstream
#endif
			) {
			/* END CSTYLED */
			/*
			 * Not M_DATA mblk or its a dup, so do the checksum now.
			 */
			if (IP_CSUM(mp, (int32_t)((uchar_t *)up - rptr), u1)) {
tcpcksumerr:
				BUMP_MIB(ip_mib.tcpInErrs);
				goto drop_pkt;
			}
		} else {
			/*
			 * M_DATA mblk and not a dup, so postpone the checksum.
			 */
			mblk_t	*mp1 = mp;
			dblk_t	*dp1;

			u1 = (u1 >> 16) + (u1 & 0xffff);
			u1 += (u1 >> 16);
			*(uint16_t *)dp->db_struioun.data = (uint16_t)u1;
			dp->db_struiobase = (uchar_t *)up;
			dp->db_struioptr = (uchar_t *)up;
			dp->db_struiolim = mp->b_wptr;
			dp->db_struioflag |= STRUIO_SPEC|STRUIO_IP;
			while ((mp1 = mp1->b_cont) != NULL) {
				dp1 = mp1->b_datap;
				*(uint16_t *)dp1->db_struioun.data = 0;
				dp1->db_struiobase = mp1->b_rptr;
				dp1->db_struioptr = mp1->b_rptr;
				dp1->db_struiolim = mp1->b_wptr;
				dp1->db_struioflag |= STRUIO_SPEC|STRUIO_IP;
			}
		}

notcpport:
		/*
		 * Drop any incoming packets for TCP, addressed to
		 * a broadcast address (destination).
		 */
		if (ire->ire_type == IRE_BROADCAST)
			goto drop_pkt;
		ip_fanout_tcp(q, first_mp, ipha, ports,
		    IP_FF_SEND_ICMP | IP_FF_CKSUM | IP_FF_SYN_ADDIRE | flags,
		    mctl_present);
		TRACE_2(TR_FAC_IP, TR_IP_RPUT_LOCL_END,
		    "ip_rput_locl_end: q %p (%S)", q, "ip_fanout_tcp");
		return;
		}
	case IPPROTO_AH:
	case IPPROTO_ESP:
		/*
		 * Fast path for AH/ESP. If this is the first time
		 * we are sending a datgram to AH/ESP, allocate
		 * a IPSEC_IN message and prepend it. Otherwise,
		 * just fanout.
		 */
		if (!mctl_present) {
			ipsec_in_t *ii;

			ASSERT(first_mp == mp);
			if ((first_mp = ipsec_in_alloc()) == NULL) {
				ip1dbg(("ip_rput_local: IPSEC_IN "
				    "allocation failure.\n"));
				BUMP_MIB(ip_mib.ipInDiscards);
				freemsg(mp);
				return;
			}
			/*
			 * Store the ill_index so that when we come back
			 * from IPSEC we ride on the same queue.
			 */
			ill = (ill_t *)q->q_ptr;
			ii = (ipsec_in_t *)first_mp->b_rptr;
			ii->ipsec_in_ill_index = ill->ill_index;
			first_mp->b_cont = mp;
		}
		ip_fanout_sec_proto(q, first_mp, ipha->ipha_protocol,
		    IP_FF_SEND_ICMP);
		return;
	default:
		break;
	}
	/*
	 * Handle protocols with which IP is less intimate.  There
	 * can be more than one stream bound to a particular
	 * protocol.  When this is the case, each one gets a copy
	 * of any incoming packets.
	 */
	ip_fanout_proto(q, first_mp, ill, ipha,
	    IP_FF_SEND_ICMP | IP_FF_CKSUM | IP_FF_RAWIP, mctl_present);
	TRACE_2(TR_FAC_IP, TR_IP_RPUT_LOCL_END,
	    "ip_rput_locl_end: q %p (%S)", q, "ip_fanout_proto");
	return;

drop_pkt:;
	freemsg(first_mp);
	TRACE_2(TR_FAC_IP, TR_IP_RPUT_LOCL_END,
	    "ip_rput_locl_end: q %p (%S)", q, "droppkt");
#undef	rptr
}

/* Update any source route, record route or timestamp options */
/* Check that we are at end of strict source route */
static int
ip_rput_local_options(queue_t *q, mblk_t *mp, ipha_t *ipha, ire_t *ire)
{
	uint32_t	totallen;
	uchar_t		*opt;
	uint32_t	optval;
	uint32_t	optlen;
	ipaddr_t	dst;
	uint32_t	ts;
	ire_t		*dst_ire;

	ASSERT(ire->ire_ipversion == IPV4_VERSION);

	ip2dbg(("ip_rput_local_options\n"));
	totallen = ipha->ipha_version_and_hdr_length -
		(uint8_t)((IP_VERSION << 4) + IP_SIMPLE_HDR_LENGTH_IN_WORDS);
	opt = (uchar_t *)&ipha[1];
	totallen <<= 2;
	while (totallen != 0) {
		switch (optval = opt[IPOPT_POS_VAL]) {
		case IPOPT_EOL:
			return (0);
		case IPOPT_NOP:
			optlen = 1;
			break;
		default:
			optlen = opt[IPOPT_POS_LEN];
		}
		ip2dbg(("ip_rput_local_options: opt %d, len %d\n",
		    optval, optlen));

		if (optlen == 0 || optlen > totallen)
			break;

		switch (optval) {
			uint32_t off;
		case IPOPT_SSRR:
		case IPOPT_LSRR:
			off = opt[IPOPT_POS_OFF];
			off--;
			if (optlen < IP_ADDR_LEN ||
			    off > optlen - IP_ADDR_LEN) {
				/* End of source route */
				ip1dbg(("ip_rput_local_options: end of SR\n"));
				break;
			}
			/*
			 * This will only happen if two consecutive entries
			 * in the source route contains our address or if
			 * it is a packet with a loose source route which
			 * reaches us before consuming the whole source route
			 */
			ip1dbg(("ip_rput_local_options: not end of SR\n"));
			if (optval == IPOPT_SSRR) {
				goto bad_src_route;
			}
			/*
			 * Hack: instead of dropping the packet truncate the
			 * source route to what has been used.
			 */
			bzero((char *)opt + off, optlen - off);
			opt[IPOPT_POS_LEN] = (uint8_t)off;
			break;
		case IPOPT_RR:
			off = opt[IPOPT_POS_OFF];
			off--;
			if (optlen < IP_ADDR_LEN ||
			    off > optlen - IP_ADDR_LEN) {
				/* No more room - ignore */
				ip1dbg((
				    "ip_rput_local_options: end of RR\n"));
				break;
			}
			bcopy(&ire->ire_src_addr, (char *)opt + off,
			    IP_ADDR_LEN);
			opt[IPOPT_POS_OFF] += IP_ADDR_LEN;
			break;
		case IPOPT_IT:
			/* Insert timestamp if there is romm */
			switch (opt[IPOPT_POS_OV_FLG] & 0x0F) {
			case IPOPT_IT_TIME:
				off = IPOPT_IT_TIMELEN;
				break;
			case IPOPT_IT_SPEC:
#ifdef IPOPT_IT_SPEC_BSD
			case IPOPT_IT_SPEC_BSD:
#endif
				/* Verify that the address matched */
				off = opt[IPOPT_POS_OFF] - 1;
				bcopy((char *)opt + off, &dst, IP_ADDR_LEN);
				dst_ire = ire_ctable_lookup(dst, 0, IRE_LOCAL,
				    NULL, NULL, MATCH_IRE_TYPE);
				if (dst_ire == NULL) {
					/* Not for us */
					break;
				}
				ire_refrele(dst_ire);
				/* FALLTHRU */
			case IPOPT_IT_TIME_ADDR:
				off = IP_ADDR_LEN + IPOPT_IT_TIMELEN;
				break;
			default:
				/*
				 * ip_*put_options should have already
				 * dropped this packet.
				 */
				cmn_err(CE_PANIC, "ip_rput_local_options: "
				    "unknown IT - bug in ip_rput_options?\n");
				return (0);	/* Keep "lint" happy */
			}
			if (opt[IPOPT_POS_OFF] - 1 + off > optlen) {
				/* Increase overflow counter */
				off = (opt[IPOPT_POS_OV_FLG] >> 4) + 1;
				opt[IPOPT_POS_OV_FLG] =
				    (uint8_t)((opt[IPOPT_POS_OV_FLG] & 0x0F) |
				    (off << 4));
				break;
			}
			off = opt[IPOPT_POS_OFF] - 1;
			switch (opt[IPOPT_POS_OV_FLG] & 0x0F) {
			case IPOPT_IT_SPEC:
#ifdef IPOPT_IT_SPEC_BSD
			case IPOPT_IT_SPEC_BSD:
#endif
			case IPOPT_IT_TIME_ADDR:
				bcopy(&ire->ire_src_addr, (char *)opt + off,
				    IP_ADDR_LEN);
				opt[IPOPT_POS_OFF] += IP_ADDR_LEN;
				/* FALLTHRU */
			case IPOPT_IT_TIME:
				off = opt[IPOPT_POS_OFF] - 1;
				/* Compute # of milliseconds since midnight */
				ts = (hrestime.tv_sec % (24 * 60 * 60)) * 1000 +
					hrestime.tv_nsec / (NANOSEC / MILLISEC);
				bcopy(&ts, (char *)opt + off, IPOPT_IT_TIMELEN);
				opt[IPOPT_POS_OFF] += IPOPT_IT_TIMELEN;
				break;
			}
			break;
		}
		totallen -= optlen;
		opt += optlen;
	}
	return (0);

bad_src_route:
	q = WR(q);
	/* make sure we clear any indication of a hardware checksum */
	mp->b_ick_flag &= ~ICK_VALID;
	icmp_unreachable(q, mp, ICMP_SOURCE_ROUTE_FAILED);
	return (-1);

}

/*
 * Process IP options in an inbound packet.  If an option affects the
 * effective destination address, return the next hop address via dstp.
 * Returns -1 if something fails in which case an ICMP error has been sent
 * and mp freed.
 */
static int
ip_rput_options(queue_t *q, mblk_t *mp, ipha_t *ipha, ipaddr_t *dstp)
{
	uint32_t	totallen;
	uchar_t		*opt;
	uint32_t	optval;
	uint32_t	optlen;
	ipaddr_t	dst;
	intptr_t	code = 0;
	ire_t		*ire = NULL;

	ip2dbg(("ip_rput_options\n"));
	dst = ipha->ipha_dst;
	totallen = ipha->ipha_version_and_hdr_length -
		(uint8_t)((IP_VERSION << 4) + IP_SIMPLE_HDR_LENGTH_IN_WORDS);
	opt = (uchar_t *)&ipha[1];
	totallen <<= 2;
	while (totallen != 0) {
		switch (optval = opt[IPOPT_POS_VAL]) {
		case IPOPT_EOL:
			*dstp = dst;
			return (0);
		case IPOPT_NOP:
			optlen = 1;
			break;
		default:
			optlen = opt[IPOPT_POS_LEN];
		}
		if (optlen == 0 || optlen > totallen) {
			ip1dbg(("ip_rput_options: bad option len %d, %d\n",
			    optlen, totallen));
			code = (char *)&opt[IPOPT_POS_LEN] - (char *)ipha;
			goto param_prob;
		}
		ip2dbg(("ip_rput_options: opt %d, len %d\n",
		    optval, optlen));
		/*
		 * Note: we need to verify the checksum before we
		 * modify anything thus this routine only extracts the next
		 * hop dst from any source route.
		 */
		switch (optval) {
			uint32_t off;
		case IPOPT_SSRR:
		case IPOPT_LSRR:
			ire = ire_ctable_lookup(dst, 0, IRE_LOCAL, NULL,
			    NULL, MATCH_IRE_TYPE);
			if (ire == NULL) {
				if (optval == IPOPT_SSRR) {
					ip1dbg(("ip_rput_options: not next"
					    " strict source route 0x%x\n",
					    ntohl(dst)));
					code = (char *)&ipha->ipha_dst -
					    (char *)ipha;
					goto param_prob; /* RouterReq's */
				}
				ip2dbg(("ip_rput_options: "
				    "not next source route 0x%x\n",
				    ntohl(dst)));
				break;
			}
			ire_refrele(ire);
			off = opt[IPOPT_POS_OFF];
			if (off < IPOPT_MINOFF_SR) {
				ip1dbg(("ip_rput_options: bad option"
				    " offset %d\n",
				    off));
				code = (char *)&opt[IPOPT_POS_OFF] -
				    (char *)ipha;
				goto param_prob;
			}
			off--;
		redo_srr:
			if (optlen < IP_ADDR_LEN ||
			    off > optlen - IP_ADDR_LEN) {
				/* End of source route */
				ip1dbg(("ip_rput_options: end of SR\n"));
				break;
			}
			bcopy((char *)opt + off, &dst, IP_ADDR_LEN);
			ip1dbg(("ip_rput_options: next hop 0x%x\n",
			    ntohl(dst)));

			/*
			 * Check if our address is present more than
			 * once as consecutive hops in source route.
			 * XXX verify per-interface ip_forwarding
			 * for source route?
			 */
			ire = ire_ctable_lookup(dst, 0, IRE_LOCAL, NULL,
			    NULL, MATCH_IRE_TYPE);

			if (ire != NULL) {
				ire_refrele(ire);
				off += IP_ADDR_LEN;
				goto redo_srr;
			}

			if (dst == htonl(INADDR_LOOPBACK)) {
				ip1dbg(("ip_rput_options: loopback addr in "
				    "source route!\n"));
				goto bad_src_route;
			}
			/*
			 * For strict: verify that dst is directly
			 * reachable.
			 */
			if (optval == IPOPT_SSRR) {
				ire = ire_ftable_lookup(dst, 0, 0,
				    IRE_INTERFACE, NULL, NULL, NULL, 0,
				    MATCH_IRE_TYPE);
				if (ire == NULL) {
					ip1dbg(("ip_rput_options: SSRR not "
					    "directly reachable: 0x%x\n",
					    ntohl(dst)));
					goto bad_src_route;
				}
				ire_refrele(ire);
			}
			/*
			 * Defer update of the offset and the record route
			 * until the packet is forwarded.
			 */
			break;
		case IPOPT_RR:
			off = opt[IPOPT_POS_OFF];
			if (off < IPOPT_MINOFF_SR) {
				ip1dbg((
				    "ip_rput_options: bad option offset %d\n",
				    off));
				code = (char *)&opt[IPOPT_POS_OFF] -
				    (char *)ipha;
				goto param_prob;
			}
			break;
		case IPOPT_IT:
			/*
			 * Verify that length >= 5 and that there is either
			 * room for another timestamp or that the overflow
			 * counter is not maxed out.
			 */
			code = (char *)&opt[IPOPT_POS_LEN] - (char *)ipha;
			if (optlen < IPOPT_MINLEN_IT) {
				goto param_prob;
			}
			switch (opt[IPOPT_POS_OV_FLG] & 0x0F) {
			case IPOPT_IT_TIME:
				off = IPOPT_IT_TIMELEN;
				break;
			case IPOPT_IT_TIME_ADDR:
			case IPOPT_IT_SPEC:
#ifdef IPOPT_IT_SPEC_BSD
			case IPOPT_IT_SPEC_BSD:
#endif
				off = IP_ADDR_LEN + IPOPT_IT_TIMELEN;
				break;
			default:
				code = (char *)&opt[IPOPT_POS_OV_FLG] -
				    (char *)ipha;
				goto param_prob;
			}
			if (opt[IPOPT_POS_OFF] - 1 + off > optlen &&
			    (opt[IPOPT_POS_OV_FLG] & 0xF0) == 0xF0) {
				/*
				 * No room and the overflow counter is 15
				 * already.
				 */
				goto param_prob;
			}
			off = opt[IPOPT_POS_OFF];
			if (off < IPOPT_MINOFF_IT) {
				ip1dbg((
				    "ip_rput_options: bad option offset %d\n",
				    off));
				code = (char *)&opt[IPOPT_POS_OFF] -
					(char *)ipha;
				goto param_prob;
			}
			break;
		}
		totallen -= optlen;
		opt += optlen;
	}
	*dstp = dst;
	return (0);

param_prob:
	q = WR(q);
	/* make sure we clear any indication of a hardware checksum */
	mp->b_ick_flag &= ~ICK_VALID;
	icmp_param_problem(q, mp, (uint8_t)code);
	return (-1);

bad_src_route:
	q = WR(q);
	/* make sure we clear any indication of a hardware checksum */
	mp->b_ick_flag &= ~ICK_VALID;
	icmp_unreachable(q, mp, ICMP_SOURCE_ROUTE_FAILED);
	return (-1);
}

/*
 * Read service routine.  We see three kinds of messages here.  The first is
 * fragmentation reassembly timer messages.  The second is data packets
 * that were queued to avoid possible intra-machine looping conditions
 * on Streams implementations that do not support QUEUEPAIR synchronization.
 * The third is the IRE expiration timer message.
 */
void
ip_rsrv(queue_t *q)
{
	mblk_t	*mp;
	ipt_t	*ipt;

	TRACE_1(TR_FAC_IP, TR_IP_RSRV_START, "ip_rsrv_start: q %p", q);

	while ((mp = getq(q)) != NULL) {
		/* TODO need define for M_PCSIG! */
		if (mp->b_datap->db_type == M_PCSIG) {
			/* Timer */
			if (!mi_timer_valid(mp))
				continue;
			if (mp == igmp_timer_mp) {
				/* It's the IGMP timer. */
				igmp_timeout();
				continue;
			}
			if (mp == mld_timer_mp) {
				/* It's the MLD timer. */
				mld_timeout();
				continue;
			}
			/* Must be nce/IRE request */
			ipt = (ipt_t *)mp->b_rptr;
			(*ipt->func)(ipt->arg);
			continue;
		}
		/* Intramachine packet forwarding. */
		putnext(q, mp);
	}
	TRACE_1(TR_FAC_IP, TR_IP_RSRV_END,
	    "ip_rsrv_end: q %p", q);
}

/*
 * IP & ICMP info in >=14 msg's ...
 *  - ip fixed part (mib2_ip_t)
 *  - icmp fixed part (mib2_icmp_t)
 *  - ipAddrEntryTable (ip 20)		all IPv4 ipifs
 *  - ipRouteEntryTable (ip 21)		all IPv4 IREs
 *  - ipNetToMediaEntryTable (ip 22)	IPv4 IREs for on-link destinations
 *  - ip multicast membership (ip_member_t)
 *  - igmp fixed part (struct igmpstat)
 *  - multicast routing stats (struct mrtstat)
 *  - multicast routing vifs (array of struct vifctl)
 *  - multicast routing routes (array of struct mfcctl)
 *  - ip6 fixed part (mib2_ipv6IfStatsEntry_t)
 *					One per ill plus one generic
 *  - icmp6 fixed part (mib2_ipv6IfIcmpEntry_t)
 *					One per ill plus one generic
 *  - ipv6RouteEntry			all IPv6 IREs
 *  - ipv6NetToMediaEntry		all Neighbor Cache entries
 *  - ipv6AddrEntry			all IPv6 ipifs
 *  - ipv6 multicast membership (ipv6_member_t)
 *
 * IP_ROUTE and IP_MEDIA are augmented in arp to include arp cache entries not
 * already present.
 * NOTE: original mpctl is copied for msg's 2..N, since its ctl part
 * already filled in by caller.
 * Return value of 0 indicates that no messages were sent and caller
 * should free mpctl.
 */
static int
ip_snmp_get(queue_t *q, mblk_t *mpctl)
{

	if (mpctl == NULL || mpctl->b_cont == NULL) {
		return (0);
	}

	if ((mpctl = ip_snmp_get_mib2_ip(q, mpctl)) == NULL) {
		return (1);
	}

	if ((mpctl = ip_snmp_get_mib2_ip6(q, mpctl)) == NULL) {
		return (1);
	}

	if ((mpctl = ip_snmp_get_mib2_icmp(q, mpctl)) == NULL) {
		return (1);
	}

	if ((mpctl = ip_snmp_get_mib2_icmp6(q, mpctl)) == NULL) {
		return (1);
	}

	if ((mpctl = ip_snmp_get_mib2_igmp(q, mpctl)) == NULL) {
		return (1);
	}

	if ((mpctl = ip_snmp_get_mib2_multi(q, mpctl)) == NULL) {
		return (1);
	}

	if ((mpctl = ip_snmp_get_mib2_ip_addr(q, mpctl)) == NULL) {
		return (1);
	}

	if ((mpctl = ip_snmp_get_mib2_ip6_addr(q, mpctl)) == NULL) {
		return (1);
	}

	if ((mpctl = ip_snmp_get_mib2_ip_group_mem(q, mpctl)) == NULL) {
		return (1);
	}

	if ((mpctl = ip_snmp_get_mib2_ip6_group_mem(q, mpctl)) == NULL) {
		return (1);
	}

	if ((mpctl = ip_snmp_get_mib2_virt_multi(q, mpctl)) == NULL) {
		return (1);
	}

	if ((mpctl = ip_snmp_get_mib2_multi_rtable(q, mpctl)) == NULL) {
		return (1);
	}

	if ((mpctl = ip_snmp_get_mib2_ip_route_media(q, mpctl)) == NULL) {
		return (1);
	}

	if ((mpctl = ip_snmp_get_mib2_ip6_route_media(q, mpctl)) == NULL) {
		return (1);
	}
	freemsg(mpctl);
	return (1);
}


/* Get global IPv4 statistics */
static mblk_t *
ip_snmp_get_mib2_ip(queue_t *q, mblk_t *mpctl)
{
	struct opthdr		*optp;
	mblk_t			*mp2ctl;

	/*
	 * make a copy of the original message
	 */
	mp2ctl = copymsg(mpctl);

	/* fixed length IP structure... */
	optp = (struct opthdr *)&mpctl->b_rptr[sizeof (struct T_optmgmt_ack)];
	optp->level = MIB2_IP;
	optp->name = 0;
	SET_MIB(ip_mib.ipForwarding,
	    (WE_ARE_FORWARDING ? 1 : 2));
	SET_MIB(ip_mib.ipDefaultTTL,
	    (uint32_t)ip_def_ttl);
	SET_MIB(ip_mib.ipReasmTimeout,
	    ip_g_frag_timeout);
	SET_MIB(ip_mib.ipAddrEntrySize,
	    sizeof (mib2_ipAddrEntry_t));
	SET_MIB(ip_mib.ipRouteEntrySize,
	    sizeof (mib2_ipRouteEntry_t));
	SET_MIB(ip_mib.ipNetToMediaEntrySize,
	    sizeof (mib2_ipNetToMediaEntry_t));
	SET_MIB(ip_mib.ipMemberEntrySize, sizeof (ip_member_t));
	if (!snmp_append_data(mpctl->b_cont, (char *)&ip_mib,
	    (int)sizeof (ip_mib))) {
		ip1dbg(("ip_snmp_get_mib2_ip: failed to allocate %u bytes\n",
			(uint_t)sizeof (ip_mib)));
	}

	optp->len = (t_uscalar_t)msgdsize(mpctl->b_cont);
	ip1dbg(("ip_snmp_get_mib2_ip: level %d, name %d, len %d\n",
	    (int)optp->level, (int)optp->name, (int)optp->len));
	qreply(q, mpctl);
	return (mp2ctl);
}

/* Global IPv4 ICMP statistics */
static mblk_t *
ip_snmp_get_mib2_icmp(queue_t *q, mblk_t *mpctl)
{
	struct opthdr		*optp;
	mblk_t			*mp2ctl;

	/*
	 * Make a copy of the original message
	 */
	mp2ctl = copymsg(mpctl);

	optp = (struct opthdr *)&mpctl->b_rptr[sizeof (struct T_optmgmt_ack)];
	optp->level = MIB2_ICMP;
	optp->name = 0;
	if (!snmp_append_data(mpctl->b_cont, (char *)&icmp_mib,
	    (int)sizeof (icmp_mib))) {
		ip1dbg(("ip_snmp_get_mib2_icmp: failed to allocate %u bytes\n",
			(uint_t)sizeof (icmp_mib)));
	}
	optp->len = (t_uscalar_t)msgdsize(mpctl->b_cont);
	ip1dbg(("ip_snmp_get_mib2_icmp: level %d, name %d, len %d\n",
	    (int)optp->level, (int)optp->name, (int)optp->len));
	qreply(q, mpctl);
	return (mp2ctl);
}

/* Global IPv4 IGMP statistics */
static mblk_t *
ip_snmp_get_mib2_igmp(queue_t *q, mblk_t *mpctl)
{
	struct opthdr		*optp;
	mblk_t			*mp2ctl;

	/*
	 * make a copy of the original message
	 */
	mp2ctl = copymsg(mpctl);

	optp = (struct opthdr *)&mpctl->b_rptr[sizeof (struct T_optmgmt_ack)];
	optp->level = EXPER_IGMP;
	optp->name = 0;
	if (!snmp_append_data(mpctl->b_cont, (char *)&igmpstat,
	    (int)sizeof (igmpstat))) {
		ip1dbg(("ip_snmp_get_mib2_igmp: failed to allocate %u bytes\n",
			(uint_t)sizeof (igmpstat)));
	}
	optp->len = (t_uscalar_t)msgdsize(mpctl->b_cont);
	ip1dbg(("ip_snmp_get_mib2_igmp: level %d, name %d, len %d\n",
	    (int)optp->level, (int)optp->name, (int)optp->len));
	qreply(q, mpctl);
	return (mp2ctl);
}

/* Global IPv4 Multicast Routing statistics */
static mblk_t *
ip_snmp_get_mib2_multi(queue_t *q, mblk_t *mpctl)
{
	struct opthdr		*optp;
	mblk_t			*mp2ctl;

	/*
	 * make a copy of the original message
	 */
	mp2ctl = copymsg(mpctl);

	optp = (struct opthdr *)&mpctl->b_rptr[sizeof (struct T_optmgmt_ack)];
	optp->level = EXPER_DVMRP;
	optp->name = 0;
	if (!ip_mroute_stats(mpctl->b_cont)) {
		ip0dbg(("ip_mroute_stats: failed\n"));
	}
	optp->len = (t_uscalar_t)msgdsize(mpctl->b_cont);
	ip1dbg(("ip_snmp_get_mib2_multi: level %d, name %d, len %d\n",
	    (int)optp->level, (int)optp->name, (int)optp->len));
	qreply(q, mpctl);
	return (mp2ctl);
}

/* IPv4 address information */
static mblk_t *
ip_snmp_get_mib2_ip_addr(queue_t *q, mblk_t *mpctl)
{
	struct opthdr		*optp;
	mblk_t			*mp2ctl;
	mblk_t			*mp_tail = NULL;
	ill_t			*ill;
	ipif_t			*ipif;
	uint_t			bitval;
	mib2_ipAddrEntry_t	mae;
	ippc_t			ippc1;

	/*
	 * make a copy of the original message
	 */
	mp2ctl = copymsg(mpctl);

	/* ipAddrEntryTable */

	optp = (struct opthdr *)&mpctl->b_rptr[sizeof (struct T_optmgmt_ack)];
	optp->level = MIB2_IP;
	optp->name = MIB2_IP_ADDR;
	for (ill = ill_g_head; ill; ill = ill->ill_next) {
		if (ill->ill_isv6)
			continue;
		for (ipif = ill->ill_ipif; ipif; ipif = ipif->ipif_next) {
			ippc1.ippc_v6addr = ipif->ipif_v6lcl_addr;
			ippc1.ippc_ib_pkt_count = ipif->ipif_ib_pkt_count;
			ippc1.ippc_ob_pkt_count = ipif->ipif_ob_pkt_count;
			ippc1.ippc_fo_pkt_count = ipif->ipif_fo_pkt_count;
			ire_walk_v4(ire_pkt_count, (char *)&ippc1);

			mae.ipAdEntInfo.ae_ibcnt = ippc1.ippc_ib_pkt_count;
			mae.ipAdEntInfo.ae_obcnt = ippc1.ippc_ob_pkt_count;
			mae.ipAdEntInfo.ae_focnt = ippc1.ippc_fo_pkt_count;

			(void) ipif_get_name(ipif,
			    mae.ipAdEntIfIndex.o_bytes,
			    OCTET_LENGTH);
			mae.ipAdEntIfIndex.o_length =
			    mi_strlen(mae.ipAdEntIfIndex.o_bytes);
			mae.ipAdEntAddr = ipif->ipif_lcl_addr;
			mae.ipAdEntNetMask = ipif->ipif_net_mask;
			mae.ipAdEntInfo.ae_subnet = ipif->ipif_subnet;
			mae.ipAdEntInfo.ae_subnet_len =
			    ip_mask_to_index(ipif->ipif_net_mask);
			mae.ipAdEntInfo.ae_src_addr = ipif->ipif_src_addr;
			for (bitval = 1;
			    bitval &&
			    !(bitval & ipif->ipif_brd_addr);
			    bitval <<= 1)
				noop;
			mae.ipAdEntBcastAddr = bitval;
			mae.ipAdEntReasmMaxSize = 65535;
			mae.ipAdEntInfo.ae_mtu = ipif->ipif_mtu;
			mae.ipAdEntInfo.ae_metric  = ipif->ipif_metric;
			mae.ipAdEntInfo.ae_broadcast_addr =
			    ipif->ipif_brd_addr;
			mae.ipAdEntInfo.ae_pp_dst_addr =
			    ipif->ipif_pp_dst_addr;
			mae.ipAdEntInfo.ae_flags = ipif->ipif_flags;

			if (!snmp_append_data2(mpctl->b_cont, &mp_tail,
			    (char *)&mae, (int)sizeof (mib2_ipAddrEntry_t))) {
				ip1dbg(("ip_snmp_get_mib2_ip_addr: failed to "
				    "allocate %u bytes\n",
				    (uint_t)sizeof (mib2_ipAddrEntry_t)));
			}
		}
	}
	optp->len = (t_uscalar_t)msgdsize(mpctl->b_cont);
	ip1dbg(("ip_snmp_get_mib2_ip_addr: level %d, name %d, len %d\n",
	    (int)optp->level, (int)optp->name, (int)optp->len));
	qreply(q, mpctl);
	return (mp2ctl);
}

/* IPv6 address information */
static mblk_t *
ip_snmp_get_mib2_ip6_addr(queue_t *q, mblk_t *mpctl)
{
	struct opthdr		*optp;
	mblk_t			*mp2ctl;
	mblk_t			*mp_tail = NULL;
	ill_t			*ill;
	ipif_t			*ipif;
	mib2_ipv6AddrEntry_t	mae6;
	ippc_t			ippc1;

	/*
	 * make a copy of the original message
	 */
	mp2ctl = copymsg(mpctl);

	/* ipv6AddrEntryTable */

	optp = (struct opthdr *)&mpctl->b_rptr[sizeof (struct T_optmgmt_ack)];
	optp->level = MIB2_IP6;
	optp->name = MIB2_IP6_ADDR;
	for (ill = ill_g_head; ill; ill = ill->ill_next) {
		if (!ill->ill_isv6)
			continue;
		for (ipif = ill->ill_ipif; ipif; ipif = ipif->ipif_next) {
			ippc1.ippc_v6addr = ipif->ipif_v6lcl_addr;
			ippc1.ippc_ib_pkt_count = ipif->ipif_ib_pkt_count;
			ippc1.ippc_ob_pkt_count = ipif->ipif_ob_pkt_count;
			ippc1.ippc_fo_pkt_count = ipif->ipif_fo_pkt_count;
			ire_walk_v6(ire_pkt_count_v6, (char *)&ippc1);

			mae6.ipv6AddrInfo.ae_ibcnt = ippc1.ippc_ib_pkt_count;
			mae6.ipv6AddrInfo.ae_obcnt = ippc1.ippc_ob_pkt_count;
			mae6.ipv6AddrInfo.ae_focnt = ippc1.ippc_fo_pkt_count;

			(void) ipif_get_name(ipif,
			    mae6.ipv6AddrIfIndex.o_bytes,
			    OCTET_LENGTH);
			mae6.ipv6AddrIfIndex.o_length =
			    mi_strlen(mae6.ipv6AddrIfIndex.o_bytes);
			mae6.ipv6AddrAddress = ipif->ipif_v6lcl_addr;
			mae6.ipv6AddrPfxLength =
			    ip_mask_to_index_v6(&ipif->ipif_v6net_mask);
			mae6.ipv6AddrInfo.ae_subnet = ipif->ipif_v6subnet;
			mae6.ipv6AddrInfo.ae_subnet_len =
			    mae6.ipv6AddrPfxLength;
			mae6.ipv6AddrInfo.ae_src_addr = ipif->ipif_v6src_addr;

			/* Type: stateless(1), stateful(2), unknown(3) */
			if (ipif->ipif_flags & IFF_ADDRCONF)
				mae6.ipv6AddrType = 1;
			else
				mae6.ipv6AddrType = 2;
			/* Anycast: true(1), false(2) */
			if (ipif->ipif_flags & IFF_ANYCAST)
				mae6.ipv6AddrAnycastFlag = 1;
			else
				mae6.ipv6AddrAnycastFlag = 2;

			/*
			 * Address status: preferred(1), deprecated(2),
			 * invalid(3), inaccessible(4), unknown(5)
			 */
			if (ipif->ipif_flags & IFF_NOLOCAL)
				mae6.ipv6AddrStatus = 3;
			else if (ipif->ipif_flags & IFF_DEPRECATED)
				mae6.ipv6AddrStatus = 2;
			else
				mae6.ipv6AddrStatus = 1;
			mae6.ipv6AddrInfo.ae_mtu = ipif->ipif_mtu;
			mae6.ipv6AddrInfo.ae_metric  = ipif->ipif_metric;
			mae6.ipv6AddrInfo.ae_pp_dst_addr =
			    ipif->ipif_v6pp_dst_addr;
			mae6.ipv6AddrInfo.ae_flags = ipif->ipif_flags;
			if (!snmp_append_data2(mpctl->b_cont, &mp_tail,
			    (char *)&mae6,
			    (int)sizeof (mib2_ipv6AddrEntry_t))) {
				ip1dbg(("ip_snmp_get_mib2_ip6_addr: failed to "
				    "allocate %u bytes\n",
				    (uint_t)sizeof (mib2_ipv6AddrEntry_t)));
			}
		}
	}
	optp->len = (t_uscalar_t)msgdsize(mpctl->b_cont);
	ip1dbg(("ip_snmp_get_mib2_ip6_addr: level %d, name %d, len %d\n",
	    (int)optp->level, (int)optp->name, (int)optp->len));
	qreply(q, mpctl);
	return (mp2ctl);
}

/* IPv4 multicast group membership. */
static mblk_t *
ip_snmp_get_mib2_ip_group_mem(queue_t *q, mblk_t *mpctl)
{
	struct opthdr		*optp;
	mblk_t			*mp2ctl;
	ill_t			*ill;
	ipif_t			*ipif;
	ilm_t			*ilm;
	ip_member_t		ipm;
	mblk_t			*mp_tail = NULL;

	/*
	 * make a copy of the original message
	 */
	mp2ctl = copymsg(mpctl);

	/* ipGroupMember table */
	optp = (struct opthdr *)&mpctl->b_rptr[
	    sizeof (struct T_optmgmt_ack)];
	optp->level = MIB2_IP;
	optp->name = EXPER_IP_GROUP_MEMBERSHIP;
	for (ill = ill_g_head; ill; ill = ill->ill_next) {
		if (ill->ill_isv6)
			continue;

		for (ipif = ill->ill_ipif; ipif;
		    ipif = ipif->ipif_next) {
			(void) ipif_get_name(ipif,
			    ipm.ipGroupMemberIfIndex.o_bytes,
			    OCTET_LENGTH);
			ipm.ipGroupMemberIfIndex.o_length =
			    mi_strlen(ipm.ipGroupMemberIfIndex.o_bytes);
			for (ilm = ill->ill_ilm; ilm; ilm = ilm->ilm_next) {
				if (ilm->ilm_ipif != ipif)
					continue;
				ipm.ipGroupMemberAddress = ilm->ilm_addr;
				ipm.ipGroupMemberRefCnt = ilm->ilm_refcnt;
				if (!snmp_append_data2(mpctl->b_cont, &mp_tail,
				    (char *)&ipm, (int)sizeof (ipm))) {
					ip1dbg(("ip_snmp_get_mib2_ip_group: "
					    "failed to allocate %u bytes\n",
					    (uint_t)sizeof (ipm)));
				}
			}
		}
	}
	optp->len = (t_uscalar_t)msgdsize(mpctl->b_cont);
	ip1dbg(("ip_snmp_get: level %d, name %d, len %d\n",
	    (int)optp->level, (int)optp->name, (int)optp->len));
	qreply(q, mpctl);
	return (mp2ctl);
}

/* IPv6 multicast group membership. */
static mblk_t *
ip_snmp_get_mib2_ip6_group_mem(queue_t *q, mblk_t *mpctl)
{
	struct opthdr		*optp;
	mblk_t			*mp2ctl;
	ill_t			*ill;
	ipif_t			*ipif;
	ilm_t			*ilm;
	ipv6_member_t		ipm6;
	mblk_t			*mp_tail = NULL;

	/*
	 * make a copy of the original message
	 */
	mp2ctl = copymsg(mpctl);

	/* ip6GroupMember table */
	optp = (struct opthdr *)&mpctl->b_rptr[sizeof (struct T_optmgmt_ack)];
	optp->level = MIB2_IP6;
	optp->name = EXPER_IP6_GROUP_MEMBERSHIP;
	for (ill = ill_g_head; ill; ill = ill->ill_next) {
		if (!ill->ill_isv6)
			continue;

		for (ipif = ill->ill_ipif; ipif;
		    ipif = ipif->ipif_next) {
			ipm6.ipv6GroupMemberIfIndex = ill->ill_index;
			for (ilm = ill->ill_ilm; ilm; ilm = ilm->ilm_next) {
				if (ilm->ilm_ipif != ipif)
					continue;
				ipm6.ipv6GroupMemberAddress = ilm->ilm_v6addr;
				ipm6.ipv6GroupMemberRefCnt = ilm->ilm_refcnt;
				if (!snmp_append_data2(mpctl->b_cont,
				    &mp_tail,
				    (char *)&ipm6, (int)sizeof (ipm6))) {
					ip1dbg(("ip_snmp_get_mib2_ip6_group: "
					    "failed to allocate %u bytes\n",
					    (uint_t)sizeof (ipm6)));
				}
			}
		}
	}
	optp->len = (t_uscalar_t)msgdsize(mpctl->b_cont);
	ip1dbg(("ip_snmp_get: level %d, name %d, len %d\n",
		(int)optp->level, (int)optp->name, (int)optp->len));
	qreply(q, mpctl);
	return (mp2ctl);
}

/* Multicast routing virtual interface table. */
static mblk_t *
ip_snmp_get_mib2_virt_multi(queue_t *q, mblk_t *mpctl)
{
	struct opthdr		*optp;
	mblk_t			*mp2ctl;

	/*
	 * make a copy of the original message
	 */
	mp2ctl = copymsg(mpctl);

	optp = (struct opthdr *)&mpctl->b_rptr[sizeof (struct T_optmgmt_ack)];
	optp->level = EXPER_DVMRP;
	optp->name = EXPER_DVMRP_VIF;
	if (!ip_mroute_vif(mpctl->b_cont)) {
		ip0dbg(("ip_mroute_vif: failed\n"));
	}
	optp->len = (t_uscalar_t)msgdsize(mpctl->b_cont);
	ip1dbg(("ip_snmp_get_mib2_virt_multi: level %d, name %d, len %d\n",
	    (int)optp->level, (int)optp->name, (int)optp->len));
	qreply(q, mpctl);
	return (mp2ctl);
}

/* Multicast routing table. */
static mblk_t *
ip_snmp_get_mib2_multi_rtable(queue_t *q, mblk_t *mpctl)
{
	struct opthdr		*optp;
	mblk_t			*mp2ctl;

	/*
	 * make a copy of the original message
	 */
	mp2ctl = copymsg(mpctl);

	optp = (struct opthdr *)&mpctl->b_rptr[sizeof (struct T_optmgmt_ack)];
	optp->level = EXPER_DVMRP;
	optp->name = EXPER_DVMRP_MRT;
	if (!ip_mroute_mrt(mpctl->b_cont)) {
		ip0dbg(("ip_mroute_mrt: failed\n"));
	}
	optp->len = (t_uscalar_t)msgdsize(mpctl->b_cont);
	ip1dbg(("ip_snmp_get_mib2_multi_rtable: level %d, name %d, len %d\n",
	    (int)optp->level, (int)optp->name, (int)optp->len));
	qreply(q, mpctl);
	return (mp2ctl);
}

/*
 * Return both ipRouteEntryTable, and ipNetToMediaEntryTable
 * in one IRE walk.
 */
static mblk_t *
ip_snmp_get_mib2_ip_route_media(queue_t *q, mblk_t *mpctl)
{
	struct opthdr		*optp;
	mblk_t			*mp2ctl;	/* Returned */
	mblk_t			*mp3ctl;	/* nettomedia */
	mblk_t			*mpdata_v4[2];

	/*
	 * make a copy of the original message
	 */
	mp2ctl = copymsg(mpctl);
	mp3ctl = copymsg(mpctl);
	if (mp3ctl == NULL) {
		freemsg(mp2ctl);
		freemsg(mpctl);
		return (NULL);
	}

	mpdata_v4[0] = mpctl->b_cont;
	mpdata_v4[1] = mp3ctl->b_cont;
	ire_walk_v4(ip_snmp_get2_v4, (char *)mpdata_v4);

	/* ipRouteEntryTable in mpctl */
	optp = (struct opthdr *)&mpctl->b_rptr[sizeof (struct T_optmgmt_ack)];
	optp->level = MIB2_IP;
	optp->name = MIB2_IP_ROUTE;
	optp->len = (t_uscalar_t)msgdsize(mpdata_v4[0]);
	ip1dbg(("ip_snmp_get_mib2_ip_route_media: level %d, name %d, len %d\n",
	    (int)optp->level, (int)optp->name, (int)optp->len));
	qreply(q, mpctl);

	/* ipNetToMediaEntryTable in mp3ctl */
	optp = (struct opthdr *)&mp3ctl->b_rptr[sizeof (struct T_optmgmt_ack)];
	optp->level = MIB2_IP;
	optp->name = MIB2_IP_MEDIA;
	optp->len = (t_uscalar_t)msgdsize(mpdata_v4[1]);
	ip1dbg(("ip_snmp_get_mib2_ip_route_media: level %d, name %d, len %d\n",
	    (int)optp->level, (int)optp->name, (int)optp->len));
	qreply(q, mp3ctl);
	return (mp2ctl);
}

/*
 * Return both ipv6RouteEntryTable, and ipv6NetToMediaEntryTable
 * in one IRE walk.
 */
static mblk_t *
ip_snmp_get_mib2_ip6_route_media(queue_t *q, mblk_t *mpctl)
{
	struct opthdr		*optp;
	mblk_t			*mp2ctl;	/* Returned */
	mblk_t			*mp3ctl;	/* nettomedia */
	mblk_t			*mpdata_v6[2];

	/*
	 * make a copy of the original message
	 */
	mp2ctl = copymsg(mpctl);
	mp3ctl = copymsg(mpctl);
	if (mp3ctl == NULL) {
		freemsg(mp2ctl);
		freemsg(mpctl);
		return (NULL);
	}

	mpdata_v6[0] = mpctl->b_cont;
	mpdata_v6[1] = mp3ctl->b_cont;
	ire_walk_v6(ip_snmp_get2_v6, (char *)mpdata_v6);

	/* ipv6RouteEntryTable in mpctl */
	optp = (struct opthdr *)&mpctl->b_rptr[sizeof (struct T_optmgmt_ack)];
	optp->level = MIB2_IP6;
	optp->name = MIB2_IP6_ROUTE;
	optp->len = (t_uscalar_t)msgdsize(mpdata_v6[0]);
	ip1dbg(("ip_snmp_get_mib2_ip6_route_media: level %d, name %d, len %d\n",
		(int)optp->level, (int)optp->name, (int)optp->len));
	qreply(q, mpctl);

	/* ipv6NetToMediaEntryTable in mp3ctl */
	optp = (struct opthdr *)&mp3ctl->b_rptr[sizeof (struct T_optmgmt_ack)];
	optp->level = MIB2_IP6;
	optp->name = MIB2_IP6_MEDIA;
	optp->len = (t_uscalar_t)msgdsize(mpdata_v6[1]);
	ip1dbg(("ip_snmp_get_mib2_ip6_route_media: level %d, name %d, len %d\n",
		(int)optp->level, (int)optp->name, (int)optp->len));
	qreply(q, mp3ctl);
	return (mp2ctl);
}

/*
 * ICMPv6 mib: One per ill
 */
static mblk_t *
ip_snmp_get_mib2_ip6(queue_t *q, mblk_t *mpctl)
{
	struct opthdr		*optp;
	mblk_t			*mp2ctl;
	ill_t			*ill;

	/*
	 * Make a copy of the original message
	 */
	mp2ctl = copymsg(mpctl);

	/* fixed length IPv6 structure ... */

	optp = (struct opthdr *)&mpctl->b_rptr[sizeof (struct T_optmgmt_ack)];
	optp->level = MIB2_IP6;
	optp->name = 0;
	/* Include "unknown interface" ip6_mib */
	ip6_mib.ipv6IfIndex = 0;	/* Flag to netstat */
	SET_MIB(ip6_mib.ipv6Forwarding, ipv6_forward ? 1 : 2);
	SET_MIB(ip6_mib.ipv6DefaultHopLimit, ipv6_def_hops);
	SET_MIB(ip6_mib.ipv6IfStatsEntrySize,
	    sizeof (mib2_ipv6IfStatsEntry_t));
	SET_MIB(ip6_mib.ipv6AddrEntrySize, sizeof (mib2_ipv6AddrEntry_t));
	SET_MIB(ip6_mib.ipv6RouteEntrySize, sizeof (mib2_ipv6RouteEntry_t));
	SET_MIB(ip6_mib.ipv6NetToMediaEntrySize,
	    sizeof (mib2_ipv6NetToMediaEntry_t));
	SET_MIB(ip6_mib.ipv6MemberEntrySize, sizeof (ipv6_member_t));
	if (!snmp_append_data(mpctl->b_cont, (char *)&ip6_mib,
	    (int)sizeof (ip6_mib))) {
		ip1dbg(("ip_snmp_get_mib2_ip6: failed to allocate %u bytes\n",
		    (uint_t)sizeof (ip6_mib)));
	}

	for (ill = ill_g_head; ill; ill = ill->ill_next) {
		if (!ill->ill_isv6)
			continue;
		ill->ill_ip6_mib->ipv6IfIndex = ill->ill_index;
		SET_MIB(ill->ill_ip6_mib->ipv6Forwarding,
		    ipv6_forward ? 1 : 2);
		SET_MIB(ill->ill_ip6_mib->ipv6DefaultHopLimit,
		    ill->ill_max_hops);
		SET_MIB(ill->ill_ip6_mib->ipv6IfStatsEntrySize,
		    sizeof (mib2_ipv6IfStatsEntry_t));
		SET_MIB(ill->ill_ip6_mib->ipv6AddrEntrySize,
		    sizeof (mib2_ipv6AddrEntry_t));
		SET_MIB(ill->ill_ip6_mib->ipv6RouteEntrySize,
		    sizeof (mib2_ipv6RouteEntry_t));
		SET_MIB(ill->ill_ip6_mib->ipv6NetToMediaEntrySize,
		    sizeof (mib2_ipv6NetToMediaEntry_t));
		SET_MIB(ill->ill_ip6_mib->ipv6MemberEntrySize,
		    sizeof (ipv6_member_t));

		if (!snmp_append_data(mpctl->b_cont, (char *)ill->ill_ip6_mib,
		    (int)sizeof (*ill->ill_ip6_mib))) {
			ip1dbg(("ip_snmp_get_mib2_ip6: failed to allocate "
			    "%u bytes\n",
			    (uint_t)sizeof (*ill->ill_ip6_mib)));
		}
	}
	optp->len = (t_uscalar_t)msgdsize(mpctl->b_cont);
	ip1dbg(("ip_snmp_get_mib2_ip6: level %d, name %d, len %d\n",
	    (int)optp->level, (int)optp->name, (int)optp->len));
	qreply(q, mpctl);
	return (mp2ctl);
}

/*
 * ICMPv6 mib: One per ill
 */
static mblk_t *
ip_snmp_get_mib2_icmp6(queue_t *q, mblk_t *mpctl)
{
	struct opthdr		*optp;
	mblk_t			*mp2ctl;
	ill_t			*ill;

	/*
	 * Make a copy of the original message
	 */
	mp2ctl = copymsg(mpctl);

	/* fixed length ICMPv6 structure ... */

	optp = (struct opthdr *)&mpctl->b_rptr[sizeof (struct T_optmgmt_ack)];
	optp->level = MIB2_ICMP6;
	optp->name = 0;
	/* Include "unknown interface" icmp6_mib */
	icmp6_mib.ipv6IfIcmpIfIndex = 0;	/* Flag to netstat */
	icmp6_mib.ipv6IfIcmpEntrySize = sizeof (mib2_ipv6IfIcmpEntry_t);
	if (!snmp_append_data(mpctl->b_cont, (char *)&icmp6_mib,
	    (int)sizeof (icmp6_mib))) {
		ip1dbg(("ip_snmp_get_mib2_icmp6: failed to allocate %u bytes\n",
		    (uint_t)sizeof (icmp6_mib)));
	}

	for (ill = ill_g_head; ill; ill = ill->ill_next) {
		if (!ill->ill_isv6)
			continue;
		ill->ill_icmp6_mib->ipv6IfIcmpIfIndex = ill->ill_index;
		ill->ill_icmp6_mib->ipv6IfIcmpEntrySize =
		    sizeof (mib2_ipv6IfIcmpEntry_t);
		if (!snmp_append_data(mpctl->b_cont, (char *)ill->ill_icmp6_mib,
		    (int)sizeof (*ill->ill_icmp6_mib))) {
			ip1dbg(("ip_snmp_get_mib2_icmp6: failed to allocate "
			    "%u bytes\n",
			    (uint_t)sizeof (*ill->ill_icmp6_mib)));
		}
	}

	optp->len = (t_uscalar_t)msgdsize(mpctl->b_cont);
	ip1dbg(("ip_snmp_get_mib2_icmp6: level %d, name %d, len %d\n",
	    (int)optp->level, (int)optp->name, (int)optp->len));
	qreply(q, mpctl);
	return (mp2ctl);
}

/*
 * ire_walk routine to create both ipRouteEntryTable and
 * ipNetToMediaEntryTable in one IRE walk
 */
static void
ip_snmp_get2_v4(ire_t *ire, mblk_t *mpdata[])
{
	ill_t				*ill;
	ipif_t				*ipif;
	mblk_t				*llmp;
	dl_unitdata_req_t		*dlup;
	mib2_ipRouteEntry_t		re;
	mib2_ipNetToMediaEntry_t	ntme;
	ipaddr_t			gw_addr;

	ASSERT(ire->ire_ipversion == IPV4_VERSION);

	/*
	 * Return all IRE types for route table... let caller pick and choose
	 */
	re.ipRouteDest = ire->ire_addr;
	ipif = ire->ire_ipif;
	re.ipRouteIfIndex.o_length = 0;
	if (ipif) {
		char buf[OCTET_LENGTH];
		char *name;

		name = ipif_get_name(ipif, buf, (int)sizeof (buf));
		re.ipRouteIfIndex.o_length =  (int)strlen(name) + 1;
		bcopy(name, re.ipRouteIfIndex.o_bytes,
		    re.ipRouteIfIndex.o_length);
	}
	re.ipRouteMetric1 = -1;
	re.ipRouteMetric2 = -1;
	re.ipRouteMetric3 = -1;
	re.ipRouteMetric4 = -1;

	gw_addr = ire->ire_gateway_addr;

	if (ire->ire_type & (IRE_INTERFACE|IRE_LOOPBACK|IRE_BROADCAST))
		re.ipRouteNextHop = ire->ire_src_addr;
	else
		re.ipRouteNextHop = gw_addr;
	/* indirect(4), direct(3), or invalid(2) */
	if (ire->ire_flags & (RTF_REJECT | RTF_BLACKHOLE))
		re.ipRouteType = 2;
	else
		re.ipRouteType = (gw_addr != 0) ? 4 : 3;
	re.ipRouteProto = -1;
	re.ipRouteAge = hrestime.tv_sec - ire->ire_create_time;
	re.ipRouteMask = ire->ire_mask;
	re.ipRouteMetric5 = -1;
	re.ipRouteInfo.re_max_frag  = ire->ire_max_frag;
	re.ipRouteInfo.re_frag_flag = ire->ire_frag_flag;
	re.ipRouteInfo.re_rtt	    = ire->ire_uinfo.iulp_rtt;
	llmp = ire->ire_dlureq_mp;
	re.ipRouteInfo.re_ref	    = ire->ire_refcnt;
	re.ipRouteInfo.re_src_addr  = ire->ire_src_addr;
	re.ipRouteInfo.re_ire_type  = ire->ire_type;
	re.ipRouteInfo.re_obpkt	    = ire->ire_ob_pkt_count;
	re.ipRouteInfo.re_ibpkt	    = ire->ire_ib_pkt_count;
	re.ipRouteInfo.re_flags	    = ire->ire_flags;
	if (!snmp_append_data(mpdata[0], (char *)&re, (int)sizeof (re))) {
		ip1dbg(("ip_snmp_get2_v4: failed to allocate %u bytes\n",
		    (uint_t)sizeof (re)));
	}

	if (ire->ire_type != IRE_CACHE || gw_addr != 0)
		return;
	/*
	 * only IRE_CACHE entries that are for a directly connected subnet
	 * get appended to net -> phys addr table
	 * (others in arp)
	 */
	ntme.ipNetToMediaIfIndex.o_length = 0;
	ill = ire_to_ill(ire);
	if (ill) {
		ntme.ipNetToMediaIfIndex.o_length = ill->ill_name_length;
		bcopy(ill->ill_name, ntme.ipNetToMediaIfIndex.o_bytes,
		    ntme.ipNetToMediaIfIndex.o_length);
	}
	ntme.ipNetToMediaPhysAddress.o_length = 0;
	if (llmp) {
		uchar_t *addr;

		dlup = (dl_unitdata_req_t *)llmp->b_rptr;
		/* Remove sap from  address */
		if (ill->ill_sap_length < 0)
			addr = llmp->b_rptr + dlup->dl_dest_addr_offset;
		else
			addr = llmp->b_rptr + dlup->dl_dest_addr_offset +
			    ill->ill_sap_length;

		ntme.ipNetToMediaPhysAddress.o_length =
		    MIN(OCTET_LENGTH, ill->ill_phys_addr_length);
		bcopy(addr, ntme.ipNetToMediaPhysAddress.o_bytes,
		    ntme.ipNetToMediaPhysAddress.o_length);
	}
	ntme.ipNetToMediaNetAddress = ire->ire_addr;
	/* assume dynamic (may be changed in arp) */
	ntme.ipNetToMediaType = 3;
	ntme.ipNetToMediaInfo.ntm_mask.o_length = sizeof (uint32_t);
	bcopy(&ire->ire_mask, ntme.ipNetToMediaInfo.ntm_mask.o_bytes,
	    ntme.ipNetToMediaInfo.ntm_mask.o_length);
	ntme.ipNetToMediaInfo.ntm_flags = ACE_F_RESOLVED;
	if (!snmp_append_data(mpdata[1], (char *)&ntme, (int)sizeof (ntme))) {
		ip1dbg(("ip_snmp_get2_v4: failed to allocate %u bytes\n",
		    (uint_t)sizeof (ntme)));
	}
}

/*
 * ire_walk routine to create both ipv6RouteEntryTable and
 * ipv6NetToMediaEntryTable in one IRE walk
 */
static void
ip_snmp_get2_v6(ire_t *ire, mblk_t *mpdata[])
{
	ill_t				*ill;
	ipif_t				*ipif;
	mib2_ipv6RouteEntry_t		re;
	mib2_ipv6NetToMediaEntry_t	ntme;
	nce_t				*nce = ire->ire_nce;
	in6_addr_t			gw_addr_v6;

	ASSERT(ire->ire_ipversion == IPV6_VERSION);

	/*
	 * Return all IRE types for route table... let caller pick and choose
	 */
	re.ipv6RouteDest = ire->ire_addr_v6;
	re.ipv6RoutePfxLength = ip_mask_to_index_v6(&ire->ire_mask_v6);
	re.ipv6RouteIndex = 0;	/* Unique when multiple with same dest/plen */
	re.ipv6RouteIfIndex.o_length = 0;
	ipif = ire->ire_ipif;
	if (ipif) {
		char buf[OCTET_LENGTH];
		char *name;

		name = ipif_get_name(ipif, buf, (int)sizeof (buf));
		re.ipv6RouteIfIndex.o_length =  (int)strlen(name) + 1;
		bcopy(name, re.ipv6RouteIfIndex.o_bytes,
		    re.ipv6RouteIfIndex.o_length);
	}

	ASSERT(!(ire->ire_type & IRE_BROADCAST));

	mutex_enter(&ire->ire_lock);
	gw_addr_v6 = ire->ire_gateway_addr_v6;
	mutex_exit(&ire->ire_lock);

	if (ire->ire_type & (IRE_INTERFACE|IRE_LOOPBACK))
		re.ipv6RouteNextHop = ire->ire_src_addr_v6;
	else
		re.ipv6RouteNextHop = gw_addr_v6;

	/* remote(4), local(3), or discard(2) */
	if (ire->ire_flags & (RTF_REJECT | RTF_BLACKHOLE))
		re.ipv6RouteType = 2;
	else if (IN6_IS_ADDR_UNSPECIFIED(&gw_addr_v6))
		re.ipv6RouteType = 3;
	else
		re.ipv6RouteType = 4;

	re.ipv6RouteProtocol		= -1;
	re.ipv6RoutePolicy		= 0;
	re.ipv6RouteAge		= hrestime.tv_sec - ire->ire_create_time;
	re.ipv6RouteNextHopRDI		= 0;
	re.ipv6RouteWeight		= 0;
	re.ipv6RouteMetric		= 0;
	re.ipv6RouteInfo.re_max_frag	= ire->ire_max_frag;
	re.ipv6RouteInfo.re_frag_flag	= ire->ire_frag_flag;
	re.ipv6RouteInfo.re_rtt		= ire->ire_uinfo.iulp_rtt;
	re.ipv6RouteInfo.re_src_addr	= ire->ire_src_addr_v6;
	re.ipv6RouteInfo.re_ire_type	= ire->ire_type;
	re.ipv6RouteInfo.re_obpkt	= ire->ire_ob_pkt_count;
	re.ipv6RouteInfo.re_ibpkt	= ire->ire_ib_pkt_count;
	re.ipv6RouteInfo.re_ref		= ire->ire_refcnt;
	re.ipv6RouteInfo.re_flags	= ire->ire_flags;

	if (!snmp_append_data(mpdata[0], (char *)&re, (int)sizeof (re))) {
		ip1dbg(("ip_snmp_get2_v6: failed to allocate %u bytes\n",
		    (uint_t)sizeof (re)));
	}

	ill = ire_to_ill(ire);
	if (ill == NULL ||
	    !IN6_IS_ADDR_UNSPECIFIED(&gw_addr_v6)) {
		/*
		 * IRE is not for an on-link destination.
		 * No "unique" NetToMedia info
		 */
		return;
	}

	ntme.ipv6NetToMediaIfIndex = 0;	/* Zero implies no entry */
	if (nce != NULL) {
		/*
		 * Neighbor cache entry attached to IRE with on-link
		 * destination.
		 */
		ntme.ipv6NetToMediaIfIndex = ill->ill_index;
		ntme.ipv6NetToMediaNetAddress = ire->ire_addr_v6;
		ntme.ipv6NetToMediaPhysAddress.o_length =
		    ill->ill_phys_addr_length;
		if (nce->nce_res_mp != NULL) {
			bcopy((char *)nce->nce_res_mp->b_rptr +
			    NCE_LL_ADDR_OFFSET(ill),
			    ntme.ipv6NetToMediaPhysAddress.o_bytes,
			    ill->ill_phys_addr_length);
		} else {
			bzero(ntme.ipv6NetToMediaPhysAddress.o_bytes,
			    ill->ill_phys_addr_length);
		}
		/*
		 * Note: Returns ND_* states. Should be:
		 * reachable(1), stale(2), delay(3), probe(4),
		 * invalid(5), unknown(6)
		 */
		ntme.ipv6NetToMediaState = nce->nce_state;
		ntme.ipv6NetToMediaLastUpdated = 0;
	} else {
		nce_t	*nce;

		/*
		 * Check in NDP's cache for permanent/mapped entries.
		 */
		nce = ndp_lookup(ill, &ire->ire_addr_v6);
		if (nce != NULL) {
			ntme.ipv6NetToMediaIfIndex = ill->ill_index;
			ntme.ipv6NetToMediaNetAddress = ire->ire_addr_v6;
			ntme.ipv6NetToMediaPhysAddress.o_length =
			    ill->ill_phys_addr_length;
			bcopy(nce->nce_res_mp->b_rptr +
			    NCE_LL_ADDR_OFFSET(ill),
			    ntme.ipv6NetToMediaPhysAddress.o_bytes,
			    ill->ill_phys_addr_length);
			/*
			 * Note: Returns ND_* states. Should be:
			 * reachable(1), stale(2), delay(3), probe(4),
			 * invalid(5), unknown(6)
			 */
			ntme.ipv6NetToMediaState = nce->nce_state;
			ntme.ipv6NetToMediaLastUpdated = 0;
			NCE_REFRELE(nce);
		}
	}
	/* other(1), dynamic(2), static(3), local(4) */
	if (ire->ire_type & (IRE_LOOPBACK|IRE_LOCAL)) {
		ntme.ipv6NetToMediaType = 4;
	} else if (IN6_IS_ADDR_MULTICAST(&ire->ire_addr_v6)) {
		ntme.ipv6NetToMediaType = 1;
	} else {
		ntme.ipv6NetToMediaType = 2;
	}

	if (ntme.ipv6NetToMediaIfIndex != 0) {
		/* Information to append */
		if (!snmp_append_data(mpdata[1], (char *)&ntme,
		    (int)sizeof (ntme))) {
			ip1dbg(("ip_snmp_get2_v6: failed to allocate %u "
			    "bytes\n",
			    (uint_t)sizeof (ntme)));
		}
	}
}

/*
 * return (0) if invalid set request, 1 otherwise, including non-tcp requests
 */
/* ARGSUSED */
static int
ip_snmp_set(queue_t *q, int level, int name, uchar_t *ptr, int len)
{
	switch (level) {
	case MIB2_IP:
	case MIB2_ICMP:
		switch (name) {
		default:
			break;
		}
		return (1);
	default:
		return (1);
	}
}

/*
 * Called before the options are updated to check if this packet will
 * be source routed from here.
 * This routine assumes that the options are well formed i.e. that they
 * have already been checked.
 */
static boolean_t
ip_source_routed(ipha_t *ipha)
{
	uint32_t	totallen;
	uchar_t		*opt;
	uint32_t	optval;
	uint32_t	optlen;
	ipaddr_t	dst;
	ire_t		*ire;

	ip2dbg(("ip_source_routed\n"));
	totallen = ipha->ipha_version_and_hdr_length -
	    (uint8_t)((IP_VERSION << 4) + IP_SIMPLE_HDR_LENGTH_IN_WORDS);
	if (totallen == 0)
		return (B_FALSE);
	dst = ipha->ipha_dst;
	opt = (uchar_t *)&ipha[1];
	totallen <<= 2;
	while (totallen != 0) {
		switch (optval = opt[IPOPT_POS_VAL]) {
		case IPOPT_EOL:
			return (B_FALSE);
		case IPOPT_NOP:
			optlen = 1;
			break;
		default:
			optlen = opt[IPOPT_POS_LEN];
		}
		ip2dbg(("ip_source_routed: opt %d, len %d\n",
		    optval, optlen));

		if (optlen == 0 || optlen > totallen)
			break;

		switch (optval) {
			uint32_t off;
		case IPOPT_SSRR:
		case IPOPT_LSRR:
			/*
			 * If dst is one of our addresses and there are some
			 * entries left in the source route return (true).
			 */
			ire = ire_ctable_lookup(dst, 0, IRE_LOCAL, NULL,
			    NULL, MATCH_IRE_TYPE);
			if (ire == NULL) {
				ip2dbg(("ip_source_routed: not next"
				    " source route 0x%x\n",
				    ntohl(dst)));
				return (B_FALSE);
			}
			ire_refrele(ire);
			off = opt[IPOPT_POS_OFF];
			off--;
			if (optlen < IP_ADDR_LEN ||
			    off > optlen - IP_ADDR_LEN) {
				/* End of source route */
				ip1dbg(("ip_source_routed: end of SR\n"));
				return (B_FALSE);
			}
			return (B_TRUE);
		}
		totallen -= optlen;
		opt += optlen;
	}
	return (B_FALSE);
}

/*
 * Check if the packet contains any source route.
 * This routine assumes that the options are well formed i.e. that they
 * have already been checked.
 */
static boolean_t
ip_source_route_included(ipha_t *ipha)
{
	uint32_t	totallen;
	uchar_t		*opt;
	uint32_t	optval;
	uint32_t	optlen;

	totallen = ipha->ipha_version_and_hdr_length -
	    (uint8_t)((IP_VERSION << 4) + IP_SIMPLE_HDR_LENGTH_IN_WORDS);
	if (totallen == 0)
		return (B_FALSE);
	opt = (uchar_t *)&ipha[1];
	totallen <<= 2;
	while (totallen != 0) {
		switch (optval = opt[IPOPT_POS_VAL]) {
		case IPOPT_EOL:
			return (B_FALSE);
		case IPOPT_NOP:
			optlen = 1;
			break;
		default:
			optlen = opt[IPOPT_POS_LEN];
		}

		if (optlen == 0 || optlen > totallen)
			break;

		switch (optval) {
		case IPOPT_SSRR:
		case IPOPT_LSRR:
			return (B_TRUE);
		}
		totallen -= optlen;
		opt += optlen;
	}
	return (B_FALSE);
}

/*
 * Called when the IRE expiration timer fires.
 */
/* ARGSUSED */
void
ip_trash_timer_expire(void *args)
{
	int	flush_flag = 0;

	ip_ire_expire_id = 0;
	/* Periodic timer */
	if (ip_ire_arp_time_elapsed >= ip_ire_arp_interval) {
		/*
		 * Remove all IRE_CACHE entries since they might
		 * contain arp information.
		 */
		flush_flag |= FLUSH_ARP_TIME;
		ip_ire_arp_time_elapsed = 0;
	}
	if (ip_ire_rd_time_elapsed >= ip_ire_redir_interval) {
		/* Remove all redirects */
		flush_flag |= FLUSH_REDIRECT_TIME;
		ip_ire_rd_time_elapsed = 0;
	}
	if (ip_ire_pmtu_time_elapsed >= ip_ire_pathmtu_interval) {
		/* Increase path mtu */
		flush_flag |= FLUSH_MTU_TIME;
		ip_ire_pmtu_time_elapsed = 0;
	}
	if (flush_flag != 0) {
		/* Walk all IPv4 IRE's and update them */
		ire_walk_v4(ire_expire, (char *)flush_flag);
	}
	if (flush_flag & FLUSH_MTU_TIME) {
		/*
		 * Walk all IPv6 IRE's and update them
		 * Note that ARP and redirect timers are not
		 * needed since NUD handles stale entries.
		 */
		flush_flag = FLUSH_MTU_TIME;
		ire_walk_v6(ire_expire, (char *)flush_flag);
	}

	if (ip_timer_ill) {
		ip_ire_arp_time_elapsed += ip_timer_interval;
		ip_ire_rd_time_elapsed += ip_timer_interval;
		ip_ire_pmtu_time_elapsed += ip_timer_interval;
		ip_ire_expire_id = qtimeout(ip_timer_ill->ill_rq,
		    ip_trash_timer_expire, ip_timer_ill->ill_rq,
		    MSEC_TO_TICK(ip_timer_interval));
	}
}

/*
 * Called by the memory allocator subsystem when the system is running
 * low on memory.
 */
/* ARGSUSED */
void
ip_trash_ire_reclaim(void *args)
{
	ire_cache_count_t icc;
	ire_cache_reclaim_t icr;
	ncc_cache_count_t ncc;
	nce_cache_reclaim_t ncr;
	uint_t delete_cnt;
	/*
	 * Memory reclaim call back.
	 * Count unused, offlink, pmtu, and onlink IRE_CACHE entries.
	 * Then, with a target of freeing 1/Nth of IRE_CACHE
	 * entries, determine what fraction to free for
	 * each category of IRE_CACHE entries giving absolute priority
	 * in the order of onlink, pmtu, offlink, unused (e.g. no pmtu
	 * entry will be freed unless all offlink entries are freed).
	 */
	ip_ire_reclaim_id = 0;
	icc.icc_total = 0;
	icc.icc_unused = 0;
	icc.icc_offlink = 0;
	icc.icc_pmtu = 0;
	icc.icc_onlink = 0;
	ire_walk(ire_cache_count, (char *)&icc);

	/*
	 * Free NCEs for IPv6 like the onlink ires.
	 */
	ncc.ncc_total = 0;
	ncc.ncc_host = 0;
	ndp_walk(NULL, (pfi_t)ndp_cache_count, (uchar_t *)&ncc);

	ASSERT(icc.icc_total == icc.icc_unused + icc.icc_offlink +
	    icc.icc_pmtu + icc.icc_onlink);
	delete_cnt = icc.icc_total/ip_ire_reclaim_fraction;
	if (delete_cnt == 0)
		return;
	/* Always delete all unused offlink entries */
	icr.icr_unused = 1;
	if (delete_cnt <= icc.icc_unused) {
		/*
		 * Only need to free unused entries.  In other words,
		 * there are enough unused entries to free to meet our
		 * target number of freed ire cache entries.
		 */
		icr.icr_offlink = icr.icr_pmtu = icr.icr_onlink = 0;
		ncr.ncr_host = 0;
	} else if (delete_cnt <= icc.icc_unused + icc.icc_offlink) {
		/*
		 * Only need to free unused entries, plus a fraction of offlink
		 * entries.  It follows from the first if statement that
		 * icc_offlink is non-zero, and that delete_cnt != icc_unused.
		 */
		delete_cnt -= icc.icc_unused;
		/* Round up # deleted by truncating fraction */
		icr.icr_offlink = icc.icc_offlink / delete_cnt;
		icr.icr_pmtu = icr.icr_onlink = 0;
		ncr.ncr_host = 0;
	} else if (delete_cnt <=
	    icc.icc_unused + icc.icc_offlink + icc.icc_pmtu) {
		/*
		 * Free all unused and offlink entries, plus a fraction of
		 * pmtu entries.  It follows from the previous if statement
		 * that icc_pmtu is non-zero, and that
		 * delete_cnt != icc_unused + icc_offlink.
		 */
		icr.icr_offlink = 1;
		delete_cnt -= icc.icc_unused + icc.icc_offlink;
		/* Round up # deleted by truncating fraction */
		icr.icr_pmtu = icc.icc_pmtu / delete_cnt;
		icr.icr_onlink = 0;
		ncr.ncr_host = 0;
	} else {
		/*
		 * Free all unused, offlink, and pmtu entries, plus a fraction
		 * of onlink entries.  If we're here, then we know that
		 * icc_onlink is non-zero, and that
		 * delete_cnt != icc_unused + icc_offlink + icc_pmtu.
		 */
		icr.icr_offlink = icr.icr_pmtu = 1;
		delete_cnt -= icc.icc_unused + icc.icc_offlink +
		    icc.icc_pmtu;
		/* Round up # deleted by truncating fraction */
		icr.icr_onlink = icc.icc_onlink / delete_cnt;
		/* Using the same delete fraction as for onlink IREs */
		ncr.ncr_host = ncc.ncc_host / delete_cnt;
	}
#ifdef DEBUG
	ip1dbg(("IP reclaim: target %d out of %d current %d/%d/%d/%d "
	    "fractions %d/%d/%d/%d\n",
	    icc.icc_total/ip_ire_reclaim_fraction, icc.icc_total,
	    icc.icc_unused, icc.icc_offlink,
	    icc.icc_pmtu, icc.icc_onlink,
	    icr.icr_unused, icr.icr_offlink,
	    icr.icr_pmtu, icr.icr_onlink));
#endif
	ire_walk(ire_cache_reclaim, (char *)&icr);
	if (ncr.ncr_host != 0)
		ndp_walk(NULL, (pfi_t)ndp_cache_reclaim,
		    (uchar_t *)&ncr);
#ifdef DEBUG
	icc.icc_total = 0; icc.icc_unused = 0; icc.icc_offlink = 0;
	icc.icc_pmtu = 0; icc.icc_onlink = 0;
	ire_walk(ire_cache_count, (char *)&icc);
	ip1dbg(("IP reclaim: result total %d %d/%d/%d/%d\n",
	    icc.icc_total, icc.icc_unused, icc.icc_offlink,
	    icc.icc_pmtu, icc.icc_onlink));
#endif
}

/*
 * ip_unbind is called when a copy of an unbind request is received from the
 * upper level protocol.  We remove this ipc from any fanout hash list it is
 * on, and zero out the bind information.  No reply is expected up above.
 */
static void
ip_unbind(queue_t *q, mblk_t *mp)
{
	ipc_t	*ipc = (ipc_t *)q->q_ptr;
	mblk_t *ipsec_out;
	mblk_t *ipsec_req_in;

	if (ipc->ipc_palist != NULL) {
		ip_proxy_remove_listener(ipc);
	} else {
		ipc_hash_remove(ipc);
	}

	/*
	 * Send two messages for handling detached connections in
	 * TCP. We do this only for hard_bound connections as only
	 * hard_bound connections detach. For all non-hard_bound
	 * connections, policy is applied only in IP and hence we
	 * don't need to send anything up.
	 *
	 * Send an IPSEC_OUT that will be attached by TCP on subsequent
	 * ip_wputs and an ipsec_req_in message for handling inbound
	 * policy at the TCP level.
	 *
	 * NOTE : Don't set ipc_policy_cached to B_FALSE as there could
	 * be some threads still having a reference to this ipc and not
	 * yet done the policy checks.
	 */
	ipsec_out = NULL;
	ipsec_req_in = NULL;
	if (ipc->ipc_ulp == IPPROTO_TCP && ipc->ipc_policy_cached) {
		if (ipc->ipc_out_enforce_policy) {
			ipsec_out = ipc->ipc_ipsec_out;
			/*
			 * We seem to get more than one unbind.
			 * If we had sent the information up already,
			 * don't bother.
			 */
			if (ipsec_out != NULL) {
				(void) ipsec_init_ipsec_out(ipsec_out, ipc,
				    NULL, IPPROTO_TCP);
				ipc->ipc_ipsec_out = NULL;
			}
		}
		if (ipc->ipc_in_enforce_policy) {
			ipsec_req_in = ipc->ipc_ipsec_req_in;
			/*
			 * We seem to get more than one unbind.
			 * If we had sent the information up already,
			 * don't bother.
			 */
			if (ipsec_req_in != NULL) {
				bcopy(ipc->ipc_inbound_policy,
				    ipsec_req_in->b_rptr,
				    sizeof (ipsec_req_t));
				ipsec_req_in->b_wptr += sizeof (ipsec_req_t);
				ipc->ipc_ipsec_req_in = NULL;
			}
		}
	}
	bzero(&ipc->ipc_ipcu, sizeof (ipc->ipc_ipcu));
	ipc->ipc_fully_bound = B_FALSE;

	/* Send a T_OK_ACK to the user */
	if ((mp = mi_tpi_ok_ack_alloc(mp)) != NULL) {
		mp->b_cont = ipsec_out;
		if (ipsec_out)
			ipsec_out->b_cont = ipsec_req_in;
		else
			mp->b_cont = ipsec_req_in;
		qreply(q, mp);
	}
}

/*
 * Compare two policies.
 * Return 0 if they are same, 1 if new is stronger, -1 if
 * new is weaker.
 *
 * Bypass policy >  AH_ESP > ESP > AH.
 */

static int
ipsec_policy_cmp(ipsec_policy_t *old, ipsec_policy_t *new)
{
	char new_prot;
	char old_prot;

	/*
	 * If the new one is bypass, then the new one is as strong
	 * as old ones.
	 */
	if (new->ipsec_conf.ipsc_policy == IPSEC_POLICY_BYPASS &&
	    old->ipsec_conf.ipsc_policy == IPSEC_POLICY_BYPASS) {
		return (0);
	}

	if (new->ipsec_conf.ipsc_policy == IPSEC_POLICY_BYPASS)
		return (1);

	if (old->ipsec_conf.ipsc_policy == IPSEC_POLICY_BYPASS)
		return (-1);

	/*
	 * If one of them is bypass, we should return from above.
	 */
	ASSERT(old->ipsec_conf.ipsc_policy != IPSEC_POLICY_BYPASS &&
	    new->ipsec_conf.ipsc_policy != IPSEC_POLICY_BYPASS);

	/*
	 * Compare the IPSEC protocol and see which is stronger.
	 */
	old_prot = old->ipsec_conf.ipsc_ipsec_prot;
	new_prot = new->ipsec_conf.ipsc_ipsec_prot;

	if (new_prot > old_prot) {
		return (1);
	} else if (old_prot == new_prot) {
		return (0);
	} else {
		return (-1);
	}
}

#ifdef DEBUG
/*
 * ipsec_config_add call this to ASSERT that the policy entries
 * are ordered.
 */
boolean_t
ipsec_policy_is_ordered(ipsec_policy_t *head)
{
	int ret;
	ipsec_policy_t *next, *prev;

	prev = head->ipsec_policy_next;
	if (prev == NULL)
		return (B_TRUE);

	rw_enter(&ipsec_conf_lock, RW_READER);
	while ((next = prev->ipsec_policy_next) != NULL) {
		ret = ipsec_policy_cmp(prev, next);
		/*
		 * Entries towards the head of the list should
		 * always be of stronger policy.
		 */
		if (ret > 0) {
			rw_exit(&ipsec_conf_lock);
			return (B_FALSE);
		}
		prev = next;
	}
	rw_exit(&ipsec_conf_lock);
	return (B_TRUE);
}

/*
 * ipsec_get_policy_index() asserts this to make sure that the
 * policy index it is returning to the caller is not a duplicate
 */
static boolean_t
ipsec_policy_index_is_not_dup(uint32_t policy_index)
{
	int i;
	ipsec_policy_t *policy_head;

	ASSERT(RW_WRITE_HELD(&ipsec_conf_lock));

	for (i = 0; i < IPSEC_NTYPES; i++) {
		policy_head = ipsec_policy_head[i].ipsec_policy_next;
		while (policy_head != NULL) {
			if (policy_index ==
			    policy_head->ipsec_conf.ipsc_policy_index) {
				return (B_FALSE);
			}
			policy_head = policy_head->ipsec_policy_next;
		}
	}
	return (B_TRUE);
}
#endif

void
ipsec_config_list(queue_t *q, mblk_t *mp)
{
	struct iocblk *iocp = (struct iocblk *)mp->b_rptr;
	ipsec_policy_t *policy_head;
	mblk_t *data_mp;
	uchar_t *rptr;
	int nconf = 0;
	int err = 0;
	int i;

	data_mp = mp->b_cont;
	ASSERT(data_mp != NULL);
	if (pullupmsg(data_mp, -1) == -1) {
		mp->b_cont = NULL;
		err = ENOMEM;
		freemsg(data_mp);
		goto err;
	}

	rptr = data_mp->b_rptr;
	rw_enter(&ipsec_conf_lock, RW_READER);
	for (i = 0; i < IPSEC_NTYPES; i++) {
		policy_head = ipsec_policy_head[i].ipsec_policy_next;
		while (policy_head != NULL) {
			if ((data_mp->b_wptr - rptr) < sizeof (ipsec_conf_t)) {
				break;
			}
			bcopy(&policy_head->ipsec_conf, rptr,
			    sizeof (ipsec_conf_t));
			nconf++;
			rptr += sizeof (ipsec_conf_t);
			policy_head = policy_head->ipsec_policy_next;
		}
	}
	rw_exit(&ipsec_conf_lock);
	data_mp->b_wptr = rptr;
err:
	iocp->ioc_count = nconf * sizeof (ipsec_conf_t);
	iocp->ioc_error = err;
	mp->b_datap->db_type = M_IOCACK;
	qreply(q, mp);
}

void
ipsec_config_flush(queue_t *q, mblk_t *mp)
{
	struct iocblk *iocp = (struct iocblk *)mp->b_rptr;
	ipsec_policy_t *policy_head, *next;
	int i;

	rw_enter(&ipsec_conf_lock, RW_WRITER);
	for (i = 0; i < IPSEC_NTYPES; i++) {
		policy_head = ipsec_policy_head[i].ipsec_policy_next;
		ipsec_policy_head[i].ipsec_policy_next = NULL;
		while (policy_head != NULL) {
			next = policy_head->ipsec_policy_next;
			kmem_free(policy_head, sizeof (ipsec_policy_t));
			policy_head = next;
		}
	}
	ipsec_next_policy_index_to_try = 1;
	rw_exit(&ipsec_conf_lock);
	iocp->ioc_count = 0;
	iocp->ioc_error = 0;
	mp->b_datap->db_type = M_IOCACK;
	qreply(q, mp);
}

/*
 * Return a policy index to be used with the new policy entry. If the
 * policy index has never wrapped, ipsec_next_policy_index_to_try has
 * the right value. If it has wrapped at least once, we need to check
 * for duplicates. We want to limit our search on duplicates so that we
 * don't loop in the kernel forever. Thus, try MAX_SEARCH times.
 */
static uint32_t
ipsec_get_policy_index()
{
	int i;
	uint32_t ret;
	ipsec_policy_t *policy_head;
	int count = 0;
#define	MAX_SEARCH	1000

	ASSERT(RW_WRITE_HELD(&ipsec_conf_lock));
	if (!ipsec_policy_index_wrapped) {
		/*
		 * This is the fast path as the policy index
		 * should rarely wrap around.
		 */
		ret = ipsec_next_policy_index_to_try;
		ipsec_next_policy_index_to_try++;
		if (ipsec_next_policy_index_to_try == 0) {
			ipsec_policy_index_wrapped = B_TRUE;
			ipsec_next_policy_index_to_try = 1;
		}
		ASSERT(ipsec_policy_index_is_not_dup(ret));
		return (ret);
	}
	/*
	 * Policy index has wrapped. Check for duplicates.
	 * NOTE : For indentation reasons, we have a goto here.
	 */
again:
	for (i = 0; i < IPSEC_NTYPES; i++) {
		policy_head = ipsec_policy_head[i].ipsec_policy_next;
		while (policy_head != NULL) {
			if (ipsec_next_policy_index_to_try ==
			    policy_head->ipsec_conf.ipsc_policy_index) {
				/*
				 * Don't use 0 for policy ids. Zero is used
				 * (by ipsecconf()) as a means to allocate
				 * new ids.
				 */
				ipsec_next_policy_index_to_try++;
				if (ipsec_next_policy_index_to_try == 0)
					ipsec_next_policy_index_to_try = 1;
				count++;
				if (count > MAX_SEARCH) {
					return (0);
				}
				goto again;
			}
			policy_head = policy_head->ipsec_policy_next;
		}
	}
	ret = ipsec_next_policy_index_to_try;
	/*
	 * We don't use 0 for policy ids. Zero is used (by ipsecconf())
	 * as a means to allocate new ids.
	 */
	ipsec_next_policy_index_to_try++;
	if (ipsec_next_policy_index_to_try == 0)
		ipsec_next_policy_index_to_try = 1;
	ASSERT(ipsec_policy_index_is_not_dup(ret));
	return (ret);
}

int
ipsec_config_add(mblk_t *mp)
{
	ipsec_conf_t *conf;
	ipsec_policy_t *attach_at;
	ipsec_policy_t *new_policy, *old_policy;
	ipsec_policy_t *policy;
	ipsec_policy_t *prev;
	mblk_t *data_mp;
	int ret;
	uint32_t policy_index;

	data_mp = mp->b_cont;
	ASSERT(data_mp != NULL);

	if (pullupmsg(data_mp, -1) == -1) {
		mp->b_cont = NULL;
		freemsg(data_mp);
		return (ENOMEM);
	}

	if ((data_mp->b_wptr - data_mp->b_rptr) != sizeof (ipsec_conf_t))
		return (EINVAL);

	conf = (ipsec_conf_t *)data_mp->b_rptr;

	switch (conf->ipsc_dir) {
	case IPSEC_TYPE_INBOUND:
	case IPSEC_TYPE_OUTBOUND:
		break;
	default:
		return (EINVAL);
	}

	attach_at = &ipsec_policy_head[conf->ipsc_dir];

	rw_enter(&ipsec_conf_lock, RW_WRITER);
	/*
	 * Check for duplicates. The logic is as follows.
	 *
	 * Definition: An entry is considered more specific
	 * if the selector has less number of wildcard entries
	 * or the prefix length(mask) of the address is longer.
	 *
	 * If the user wants to add an entry which is more
	 * specific than an already existing entry i.e
	 * if there is an old entry that would already
	 * match an existing traffic, a new entry will not
	 * be considered as a duplicate if its policy is
	 * stronger and matches the same traffic. As the
	 * stronger policy is ordered before a weaker one,
	 * the new entry will match the traffic. If the policy
	 * is same or weaker, one cannot add a more specific
	 * entry as there is already an entry matching
	 * the traffic. This logic is subject to BYPASS constraints
	 * as discussed below.
	 */
	policy = attach_at->ipsec_policy_next;
	if (policy != NULL) {
		ipsec_selector_t sel;
		ipsec_conf_t match;
		/*
		 * Check whether there is any entry that would
		 * match the same traffic pattern.
		 */
		bcopy(conf->ipsc_src_addr, (uint8_t *)&sel.src_addr,
		    IP_ADDR_LEN);
		bcopy(conf->ipsc_dst_addr, (uint8_t *)&sel.dst_addr,
		    IP_ADDR_LEN);
		/*
		 * This is the only place, where we set mask in sel.
		 * ipsec_match_policy uses this mask to check for
		 * duplicates.
		 */
		bcopy(conf->ipsc_src_mask, (uint8_t *)&sel.src_mask,
		    IP_ADDR_LEN);
		bcopy(conf->ipsc_dst_mask, (uint8_t *)&sel.dst_mask,
		    IP_ADDR_LEN);
		sel.src_port = conf->ipsc_src_port;
		sel.dst_port = conf->ipsc_dst_port;
		sel.protocol = conf->ipsc_ulp_prot;
		sel.outbound = conf->ipsc_dir;
		if (ipsec_match_policy(policy, &sel, &match)) {
			/*
			 * 1) If there is already an existing entry with
			 *    BYPASS policy, don't allow any more additions
			 *    as BYPASS is strongest.
			 * 2) If the exiting entry is not BYPASS, allow to
			 *    add a BYPASS entry - equivalent to deleting the
			 *    old entry. If the new entry is not BYPASS,
			 *    allow only if the new one is of stronger
			 *    protection.
			 */
			if (match.ipsc_policy == IPSEC_POLICY_BYPASS ||
			    (conf->ipsc_policy != IPSEC_POLICY_BYPASS &&
			    match.ipsc_ipsec_prot >= conf->ipsc_ipsec_prot)) {
				rw_exit(&ipsec_conf_lock);
				return (EEXIST);
			}
		}
	}

	/*
	 * Allocate memory before we get an index. We don't want to fail
	 * after we allocate an index just to prevent wastage of indices.
	 */
	if ((new_policy = kmem_alloc(sizeof (ipsec_policy_t), KM_NOSLEEP))
	    == NULL) {
		rw_exit(&ipsec_conf_lock);
		return (ENOMEM);
	}

	if ((policy_index = conf->ipsc_policy_index) == 0) {
		if ((policy_index = ipsec_get_policy_index()) == 0) {
			rw_exit(&ipsec_conf_lock);
			kmem_free(new_policy, sizeof (ipsec_policy_t));
			return (ENOSPC);
		}
	}
	/*
	 * Attach the new policy after all the entries that has
	 * bypass policy set and before weaker ones. Thus when
	 * we search for a policy match, bypass would be the
	 * first hit and then the stronger ones. Currently we
	 * define strong as the one which has AH and ESP, ESP,
	 * and AH.
	 */
	new_policy->ipsec_policy_next = NULL;
	new_policy->ipsec_conf = *conf;		/* Structure copy */
	prev = attach_at;

	if ((old_policy = attach_at->ipsec_policy_next) == NULL) {
		attach_at->ipsec_policy_next = new_policy;
	} else {
		while (old_policy != NULL) {
			ret = ipsec_policy_cmp(old_policy, new_policy);
			if (ret > 0) {
				/*
				 * new policy is stronger than the old
				 * policy.
				 */
				new_policy->ipsec_policy_next = old_policy;
				prev->ipsec_policy_next = new_policy;
				break;
			}
			/*
			 * Either new policy is weaker than the old one
			 * or same as the old one. If we have bypass
			 * entries in the list we might find ourself
			 * stronger once we skip all bypass entries.
			 * Just keep going.
			 */
			if (old_policy->ipsec_policy_next == NULL) {
				/* Last entry */
				old_policy->ipsec_policy_next = new_policy;
				break;
			}
			prev = old_policy;
			old_policy = old_policy->ipsec_policy_next;
		}
	}
	new_policy->ipsec_conf.ipsc_policy_index = policy_index;
	/* Pass the index back to the userland */
	conf->ipsc_policy_index = policy_index;
	rw_exit(&ipsec_conf_lock);
	ASSERT(ipsec_policy_is_ordered(attach_at));
	return (0);
}

int
ipsec_config_delete(mblk_t *mp)
{
	ipsec_policy_t *detach_at, *policy;
	ipsec_policy_t *prev;
	mblk_t *data_mp;
	uint32_t policy_index;
	int i;
	boolean_t once;

	data_mp = mp->b_cont;
	ASSERT(data_mp != NULL);

	if (pullupmsg(data_mp, -1) == -1) {
		mp->b_cont = NULL;
		freemsg(data_mp);
		return (ENOMEM);
	}

	if ((data_mp->b_wptr - data_mp->b_rptr) != sizeof (uint32_t))
		return (EINVAL);

	once = B_FALSE;
	policy_index = *(uint32_t *)data_mp->b_rptr;
	rw_enter(&ipsec_conf_lock, RW_WRITER);
again:
	for (i = 0; i < IPSEC_NTYPES; i++) {
		prev = detach_at = &ipsec_policy_head[i];
		policy = detach_at->ipsec_policy_next;
		while (policy != NULL) {
			if (policy->ipsec_conf.ipsc_policy_index ==
			    policy_index) {
				prev->ipsec_policy_next =
				    policy->ipsec_policy_next;
				kmem_free(policy, sizeof (ipsec_policy_t));
				/*
				 * Multiple policies could be with the same
				 * index. Delete all of them.
				 */
				once = B_TRUE;
				goto again;
			}
			prev = policy;
			policy = policy->ipsec_policy_next;
		}
	}
	rw_exit(&ipsec_conf_lock);
	if (!once)
		return (ENOENT);
	return (0);
}

static void
ipsec_copy_policy(ipsec_conf_t *conf, ipsec_req_t *isr)
{
	int prot;

	prot = conf->ipsc_ipsec_prot;

	switch (prot) {
	case IPSEC_AH_ESP:
		/* FALLTHRU */
	case IPSEC_AH_ONLY:
		isr->ipsr_ah_req = IPSEC_PREF_REQUIRED;
		if (conf->ipsc_no_of_ah_algs != 0) {
			isr->ipsr_auth_alg = conf->ipsc_ah_algs[0];
		} else {
			isr->ipsr_auth_alg = 0;
		}
		if (conf->ipsc_sa_attr == IPSEC_UNIQUE_SA) {
			isr->ipsr_ah_req |= IPSEC_PREF_UNIQUE;
		}
		if (prot == IPSEC_AH_ONLY)
			break;
		/* FALLTHRU */
	case IPSEC_ESP_ONLY:
		isr->ipsr_esp_req = IPSEC_PREF_REQUIRED;
		if (conf->ipsc_no_of_esp_algs != 0) {
			isr->ipsr_esp_alg = conf->ipsc_esp_algs[0];
		} else {
			isr->ipsr_esp_alg = 0;
		}
		if (conf->ipsc_no_of_esp_auth_algs != 0) {
			isr->ipsr_esp_auth_alg =
			    conf->ipsc_esp_auth_algs[0];
		} else {
			isr->ipsr_esp_auth_alg = 0;
		}
		if (conf->ipsc_sa_attr == IPSEC_UNIQUE_SA) {
			isr->ipsr_esp_req |= IPSEC_PREF_UNIQUE;
		}
	default:
		break;
	}
}


static uint8_t zeroes[16];
/*
 * Match a policy entry for the given protocol,addr,port.
 * If match is non-null, the matching policy entry is
 * copied back and B_TRUE is returned. Otherwise return
 * B_FALSE.
 */
static boolean_t
ipsec_match_policy(ipsec_policy_t *policy, ipsec_selector_t *sel,
    ipsec_conf_t *match)
{
	uint8_t *csmask;
	uint8_t *cdmask;
	uint8_t *csaddr;
	uint8_t *cdaddr;
	ipsec_conf_t *conf;
	ipaddr_t addr1;
	ipaddr_t mask1;
	ipaddr_t saddr, daddr, smask, dmask;
	uint16_t sport, dport;
	uint8_t protocol;

	/*
	 * Lock needs to be held before this routine is called.
	 * Otherwise policy could point to freed memory if it
	 * gets deleted by the time we try to acquire the lock
	 * here.
	 */
	ASSERT(RW_LOCK_HELD(&ipsec_conf_lock));

	protocol = sel->protocol;
	saddr = sel->src_addr;
	smask = sel->src_mask;
	daddr = sel->dst_addr;
	dmask = sel->dst_mask;
	sport = sel->src_port;
	dport = sel->dst_port;

	do {
		conf = &policy->ipsec_conf;
		/* XXX IPv6 : Fix needed for IPV6 */
		csaddr = conf->ipsc_src_addr;
		csmask = conf->ipsc_src_mask;
		cdaddr = conf->ipsc_dst_addr;
		cdmask = conf->ipsc_dst_mask;

		/*
		 * We match the ULP first, then address and port
		 * number. 0 means wildcard.
		 * NOTE : All the entries including the mask is
		 * in network order.
		 */
		if (conf->ipsc_ulp_prot == 0 ||
		    protocol == conf->ipsc_ulp_prot) {
			/*
			 * We support *contiguous* masks only. This function
			 * is called to check for duplicates during
			 * addition of new policy entries (ipsec_config_add)
			 * and also for matching inbound/outbound datagrams.
			 * While matching inobund/outbound, smask/dmask is
			 * always zero. If called from ipsec_config_add
			 * smask/dmask is always non-zero. Address matching
			 * is done as follows :
			 *
			 * o If address & mask does not match then they are
			 *   not referring to the same address. Thus we can
			 *   skip this one.
			 * o If address & mask matches, we need to look at
			 *   the prefix length only if we are called from
			 *   ipsec_config_add. If we are called from
			 *   ipsec_config_add smask/dmask is non-zero whenver
			 *   the address is non-zero. In that case the new
			 *   entry that we are adding should have a smaller
			 *   prefix length. E.g. if we have a /24 we can add
			 *   a /15 but not the other way around.
			 */
			if (bcmp(csaddr, zeroes, IP_ADDR_LEN) != 0) {
				addr1 = *(ipaddr_t *)csaddr;
				mask1 = *(ipaddr_t *)csmask;
				if (((addr1 & mask1) != (saddr & mask1)) ||
				    (smask != 0 && ntohl(smask) <
				    ntohl(mask1))) {
					policy = policy->ipsec_policy_next;
					continue;
				}
			}
			if (bcmp(cdaddr, zeroes, IP_ADDR_LEN) != 0) {
				addr1 = *(ipaddr_t *)cdaddr;
				mask1 = *(ipaddr_t *)cdmask;
				if (((addr1 & mask1) != (daddr & mask1)) ||
				    (dmask != 0 && ntohl(dmask) <
				    ntohl(mask1))) {
					policy = policy->ipsec_policy_next;
					continue;
				}
			}
			if (conf->ipsc_src_port != 0 &&
			    conf->ipsc_src_port != sport) {
				policy = policy->ipsec_policy_next;
				continue;
			}
			if (conf->ipsc_dst_port != 0 &&
			    conf->ipsc_dst_port != dport) {
				policy = policy->ipsec_policy_next;
				continue;
			}
			if (match != NULL)
				bcopy(conf, match, sizeof (ipsec_conf_t));
			return (B_TRUE);
		}
		policy = policy->ipsec_policy_next;
	} while (policy != NULL);

	return (B_FALSE);
}

/*
 * This function is called both for outbound and inbound datagrams
 * to get the right policy. "conf" is the global policy match and
 * "isr" is the per-socket policy match for a given datagram/socket
 * end point (ipc). This function sets/copies the right policy on
 * to "isr".
 */
static boolean_t
ipsec_override_policy(ipsec_conf_t *conf, ipsec_req_t *isr)
{
	uint_t ah_req, esp_req;
	uint_t req_mask;

	ASSERT(conf != NULL);

	ah_req = isr->ipsr_ah_req;
	esp_req = isr->ipsr_esp_req;

	req_mask = (IPSEC_PREF_REQUIRED|IPSEC_PREF_NEVER);

	if (!(ah_req & req_mask) && !(esp_req & req_mask)) {
		/*
		 * This does not have any per-socket policy.
		 * We always over-ride.
		 */
		ipsec_copy_policy(conf, isr);
		return (B_TRUE);
	}

	/*
	 * We don't want to mix per-socket policy with global policy.
	 * We either retain per-socket policy or the global policy
	 * and never a mix of both.
	 *
	 * First, check the cases where one of the per-socket policy
	 * is NEVER and the global policy can be applied to the other.
	 * For e.g. ah_req is 0 and esp_req is IPSEC_PREF_NEVER.
	 * We don't want to inherit policy if ah_req is 0 and esp_req
	 * is IPSEC_PREF_REQUIRED because we don't want to mix global
	 * and per-socket policy.
	 */
	if (conf->ipsc_ipsec_prot == IPSEC_AH_ONLY &&
	    !(ah_req & req_mask) && (esp_req & IPSEC_PREF_NEVER)) {

		/*
		 * We have a global policy with AH_ONLY and we inherit if
		 * the AH per-socket requests don't have any preference set
		 * and ESP preference is NEVER.
		 */
		ipsec_copy_policy(conf, isr);
		return (B_TRUE);

	} else if (conf->ipsc_ipsec_prot == IPSEC_ESP_ONLY &&
	    !(esp_req & req_mask) && (ah_req & IPSEC_PREF_NEVER)) {

		/*
		 * We have a global policy with ESP_ONLY and we inherit, if
		 * the ESP per-socket requests don't have any preference set
		 * and AH preference is NEVER.
		 */
		ipsec_copy_policy(conf, isr);
		return (B_TRUE);

	} else if (ipsec_override_persocket_policy &&
	    conf->ipsc_ipsec_prot == IPSEC_AH_ESP &&
	    !(ah_req & IPSEC_PREF_NEVER) && !(esp_req & IPSEC_PREF_NEVER) &&
	    (!(ah_req & req_mask) || !(esp_req & req_mask))) {
		/*
		 * We have a global policy with AH and ESP and we inherit
		 * if :
		 *
		 * 1) both AH's and ESP's per-socket preference does not
		 *    have IPSEC_PREF_NEVER set and
		 *
		 * 2) either one of AH/ESP requests is zero.
		 *
		 * NOTE: If both of them are zero, we would have returned
		 * from the first check in this function. And if both
		 * of them are set, we don't want to inherit global
		 * policy.
		 */
		ipsec_copy_policy(conf, isr);
		return (B_TRUE);
	}
	return (B_FALSE);
}

/*
 * Given a selector(sel) and per-socket policy(ipc), this function
 * looks up global policy for a match, calls ipsec_override_policy()
 * with both per-socket policy and global policy to derive the right
 * policy.
 */
static boolean_t
ipsec_inherit_global_policy(ipc_t *ipc, ipsec_req_t *isr,
    ipsec_selector_t *sel)
{
	ipsec_policy_t *head;
	ipsec_policy_t *policy;
	boolean_t outbound;
	boolean_t ret;
	ipsec_conf_t ma;
	uint32_t saddr;
	uint16_t sport;

	ASSERT(ipc != NULL || isr != NULL);
	outbound = sel->outbound;

	if (outbound) {
		head = &ipsec_policy_head[IPSEC_TYPE_OUTBOUND];
	} else {
		head = &ipsec_policy_head[IPSEC_TYPE_INBOUND];
	}
	/*
	 * Quick check to see if there is any global policy.
	 */
	if (head->ipsec_policy_next == NULL)
		return (B_FALSE);

	if (!outbound) {
		/*
		 * Change our notion of addresses as this
		 * is how the incoming packet would look.
		 */
		saddr = sel->src_addr;
		sport = sel->src_port;
		sel->src_addr = sel->dst_addr;
		sel->dst_addr = saddr;
		sel->src_port = sel->dst_port;
		sel->dst_port = sport;
	}
	/*
	 * Find a matching policy for the given selector.
	 *
	 * 1) If it is bypass, it overrides any other policy i.e
	 *    it goes out in clear.
	 *
	 * 2) If it is not bypass, call ipsec_override_policy()
	 *    to get the right policy.
	 */

	ret = B_FALSE;
	rw_enter(&ipsec_conf_lock, RW_READER);
	policy = head->ipsec_policy_next;
	if (policy != NULL) {
		ret = ipsec_match_policy(policy, sel, &ma);
	}
	rw_exit(&ipsec_conf_lock);
	if (!ret) {
		return (B_FALSE);
	}
	if (ma.ipsc_policy == IPSEC_POLICY_BYPASS) {
		bzero(isr, sizeof (ipsec_req_t));
		if (ipc != NULL) {
			if (outbound) {
				ipc->ipc_out_enforce_policy = B_FALSE;
			} else {
				ipc->ipc_in_enforce_policy = B_FALSE;
			}
		}
		return (B_FALSE);
	} else {
		return (ipsec_override_policy(&ma, isr));
	}
}

/*
 * This is used for loopback to convert the IPSEC_OUT
 * to a IPSEC_IN before fanout, where the policy check happens.
 * In most of the cases, IPSEC processing has *never* been done.
 * There is one case (ip_wput_ire_fragmentit -> ip_wput_frag ->
 * icmp_frag_needed) where the packet is destined for localhost,
 * IPSEC processing has already been done.
 */
static void
ipsec_out_to_in(mblk_t *ipsec_mp)
{
	ipsec_out_t *io;
	ipsec_in_t  *ii;
	int ah_req;
	int esp_req;
	int se_req;

	ASSERT(ipsec_mp->b_datap->db_type == M_CTL);

	io = (ipsec_out_t *)ipsec_mp->b_rptr;

	ah_req = io->ipsec_out_ah_req;
	esp_req = io->ipsec_out_esp_req;
	se_req = io->ipsec_out_self_encap_req;

	ii = (ipsec_in_t *)ipsec_mp->b_rptr;
	bzero(ii, sizeof (ipsec_in_t));

	ii->ipsec_in_type = IPSEC_IN;
	ii->ipsec_in_len = sizeof (ipsec_in_t);
	ii->ipsec_in_loopback = B_TRUE;
	/*
	 * In most of the cases, we can't look at the ipsec_out_XXX_spi
	 * because this never went thru' IPSEC processing. So, look at
	 * the requests and infer whether it would have gone through
	 * IPSEC processing or not. Initialize the "done" fields with
	 * the requests. The possible values for "done" fields are :
	 *
	 * 1) zero, indicates that a particular preference was never
	 *    requested.
	 * 2) non-zero, indicates that it could be IPSEC_PREF_REQUIRED/
	 *    IPSEC_PREF_NEVER. If IPSEC_REQ_DONE is set, it means that
	 *    IPSEC processing has been completed.
	 */
	ii->ipsec_in_ah_done = ah_req;
	ii->ipsec_in_esp_done = esp_req;
	ii->ipsec_in_self_encap_done = se_req;
	ii->ipsec_in_secure = B_TRUE;
	ii->ipsec_in_v4 = B_TRUE;
}

static void
ipsec_init_ports(ipsec_selector_t *sel, mblk_t *mp)
{
	uint16_t *ports;
	int hdr_len;
	ipha_t *ipha;

	ipha = (ipha_t *)mp->b_rptr;
	hdr_len = IPH_HDR_LENGTH(mp->b_rptr);

	switch (ipha->ipha_protocol) {
	case IPPROTO_TCP:
	case IPPROTO_UDP:
		ports = (uint16_t *)&mp->b_rptr[hdr_len];
		sel->src_port = *ports++;
		sel->dst_port = *ports;
		break;
	default:
		sel->src_port = sel->dst_port = 0;
		break;
	}
}

/*
 * When sending a response to a ICMP request or generating a RST
 * in the TCP case, the outbound packets need to go at the same level
 * of protection as the incoming ones i.e we associate our outbound
 * policy with how the packet came in. We call this after we have
 * accepted the incoming packet which may or may not have been in
 * clear and hence we are sending the reply back with the policy
 * matching the incoming datagram's policy.
 *
 * NOTE : This technology serves two purposes :
 *
 * 1) If we have multiple outbound policies, we send out a reply
 *    matching with how it came in rather than matching the outbound
 *    policy.
 *
 * 2) For assymetric policies, we want to make sure that incoming
 *    and outgoing has the same level of protection. Assymetric
 *    policies exist only with global policy where we may not have
 *    both outbound and inbound at the same time.
 */
mblk_t *
ipsec_in_to_out(mblk_t *ipsec_mp)
{
	ipsec_in_t  *ii;
	ipsec_out_t  *io;
	uint32_t ah_spi;
	uint32_t esp_spi;
	boolean_t decaps;
	uint_t ah_alg, esp_ealg, esp_aalg;
	ipsa_t *assoc;
	mblk_t *mp;
	ipha_t *ipha;
	boolean_t secure;
	ipsec_selector_t sel;

	ASSERT(ipsec_mp->b_datap->db_type == M_CTL);

	ii = (ipsec_in_t *)ipsec_mp->b_rptr;

	mp = ipsec_mp->b_cont;
	ASSERT(mp != NULL);
	ipha = (ipha_t *)mp->b_rptr;
	ah_alg = esp_ealg = esp_aalg = 0;

	if (ii->ipsec_in_loopback) {
		/*
		 * There is no spi for this case as in most of the
		 * cases this never goes through IPSEC processing.
		 * So, look at the "done" fields which is initalized
		 * in ipsec_out_to_in. Initialize the "spi" fields
		 * with non-zero value if the datagram has some
		 * security requests. Refer to ipsec_out_to_in for
		 * details of loopback initializations.
		 */
		if (ii->ipsec_in_ah_done & IPSEC_PREF_REQUIRED)
			ah_spi = 1;
		else
			ah_spi = 0;
		if (ii->ipsec_in_esp_done & IPSEC_PREF_REQUIRED)
			esp_spi = 1;
		else
			esp_spi = 0;
		if (ii->ipsec_in_self_encap_done & IPSEC_PREF_REQUIRED)
			decaps = B_TRUE;
		else
			decaps = B_FALSE;
	} else {
		ah_spi = ii->ipsec_in_ah_spi;
		esp_spi = ii->ipsec_in_esp_spi;
		/*
		 * Try to get the algorithms that were used for
		 * this packet.
		 */

		if (ah_spi != 0) {
			assoc = getahassoc(ipsec_mp,
			    (uint8_t *)&ipha->ipha_src,
			    (uint8_t *)&ipha->ipha_dst, IP_ADDR_LEN);
			if (assoc != NULL) {
				ah_alg = assoc->ipsa_auth_alg;
				IPSA_REFRELE(assoc);
			}
		}
		if (esp_spi != 0) {
			assoc = getespassoc(ipsec_mp,
			    (uint8_t *)&ipha->ipha_src,
			    (uint8_t *)&ipha->ipha_dst, IP_ADDR_LEN);
			if (assoc != NULL) {
				esp_ealg = assoc->ipsa_encr_alg;
				esp_aalg = assoc->ipsa_auth_alg;
				IPSA_REFRELE(assoc);
			}
		}
		decaps = ii->ipsec_in_decaps;
	}
	secure = ii->ipsec_in_secure;


	/*
	 * The caller is going to send the datagram out which might
	 * go on the wire or delivered locally through ip_wput_local.
	 *
	 * 1) If it goes out on the wire, new associations will be
	 *    obtained.
	 * 2) If it is delivered locally, ip_wput_local will convert
	 *    this IPSEC_OUT to a IPSEC_IN looking at the requests.
	 */

	io = (ipsec_out_t *)ipsec_mp->b_rptr;
	bzero(io, sizeof (ipsec_out_t));
	io->ipsec_out_type = IPSEC_OUT;
	io->ipsec_out_len = sizeof (ipsec_out_t);

	io->ipsec_out_ah_req = (ah_spi != 0) ? IPSEC_PREF_REQUIRED : 0;
	io->ipsec_out_esp_req = (esp_spi != 0) ? IPSEC_PREF_REQUIRED : 0;
	io->ipsec_out_self_encap_req = decaps ? IPSEC_PREF_REQUIRED : 0;

	io->ipsec_out_esp_alg = esp_ealg;
	io->ipsec_out_esp_ah_alg = esp_aalg;
	io->ipsec_out_ah_alg = ah_alg;

	ipsec_init_ports(&sel, mp);
	io->ipsec_out_src_port = sel.src_port;
	io->ipsec_out_dst_port = sel.dst_port;
	io->ipsec_out_proto = ipha->ipha_protocol;

	/*
	 * Don't use global policy for this, as we want
	 * to match with the IPSEC_IN policy.
	 */
	io->ipsec_out_use_global_policy = B_FALSE;
	io->ipsec_out_proc_begin = B_FALSE;
	io->ipsec_out_secure = secure;
	return (ipsec_mp);
}

/*
 * This is called only for outbound datagrams if the datagram needs to
 * go out secure.  A NULL mp can be passed to get an ipsec_out. This
 * facility is used by ip_unbind.
 *
 * NOTE : o As the data part could be modified by ipsec_out_process etc.
 *	    we can't make it fast by calling a dup.
 *
 *        o We can ASSERT that one of ipc_out_ah_req/ipc_out_esp_req/
 *	    ipc_out_self_encap_req is IPSEC_PREF_REQUIRED here. We don't
 *	    do it as these fields are not protected by any locks.
 */
static mblk_t *
ipsec_attach_ipsec_out(mblk_t *mp, ipc_t *ipc, ipsec_req_t *isr, uint8_t proto)
{
	mblk_t *ipsec_mp;

	if ((ipsec_mp = allocb(sizeof (ipsec_info_t), BPRI_HI)) == NULL) {
		(void) mi_strlog(ipc->ipc_wq, 0, SL_ERROR|SL_NOTE,
		    "ipsec_attach_ipsec_out: Allocation failure\n");
		BUMP_MIB(ip_mib.ipOutDiscards);
		freemsg(mp);
		return (NULL);
	}
	ipsec_mp->b_cont = mp;
	return (ipsec_init_ipsec_out(ipsec_mp, ipc, isr, proto));
}

/*
 * Initialize the IPSEC_OUT (ipsec_mp) using isr if it is non-null.
 * Otherwise initialize using ipc.
 */
static mblk_t *
ipsec_init_ipsec_out(mblk_t *ipsec_mp, ipc_t *ipc, ipsec_req_t *isr,
    uint8_t proto)
{
	mblk_t *mp;
	ipsec_out_t *io;

	/*
	 * If mp is NULL, we won't/should not be using it.
	 */
	mp = ipsec_mp->b_cont;

	ipsec_mp->b_datap->db_type = M_CTL;
	ipsec_mp->b_wptr += sizeof (ipsec_info_t);

	io = (ipsec_out_t *)ipsec_mp->b_rptr;
	bzero(io, sizeof (ipsec_out_t));

	io->ipsec_out_type = IPSEC_OUT;
	io->ipsec_out_len = sizeof (ipsec_out_t);
	if (ipc != NULL) {
		ASSERT(ipc->ipc_outbound_policy != NULL);
		io->ipsec_out_ah_req = ipc->ipc_out_ah_req;
		io->ipsec_out_esp_req = ipc->ipc_out_esp_req;
		io->ipsec_out_self_encap_req = ipc->ipc_out_self_encap_req;
		io->ipsec_out_ah_alg = ipc->ipc_out_auth_alg;
		io->ipsec_out_esp_alg = ipc->ipc_out_esp_alg;
		io->ipsec_out_esp_ah_alg = ipc->ipc_out_esp_auth_alg;
		io->ipsec_out_policy_cached = ipc->ipc_policy_cached;
		if (ipc->ipc_policy_cached) {
			io->ipsec_out_src_port = ipc->ipc_lport;
			io->ipsec_out_dst_port = ipc->ipc_fport;
		} else {
			/*
			 * ipc does not have the port information. Get
			 * it from the packet.
			 */
			ipsec_selector_t sel;

			ASSERT(mp != NULL);
			ipsec_init_ports(&sel, mp);
			io->ipsec_out_src_port = sel.src_port;
			io->ipsec_out_dst_port = sel.dst_port;
		}
	} else {
		io->ipsec_out_ah_req = isr->ipsr_ah_req;
		io->ipsec_out_esp_req = isr->ipsr_esp_req;
		io->ipsec_out_self_encap_req = isr->ipsr_self_encap_req;
		io->ipsec_out_ah_alg = isr->ipsr_auth_alg;
		io->ipsec_out_esp_alg = isr->ipsr_esp_alg;
		io->ipsec_out_esp_ah_alg = isr->ipsr_esp_auth_alg;
	}

	io->ipsec_out_proto = proto;
	io->ipsec_out_use_global_policy = B_TRUE;
	io->ipsec_out_secure = B_TRUE;
	return (ipsec_mp);
}

/*
 * We check whether an inbound datagram is a valid one
 * to accept in clear. If it is secure, it is the job
 * of IPSEC to log information appropriately if it
 * suspects that it may not be the real one.
 *
 * It is called only while fanning out to the ULP
 * where ULP accepts only secure data and the incoming
 * is clear. Usually we never accept clear datagrams in
 * such cases. ICMP is the only exception.
 *
 * NOTE : We don't call this function if the client (ULP)
 * is willing to accept things in clear.
 */
boolean_t
ipsec_inbound_accept_clear(mblk_t *mp)
{
	int iph_hdr_length;
	ipha_t *ipha;
	icmph_t *icmph;

	ipha = (ipha_t *)mp->b_rptr;
	/*
	 * If it is not ICMP, fail this request.
	 */
	if (ipha->ipha_protocol != IPPROTO_ICMP)
		return (B_FALSE);

	/*
	 * It is an insecure icmp message. Check to see whether we are
	 * willing to accept this one.
	 */
	iph_hdr_length = IPH_HDR_LENGTH(ipha);
	icmph = (icmph_t *)&mp->b_rptr[iph_hdr_length];

	switch (icmph->icmph_type) {
	case ICMP_ECHO_REPLY:
	case ICMP_TIME_STAMP_REPLY:
	case ICMP_INFO_REPLY:
	case ICMP_ROUTER_ADVERTISEMENT:
		/*
		 * We should not encourage clear replies if this
		 * client expects secure. If somebody is replying
		 * in clear some mailicious user watching both the
		 * request and reply, can do chosen-plain-text attacks.
		 * With global policy we might be just expecting secure
		 * but sending out clear. We don't know what the right
		 * thing is. We can't do much here as we can't control
		 * the sender here. Till we are sure of what to do,
		 * accept them.
		 */
		return (B_TRUE);
	case ICMP_ECHO_REQUEST:
	case ICMP_TIME_STAMP_REQUEST:
	case ICMP_INFO_REQUEST:
	case ICMP_ADDRESS_MASK_REQUEST:
	case ICMP_ROUTER_SOLICITATION:
	case ICMP_ADDRESS_MASK_REPLY:
		/*
		 * Don't accept this as somebody could be sending
		 * us plain text to get encrypted data. If we reply,
		 * it will lead to chosen plain text attack.
		 */
		return (B_FALSE);
	case ICMP_DEST_UNREACHABLE:
		switch (icmph->icmph_code) {
		case ICMP_FRAGMENTATION_NEEDED:
			/*
			 * Be in sync with icmp_inbound, where we have
			 * already set ire_max_frag.
			 */
			return (B_TRUE);
		case ICMP_HOST_UNREACHABLE:
		case ICMP_NET_UNREACHABLE:
			/*
			 * By accepting, we could reset a connection.
			 * How do we solve the problem of some
			 * intermediate router sending in-secure ICMP
			 * messages ?
			 */
			return (B_TRUE);
		case ICMP_PORT_UNREACHABLE:
		case ICMP_PROTOCOL_UNREACHABLE:
		default :
			return (B_FALSE);
		}
	case ICMP_SOURCE_QUENCH:
		/*
		 * If this is an attack, TCP will slow start
		 * because of this. Is it very harmful ?
		 */
		return (B_TRUE);
	case ICMP_PARAM_PROBLEM:
		return (B_FALSE);
	case ICMP_TIME_EXCEEDED:
		return (B_TRUE);
	case ICMP_REDIRECT:
		return (B_FALSE);
	default :
		return (B_FALSE);
	}
}

/*
 * Check to see whether this outbound datagram met the
 * policy constraints. Called only if IPSEC processing fails.
 */
static boolean_t
ipsec_check_ipsecout_policy(mblk_t *ipsec_mp)
{
	ipsec_out_t *io;
	uint_t ah_req;
	uint_t esp_req;
	uint_t se_req;
#ifdef DEBUG
	mblk_t *mp;
	ipha_t *ipha;
	uint32_t ah_spi;
	uint32_t esp_spi;
	ipsa_t *ah_assoc;
	ipsa_t *esp_assoc;
#endif

	io = (ipsec_out_t *)ipsec_mp->b_rptr;
	ASSERT(io->ipsec_out_type == IPSEC_OUT);

#ifdef DEBUG
	/*
	 * We come here because of memory allocation failures
	 * while doing IPSEC processing, or no associations
	 * found etc.
	 */

	ah_spi = io->ipsec_out_ah_spi;
	esp_spi = io->ipsec_out_esp_spi;
	mp = ipsec_mp->b_cont;
	ipha = (ipha_t *)mp->b_rptr;
	/*
	 * As of now we don't do much by getting the association.
	 * But this is where we would do checks in the future
	 * if necessary.
	 */
	ah_assoc = NULL;
	esp_assoc = NULL;
	if (ah_spi != 0) {
		ah_assoc = getahassoc(ipsec_mp, (uint8_t *)
		    &ipha->ipha_src, (uint8_t *)&ipha->ipha_dst,
		    IP_ADDR_LEN);
		if (ah_assoc != NULL) {
			if (io->ipsec_out_ah_alg != 0) {
				ASSERT(io->ipsec_out_ah_alg ==
				    ah_assoc->ipsa_auth_alg);
			}
			IPSA_REFRELE(ah_assoc);
		}
	}

	if (esp_spi != 0) {
		esp_assoc = getespassoc(ipsec_mp, (uint8_t *)
		    &ipha->ipha_src, (uint8_t *)&ipha->ipha_dst,
		    IP_ADDR_LEN);
		if (esp_assoc != NULL) {
			if (io->ipsec_out_esp_alg != 0) {
				ASSERT(io->ipsec_out_esp_alg ==
				    esp_assoc->ipsa_encr_alg);
			}
			if (io->ipsec_out_esp_ah_alg != 0) {
				ASSERT(io->ipsec_out_esp_ah_alg ==
				    esp_assoc->ipsa_auth_alg);
			}
		}
	}
#endif

	ah_req = io->ipsec_out_ah_req;
	esp_req = io->ipsec_out_esp_req;
	se_req = io->ipsec_out_self_encap_req;

	/*
	 * If a request preference is "REQUIRED" and it has
	 * failed, we should return B_FALSE meaning that the
	 * caller should drop the datagram. If the preference
	 * is NEVER, assert that IPSEC processing has not been
	 * done. We may  be called more than once per datagram
	 * (for AH and ESP separately) unlike the inbound side
	 * where we call once before delivery to the ULP.
	 */
	if ((esp_req & IPSEC_PREF_REQUIRED) && (esp_req & IPSEC_REQ_FAILED)) {
		ip1dbg(("Encryption request failed\n"));
		return (B_FALSE);
	}

	if ((esp_req & IPSEC_PREF_NEVER) && (esp_req & IPSEC_REQ_DONE)) {
		ip1dbg(("Packet encrypted while not required"));
		return (B_FALSE);
	}

	if ((ah_req & IPSEC_PREF_REQUIRED) && (ah_req & IPSEC_REQ_FAILED)) {
		ip1dbg(("Authentication request failed\n"));
		return (B_FALSE);
	}

	if ((ah_req & IPSEC_PREF_NEVER) && (ah_req & IPSEC_REQ_DONE)) {
		ip1dbg(("Packet authenticated while not required"));
		return (B_FALSE);
	}

	if ((se_req & IPSEC_PREF_REQUIRED) && (se_req & IPSEC_REQ_FAILED)) {
		ip1dbg(("Self-Encapsulation request failed\n"));
		return (B_FALSE);
	}

	if ((se_req & IPSEC_PREF_NEVER) && (se_req & IPSEC_REQ_DONE)) {
		ip1dbg(("Packet self-encapsulated while not required"));
		return (B_FALSE);
	}

	return (B_TRUE);
}

/*
 * Check to see whether this datagram meets the policy
 * constraints. ipsr has the policy constraints.
 */
boolean_t
ipsec_check_ipsecin_policy(char *fcn_name, queue_t *q, mblk_t *ipsec_mp,
    ipsec_req_t *ipsr)
{
	mblk_t *mp;
	ipha_t *ipha;
	uint32_t ah_spi;
	uint32_t esp_spi;
	boolean_t decaps;
	ipsec_in_t *ii;
	ipsa_t *ah_assoc;
	ipsa_t *esp_assoc;
	boolean_t ret;

	ASSERT(ipsr != NULL);
	/*
	 * TCP sends its own function name.
	 */
	if (fcn_name == NULL)
		fcn_name = "ipsec_check_ipsecin_policy";

	ii = (ipsec_in_t *)ipsec_mp->b_rptr;
	ASSERT(ii->ipsec_in_type == IPSEC_IN);

	ah_spi = ii->ipsec_in_ah_spi;
	esp_spi = ii->ipsec_in_esp_spi;
	decaps = ii->ipsec_in_decaps;

	mp = ipsec_mp->b_cont;
	ipha = (ipha_t *)mp->b_rptr;

	/*
	 * Handle the loopback separately as there are
	 * no security associations for this case.
	 *
	 * NOTE : No algorithm checks are done.
	 */
	if (ii->ipsec_in_loopback) {
		/*
		 * We don't have spi to look at, as this datagram
		 * may not have gone through IPSEC processing. Look at
		 * what operations would have been done by looking
		 * at the done fields.
		 */
		if (ipsr->ipsr_ah_req & IPSEC_PREF_REQUIRED) {
			if (!(ii->ipsec_in_ah_done & IPSEC_PREF_REQUIRED)) {
				return (B_FALSE);
			}
		} else if (ii->ipsec_in_ah_done & IPSEC_PREF_REQUIRED) {
			/*
			 * This can happen if somebody uses per-socket
			 * policy to send secure data while the recipient
			 * does not expect one.
			 */
			ipsec_log_policy_failure(q,
			    IPSEC_POLICY_AUTH_NOT_NEEDED,
			    fcn_name, ipha, B_TRUE);
			return (B_FALSE);
		}

		if (ipsr->ipsr_esp_req & IPSEC_PREF_REQUIRED) {
			if (!(ii->ipsec_in_esp_done & IPSEC_PREF_REQUIRED)) {
				return (B_FALSE);
			}
		} else if (ii->ipsec_in_esp_done & IPSEC_PREF_REQUIRED) {
			/*
			 * This can happen if somebody uses per-socket
			 * policy to send secure data while the recipient
			 * does not expect one.
			 */
			ipsec_log_policy_failure(q,
			    IPSEC_POLICY_ENCR_NOT_NEEDED,
			    fcn_name, ipha, B_TRUE);
			return (B_FALSE);
		}

		if (ipsr->ipsr_self_encap_req & IPSEC_PREF_REQUIRED) {
			if (!(ii->ipsec_in_self_encap_done &
			    IPSEC_PREF_REQUIRED)) {
				return (B_FALSE);
			}
		} else if (ii->ipsec_in_self_encap_done & IPSEC_PREF_REQUIRED) {
			/*
			 * This can happen if somebody uses per-socket
			 * policy to send secure data while the recipient
			 * does not expect one.
			 */
			ipsec_log_policy_failure(q, IPSEC_POLICY_SE_NOT_NEEDED,
			    fcn_name, ipha, B_TRUE);
			return (B_FALSE);
		}
		return (B_TRUE);
	}

	ah_assoc = NULL;
	esp_assoc = NULL;
	ret = B_TRUE;
	if (ah_spi != 0) {
		ah_assoc = getahassoc(ipsec_mp, (uint8_t *)
		    &ipha->ipha_src, (uint8_t *)&ipha->ipha_dst,
		    IP_ADDR_LEN);
		if (ah_assoc == NULL) {
			(void) mi_strlog(q, 0, SL_ERROR|SL_WARN|SL_CONSOLE,
			    "%s: No AH association "
			    "found for the inbound datagram\n", fcn_name);
			ret = B_FALSE;
			goto err;
		}
	}

	if (esp_spi != 0) {
		esp_assoc = getespassoc(ipsec_mp, (uint8_t *)
		    &ipha->ipha_src, (uint8_t *)&ipha->ipha_dst,
		    IP_ADDR_LEN);

		if (esp_assoc == NULL) {
			(void) mi_strlog(q, 0, SL_ERROR|SL_WARN|SL_CONSOLE,
			    "%s: No ESP association "
			    "found for the inbound datagram\n", fcn_name);
			ret = B_FALSE;
			goto err;
		}
	}
	/*
	 * As of now we do the simple checks of whether
	 * the datagram has gone through the required IPSEC
	 * protocol constraints or not. We might have more
	 * in the future like sensitive levels, key bits, etc.
	 * If it fails the constraints, check whether we would
	 * have accepted this if it had come in clear.
	 */
	if (ipsr->ipsr_ah_req & IPSEC_PREF_REQUIRED) {
		if (ah_spi == 0) {
			ret = ipsec_inbound_accept_clear(mp);
			goto err;
		}

		ASSERT(ah_assoc != NULL);
		if (ipsr->ipsr_auth_alg != 0) {
			if (ah_assoc->ipsa_auth_alg != ipsr->ipsr_auth_alg) {
				(void) mi_strlog(q, 0,
				    SL_ERROR|SL_WARN|SL_CONSOLE,
				    "%s: Authentication Algorithm "
				    "mismatch for the inbound datagram\n",
				    fcn_name);
				ret = B_FALSE;
				goto err;
			}
		}
	} else if (ah_spi != 0) {
		/*
		 * Don't allow this. Check IPSEC NOTE above
		 * ip_fanout_proto().
		 */
		ipsec_log_policy_failure(q,
		    IPSEC_POLICY_AUTH_NOT_NEEDED, fcn_name, ipha, B_TRUE);
		ret = B_FALSE;
		goto err;
	}

	if (ipsr->ipsr_esp_req & IPSEC_PREF_REQUIRED) {
		if (esp_spi == 0) {
			ret = ipsec_inbound_accept_clear(mp);
			goto err;
		}

		ASSERT(esp_assoc != NULL);
		if (ipsr->ipsr_esp_alg != 0) {
			if (esp_assoc->ipsa_encr_alg != ipsr->ipsr_esp_alg) {
				(void) mi_strlog(q, 0,
				    SL_ERROR|SL_WARN|SL_CONSOLE,
				    "%s: Encryption "
				    "Algorithm mismatch for the inbound "
				    "datagram\n", fcn_name);
				ret = B_FALSE;
				goto err;
			}
		}
		/*
		 * If the client does not need authentication, we don't
		 * verify the alogrithm.
		 */
		if (ipsr->ipsr_esp_auth_alg != 0) {
			if (esp_assoc->ipsa_auth_alg !=
			    ipsr->ipsr_esp_auth_alg) {
				(void) mi_strlog(q, 0,
				    SL_ERROR|SL_WARN|SL_CONSOLE,
				    "%s: Authentication Algorithm "
				    "used with Encryption, mismatch "
				    "for the inbound datagram\n", fcn_name);
				ret = B_FALSE;
				goto err;
			}
		}
	} else if (esp_spi != 0) {
		/*
		 * Don't allow this. Check IPSEC NOTE above
		 * ip_fanout_proto().
		 */
		ipsec_log_policy_failure(q,
		    IPSEC_POLICY_ENCR_NOT_NEEDED, fcn_name, ipha, B_TRUE);
		ret = B_FALSE;
		goto err;
	}

	if (ipsr->ipsr_self_encap_req & IPSEC_PREF_REQUIRED) {
		if (!decaps) {
			ret = ipsec_inbound_accept_clear(mp);
		}
	} else if (decaps) {
		/*
		 * XXX If the packet comes in tunneled and the recipient does
		 * not expect it to be tunneled, it is okay. But we drop  to
		 * be consistent with the other cases.
		 */
		ipsec_log_policy_failure(q,
		    IPSEC_POLICY_SE_NOT_NEEDED, fcn_name, ipha, B_TRUE);
		ret = B_FALSE;
	}
err:
	if (ah_assoc != NULL)
		IPSA_REFRELE(ah_assoc);
	if (esp_assoc != NULL)
		IPSA_REFRELE(esp_assoc);
	if (!ret) {
		BUMP_MIB(ip_mib.ipsecInFailed);
	} else {
		BUMP_MIB(ip_mib.ipsecInSucceeded);
	}
	return (ret);
}

/*
 * Check with global policy and see whether this inbound
 * packet meets the policy constraints.
 *
 * 1) ipc is null and incoming packet is clear.
 *	- if global policy matches, then return B_FALSE to
 *	  indicate policy failure. Otherwise return B_TRUE.
 *
 * 2) ipc is null and incoming datagram is secure.
 *	- if global policy matches, copy the global policy by calling
 *	  ipsec_override_policy onto a ipsec_req_t struct and call
 *	  ipsec_check_ipsecin_policy.
 *
 * 3) ipc is non-null and incoming datagram is clear.
 *	- if global policy matches, call ipsec_override_policy which
 *	  looks at both policy in ipc and the global and returns
 *	  the result. If at the end we still require some level of
 *	  protection, we return B_FALSE.
 *
 * 4) ipc is non-null and incoming datagram is secure.
 *	- if global policy matches, call ipsec_check_ipsecin_policy
 *	  after overriding the per-socket policy with the global policy.
 */
boolean_t
ipsec_check_global_policy(mblk_t *mp, ipc_t *ipc)
{
	ipsec_policy_t *i_head;
	ipsec_policy_t *policy;
	ipha_t *ipha;
	mblk_t *ipsec_mp;
	ipsec_req_t inbp;
	ipsec_req_t *ipsr;
	ipsec_conf_t match;
	ipsec_selector_t sel;
	boolean_t secure;
	queue_t *q;

	if (ipc != NULL) {
		q = ipc->ipc_wq;
		/*
		 * We need to copy the inbound policy, because
		 * ipsec_override_policy called either from this function
		 * or ipsec_check_ipsecin_policy might overwrite the
		 * ipc->ipc_inbound_policy if we have a global policy match.
		 */
		if (ipc->ipc_inbound_policy != NULL) {
			bcopy(ipc->ipc_inbound_policy, &inbp,
			    sizeof (ipsec_req_t));
		} else {
			bzero(&inbp, sizeof (ipsec_req_t));
		}
		ipsr = &inbp;
	} else {
		q = NULL;
	}

	if (ipsec_policy_head[IPSEC_TYPE_INBOUND].ipsec_policy_next == NULL &&
	    ipc == NULL) {
		/*
		 * No global policy and no per-socket policy.
		 */
		return (B_TRUE);
	}

	if (mp->b_datap->db_type == M_CTL) {
		secure = B_TRUE;
		ipsec_mp = mp;
		ASSERT(((ipsec_in_t *)ipsec_mp->b_rptr)->ipsec_in_type ==
		    IPSEC_IN);
		mp = mp->b_cont;
	} else {
		secure = B_FALSE;
		ASSERT(mp->b_datap->db_type == M_DATA);
		ipsec_mp = mp;
	}

	if (ipsec_policy_head[IPSEC_TYPE_INBOUND].ipsec_policy_next == NULL) {
		/*
		 * There is no global policy to check. Check
		 * for per-socket policy for non-null ipcs.
		 */
		if (ipc != NULL) {
			goto no_gpolicy;
		} else {
			return (B_TRUE);
		}
	}

	ipha = (ipha_t *)mp->b_rptr;
	sel.dst_addr = ipha->ipha_dst;
	sel.src_addr = ipha->ipha_src;
	sel.src_mask = sel.dst_mask = 0;
	sel.protocol = ipha->ipha_protocol;

	/* Initialize the ports in the selector */
	ipsec_init_ports(&sel, mp);

	/*
	 * Look up the global policy.
	 *
	 * If we find global policy, we should look at both
	 * local policy and global policy and see which is
	 * stronger and match accordingly.
	 *
	 * If we don't find a global policy, check with
	 * local policy alone.
	 */
	i_head = &ipsec_policy_head[IPSEC_TYPE_INBOUND];
	rw_enter(&ipsec_conf_lock, RW_READER);
	policy = i_head->ipsec_policy_next;
	if (policy != NULL) {
		if (ipsec_match_policy(policy, &sel, &match)) {

			rw_exit(&ipsec_conf_lock);
			/* If global policy is BYPASS, return success. */
			if (match.ipsc_policy == IPSEC_POLICY_BYPASS) {
				BUMP_MIB(ip_mib.ipsecInSucceeded);
				return (B_TRUE);
			}

			if (secure) {
				if (ipc == NULL) {
					bzero(&inbp, sizeof (ipsec_req_t));
					ipsr = &inbp;
				}
				/*
				 * Derive the stronger policy and compare with
				 * that. Don't update MIB as it is done in
				 * ipsec_check_ipsecin_policy().
				 */
				(void) ipsec_override_policy(&match, ipsr);
				return (ipsec_check_ipsecin_policy(NULL, q,
				    ipsec_mp, ipsr));
			} else if (ipc == NULL) {
				BUMP_MIB(ip_mib.ipsecInFailed);
				return (B_FALSE);
			} else {
				/*
				 * This datagram has come in clear.
				 * We can't fail this because there
				 * is a global policy match e.g. there
				 * could be a per-socket option of
				 * NEVER in which case we would not
				 * have inherited the policy.
				 */
				ASSERT(!ipc->ipc_policy_cached);
				(void) ipsec_override_policy(&match, ipsr);
				goto check;
			}
		}
	}
	rw_exit(&ipsec_conf_lock);

no_gpolicy:
	/*
	 * There is no global policy. There could be some
	 * per-socket policy.  (Assuming there is a socket instance...)
	 */
	if (secure) {
		/*
		 * As there is no global policy, just check with
		 * per-socket policy alone by passing in a NULL
		 * for the match. If ipc is NULL, we don't have
		 * to check anything.
		 */
		if (ipc == NULL)
			return (B_TRUE);
		return (ipsec_check_ipsecin_policy(NULL, q, ipsec_mp, ipsr));
	} else {
check:
		/*
		 * We come here if it is a clear datagram and
		 *
		 * 1) if there is no global policy.
		 * 2) if there is both global policy and per-socket policy
		 *    which is contained in ipsr.
		 */
		if (ipc != NULL &&
		    ((ipsr->ipsr_ah_req & IPSEC_PREF_REQUIRED) ||
			(ipsr->ipsr_esp_req & IPSEC_PREF_REQUIRED) ||
			(ipsr->ipsr_self_encap_req & IPSEC_PREF_REQUIRED))) {

			boolean_t ret = ipsec_inbound_accept_clear(mp);

			if (!ret) {
				BUMP_MIB(ip_mib.ipsecInFailed);
			} else {
				BUMP_MIB(ip_mib.ipsecInSucceeded);
			}
			return (ret);
		} else {
			BUMP_MIB(ip_mib.ipsecInSucceeded);
			return (B_TRUE);
		}
	}
}

/*
 * Check whether the policy constraints are met either for an
 * outbound or an inbound datagram.
 */
static boolean_t
ipsec_check_policy(mblk_t *ipsec_mp, ipc_t *ipc, boolean_t outbound,
    boolean_t mctl_present)
{
	ipsec_in_t *ii;
	boolean_t ret;

	if (mctl_present && !outbound) {
		/*
		 * If it is inbound check whether the attached message
		 * is secure or not. We have a special case for ICMP,
		 * where we have a IPSEC_IN message and the attached
		 * message is not secure. See icmp_inbound_error_fanout
		 * for details.
		 */
		ii = (ipsec_in_t *)ipsec_mp->b_rptr;
		ASSERT(ii->ipsec_in_type == IPSEC_IN);
		if (!ii->ipsec_in_secure) {
			mctl_present = B_FALSE;
			ipsec_mp = ipsec_mp->b_cont;
			ASSERT(ipsec_mp != NULL);
			ASSERT(ipsec_mp->b_datap->db_type == M_CTL);
		}
	}
	if (!mctl_present) {
		/*
		 * This is the case where the incoming datagram is
		 * cleartext and we need to see whether this client
		 * would like to receive such untrusty things from
		 * the wire.
		 *
		 * NOTE: This case never comes for outbound.
		 */
		mblk_t *mp = ipsec_mp;		/* For readability */
		ASSERT(ipc != NULL);
		ASSERT(!outbound);
		ASSERT(mp != NULL);

		if (ipc->ipc_policy_cached) {
			/*
			 * Policy is cached in the ipc.
			 */
			ASSERT(ipc->ipc_inbound_policy != NULL);
			if ((ipc->ipc_in_ah_req & IPSEC_PREF_REQUIRED) ||
			    (ipc->ipc_in_esp_req & IPSEC_PREF_REQUIRED) ||
			    (ipc->ipc_in_self_encap_req &
			    IPSEC_PREF_REQUIRED)) {
				ret = ipsec_inbound_accept_clear(mp);
				if (ret) {
					BUMP_MIB(ip_mib.ipsecInSucceeded);
				} else {
					BUMP_MIB(ip_mib.ipsecInFailed);
				}
				return (ret);
			} else {
				BUMP_MIB(ip_mib.ipsecInSucceeded);
				return (B_TRUE);
			}
		} else {
			/*
			 * As this is a non-hardbound connection we need
			 * to look at both per-socket policy and global
			 * policy. As this is cleartext, mark the mp as
			 * M_DATA in case if it is an ICMP error being
			 * reported before calling ipsec_check_global_policy
			 * so that it does not mistake it for IPSEC_IN.
			 */
			uchar_t db_type = mp->b_datap->db_type;
			mp->b_datap->db_type = M_DATA;
			ret = ipsec_check_global_policy(mp, ipc);
			mp->b_datap->db_type = db_type;
			return (ret);
		}
	}

	/* Could be a IPSEC_OUT or IPSEC_IN message */
	ASSERT(ipsec_mp->b_datap->db_type == M_CTL);

	/*
	 * ipsec_mp->b_cont could be either a M_CTL message
	 * for icmp errors being sent up or a M_DATA message.
	 */
	ASSERT(ipsec_mp->b_cont->b_datap->db_type == M_CTL ||
	    ipsec_mp->b_cont->b_datap->db_type == M_DATA);

	if (outbound) {
		ASSERT(((ipsec_out_t *)ipsec_mp->b_rptr)->
		    ipsec_out_type == IPSEC_OUT);
		ASSERT(ipc == NULL);
		return (ipsec_check_ipsecout_policy(ipsec_mp));
	} else {
		if (!ipc->ipc_policy_cached) {
			/*
			 * We don't have policies cached in the ipc's
			 * for these connections. So, look at the global
			 * policy. It will check against ipc or global
			 * depending on whichever is stronger.
			 */
			return (ipsec_check_global_policy(ipsec_mp, ipc));
		} else {
			return (ipsec_check_ipsecin_policy(NULL, ipc->ipc_wq,
			    ipsec_mp, ipc->ipc_inbound_policy));
		}
	}
}

/*
 * Write side put procedure.  Outbound data, IOCTLs, responses from
 * resolvers, etc, come down through here.
 */
void
ip_wput(queue_t *q, mblk_t *mp)
{
	ipha_t	*ipha;
#define	rptr	((uchar_t *)ipha)
	ire_t	*ire = NULL;
	ipc_t	*ipc;
	uint32_t	v_hlen_tos_len;
	ipaddr_t	dst;
	mblk_t *first_mp = NULL;
	boolean_t secure;
	ipsec_out_t *io;

#ifdef	_BIG_ENDIAN
#define	V_HLEN	(v_hlen_tos_len >> 24)
#else
#define	V_HLEN	(v_hlen_tos_len & 0xFF)
#endif

	TRACE_1(TR_FAC_IP, TR_IP_WPUT_START,
	    "ip_wput_start: q %p", q);

	/*
	 * ip_wput fast path
	 */

	/* is packet from ARP ? */
	if (q->q_next)
		goto qnext;
	ipc = (ipc_t *)q->q_ptr;

	/* is queue flow controlled? */
	if (q->q_first && !ipc->ipc_draining)
		goto doputq;

	if (mp->b_datap->db_type != M_DATA)
		goto notdata;
	ipha = (ipha_t *)mp->b_rptr;

	/* is IP header non-aligned or mblk smaller than basic IP header */
#ifndef SAFETY_BEFORE_SPEED
	if (!OK_32PTR(rptr) ||
	    (mp->b_wptr - rptr) < IP_SIMPLE_HDR_LENGTH)
		goto hdrtoosmall;
#endif

	/*
	 * If there is a policy, try to attach an ipsec_out in
	 * the front. At the end, first_mp either points to a
	 * M_DATA message or IPSEC_OUT message linked to a
	 * M_DATA message. We have to do it now as we might
	 * lose the "ipc" if we go through ip_newroute.
	 */

	if (ipc->ipc_out_enforce_policy) {
		if (((mp = ipsec_attach_ipsec_out(mp, ipc, NULL,
		    ipha->ipha_protocol)) == NULL)) {
			return;
		} else {
			ASSERT(mp->b_datap->db_type == M_CTL);
			first_mp = mp;
			mp = mp->b_cont;
			secure = B_TRUE;
		}
	} else {
		/*
		 * XXX What if somebody added global policy which can affect
		 * this datagram. ip_wput_ire will apply policy and our peer
		 * would drop this as the other side may still be expecting
		 * in clear. Should we fix this at least for hard_bound
		 * connections where any global policy change should not
		 * affect the connection after it became hard_bound ?
		 */
		first_mp = mp;
		secure = B_FALSE;
	}

	v_hlen_tos_len = ((uint32_t *)ipha)[0];

	/* is wrong version or IP options present */
	if (V_HLEN != IP_SIMPLE_HDR_VERSION)
		goto version_hdrlen_check;
	dst = ipha->ipha_dst;

	/* is packet multicast? */
	if (CLASSD(dst))
		goto multicast;

	/* bypass routing checks and go directly to interface */
	if (ipc->ipc_dontroute)
		goto dontroute;

	/*
	 * We cache IRE_CACHEs to avoid lookups. We don't do
	 * this for the tcp global queue and listen end point
	 * as it does not really have a real destination to
	 * talk to.
	 */
	if (ipc->ipc_ulp == IPPROTO_TCP && !ipc->ipc_fully_bound) {
		ire = ire_cache_lookup(dst);
		if (ire == NULL)
			goto noirefound;
		TRACE_2(TR_FAC_IP, TR_IP_WPUT_END,
		    "ip_wput_end: q %p (%S)", q, "end");
		ip_wput_ire(q, first_mp, ire, ipc);
		return;
	}

	/*
	 * IRE_MARK_CONDEMNED is marked in ire_delete. We don't grab
	 * a lock here to check for CONDEMNED as it is okay to
	 * send a packet or two with the IRE_CACHE that is going away.
	 */
	mutex_enter(&ipc->ipc_irc_lock);
	ire = ipc->ipc_ire_cache;
	if (ire != NULL && ire->ire_addr == dst &&
	    !(ire->ire_marks & IRE_MARK_CONDEMNED)) {

		IRE_REFHOLD(ire);
		mutex_exit(&ipc->ipc_irc_lock);

	} else {
		ipc->ipc_ire_cache = NULL;
		mutex_exit(&ipc->ipc_irc_lock);
		if (ire != NULL)
			IRE_REFRELE(ire);
		ire = (ire_t *)ire_cache_lookup(dst);
		if (ire == NULL)
			goto noirefound;
		mutex_enter(&ipc->ipc_irc_lock);
		if (ipc->ipc_ire_cache == NULL) {
			ipc->ipc_ire_cache = ire;
			IRE_REFHOLD(ire);
		}
		mutex_exit(&ipc->ipc_irc_lock);
	}

	TRACE_2(TR_FAC_IP, TR_IP_WPUT_END,
	    "ip_wput_end: q %p (%S)", q, "end");

	ip_wput_ire(q, first_mp, ire, ipc);
	return;

doputq:
	(void) putq(q, mp);
	return;

qnext:
	ipc = NULL;

	/*
	 * Upper Level Protocols pass down complete IP datagrams
	 * as M_DATA messages.	Everything else is a sideshow.
	 *
	 * 1) We could be re-entering ip_wput because of ip_neworute
	 *    in which case we could have a IPSEC_OUT message. We
	 *    need to pass through ip_wput like other datagrams and
	 *    hence cannot branch to ip_wput_nondata.
	 *
	 * 2) ARP, AH, ESP, and other clients who are on the module
	 *    instance of IP stream, give us something to deal with.
	 *    We will handle AH and ESP here and rest in ip_wput_nondata.
	 *
	 * 3) ICMP replies also could come here.
	 */
	if (mp->b_datap->db_type != M_DATA) {
	    notdata:
		if (mp->b_datap->db_type == M_CTL) {
			/*
			 * M_CTL messages are used by ARP, AH and ESP to
			 * communicate with IP. We deal with IPSEC_IN and
			 * IPSEC_OUT here. ip_wput_nondata handles other
			 * cases.
			 */
			ipsec_info_t *ii = (ipsec_info_t *)mp->b_rptr;
			if (ii->ipsec_info_type == IPSEC_IN) {
				/*
				 * Either this message goes back to
				 * IPSEC for further processing or to
				 * ULP after policy checks.
				 */
				ip_fanout_proto_again(q, mp);
				return;
			} else if (ii->ipsec_info_type == IPSEC_OUT) {
				io = (ipsec_out_t *)ii;
				if (io->ipsec_out_proc_begin) {
					/*
					 * IPSEC processing has already started.
					 * Complete it.
					 */
					ipsec_out_process(q, mp);
					return;
				} else {
					ipc = (q->q_next ? NULL : q->q_ptr);
					first_mp = mp;
					mp = mp->b_cont;
					secure = B_TRUE;
				}
			} else {
				/*
				 * This must be ARP.
				 */
				ip_wput_nondata(q, mp);
				TRACE_2(TR_FAC_IP, TR_IP_WPUT_END,
				    "ip_wput_end: q %p (%S)", q, "nondata");
				return;
			}
		} else {
			/*
			 * This must be non-(ARP/AH/ESP) messages.
			 */
			ip_wput_nondata(q, mp);
			TRACE_2(TR_FAC_IP, TR_IP_WPUT_END,
			    "ip_wput_end: q %p (%S)", q, "nondata");
			return;
		}
	} else {
		first_mp = mp;
		secure = B_FALSE;
	}

	ASSERT(first_mp != NULL);

	/* We have a complete IP datagram heading outbound. */
	ipha = (ipha_t *)mp->b_rptr;

#ifndef SPEED_BEFORE_SAFETY
	/*
	 * Make sure we have a full-word aligned message and that at least
	 * a simple IP header is accessible in the first message.  If not,
	 * try a pullup.
	 */
	if (!OK_32PTR(rptr) ||
	    (mp->b_wptr - rptr) < IP_SIMPLE_HDR_LENGTH) {
	    hdrtoosmall:
		if (!pullupmsg(mp, IP_SIMPLE_HDR_LENGTH)) {
			BUMP_MIB(ip_mib.ipOutDiscards);
			TRACE_2(TR_FAC_IP, TR_IP_WPUT_END,
			    "ip_wput_end: q %p (%S)", q, "pullupfailed");
			goto drop_pkt;
		}
		ipha = (ipha_t *)mp->b_rptr;
		if (first_mp == NULL) {
			/*
			 * If we got here because of "goto hdrtoosmall"
			 * We need to attach a IPSEC_OUT.
			 */
			if (ipc->ipc_out_enforce_policy) {
				if (((mp = ipsec_attach_ipsec_out(mp, ipc,
				    NULL, ipha->ipha_protocol)) == NULL)) {
					return;
				} else {
					ASSERT(mp->b_datap->db_type == M_CTL);
					first_mp = mp;
					mp = mp->b_cont;
					secure = B_TRUE;
				}
			} else {
				first_mp = mp;
				secure = B_FALSE;
			}
		}
	}
#endif

	/* Most of the code below is written for speed, not readability */
	v_hlen_tos_len = ((uint32_t *)ipha)[0];

	/*
	 * If ip_newroute() fails, we're going to need a full
	 * header for the icmp wraparound.
	 */
	if (V_HLEN != IP_SIMPLE_HDR_VERSION) {
		uint_t	v_hlen;
	    version_hdrlen_check:
		ASSERT(first_mp != NULL);
		v_hlen = V_HLEN;
		/*
		 * siphon off IPv6 packets coming down from transport
		 * layer modules here.
		 * Note: high-order bit carries NUD reachability confirmation
		 */
		if (((v_hlen >> 4) & 0x7) == IPV6_VERSION) {
			/*
			 * XXX implement a IPv4 and IPv6 packet counter per
			 * ipc and switch when ratio exceeds e.g. 10:1
			 */
#ifdef notyet
			if (q->q_next == NULL) /* Avoid ill queue */
				ip_setqinfo(RD(q), B_TRUE, B_TRUE);
#endif
			BUMP_MIB(ip_mib.ipOutIPv6);
			ip_wput_v6(q, mp);
			return;
		}

		if ((v_hlen >> 4) != IP_VERSION) {
			BUMP_MIB(ip_mib.ipOutDiscards);
			TRACE_2(TR_FAC_IP, TR_IP_WPUT_END,
			    "ip_wput_end: q %p (%S)", q, "badvers");
			goto drop_pkt;
		}
		/*
		 * Is the header length at least 20 bytes?
		 *
		 * Are there enough bytes accessible in the header?  If
		 * not, try a pullup.
		 */
		v_hlen &= 0xF;
		v_hlen <<= 2;
		if (v_hlen < IP_SIMPLE_HDR_LENGTH) {
			BUMP_MIB(ip_mib.ipOutDiscards);
			TRACE_2(TR_FAC_IP, TR_IP_WPUT_END,
			    "ip_wput_end: q %p (%S)", q, "badlen");
			goto drop_pkt;
		}
		if (v_hlen > (mp->b_wptr - rptr)) {
			if (!pullupmsg(mp, v_hlen)) {
				BUMP_MIB(ip_mib.ipOutDiscards);
				TRACE_2(TR_FAC_IP, TR_IP_WPUT_END,
				    "ip_wput_end: q %p (%S)", q, "badpullup2");
				goto drop_pkt;
			}
			ipha = (ipha_t *)mp->b_rptr;
		}
		/*
		 * Move first entry from any source route into ipha_dst and
		 * verify the options
		 */
		if (ip_wput_options(q, first_mp, ipha, secure)) {
			TRACE_2(TR_FAC_IP, TR_IP_WPUT_END,
			    "ip_wput_end: q %p (%S)", q, "badopts");
			return;
		}
	}
	dst = ipha->ipha_dst;

	/*
	 * Try to get an IRE_CACHE for the destination address.	 If we can't,
	 * we have to run the packet through ip_newroute which will take
	 * the appropriate action to arrange for an IRE_CACHE, such as querying
	 * a resolver, or assigning a default gateway, etc.
	 */
	if (CLASSD(dst)) {
		ipif_t	*ipif;

	    multicast:
		ASSERT(first_mp != NULL);
		ip2dbg(("ip_wput: CLASSD\n"));
		if (ipc == NULL) {
			/*
			 * Use the first ipif on the ill.
			 * XXX Should this ever happen? (Appears
			 * to show up with just ppp and no ethernet due
			 * to in.rdisc.)
			 * However, ire_send should be able to
			 * call ip_wput_ire directly.
			 *
			 * XXX Also, this can happen for ICMP and other packets
			 * with multicast source addresses.  Perhaps we should
			 * fix things so that we drop the packet in question,
			 * but for now, just run with it.
			 */
			ill_t *ill = (ill_t *)q->q_ptr;

			ipif = ill->ill_ipif;
			ASSERT(ipif);
			ASSERT(ipif->ipif_flags & IFF_UP);
			ip1dbg(("ip_wput: CLASSD no IPC: dst 0x%x on %s\n",
			    ntohl(dst), ill->ill_name));
		} else {
			ipif = ipc->ipc_multicast_ipif;
			if (ipif == NULL || ipif->ipif_isv6) {
				/*
				 * We must do this ipif determination here
				 * else we could pass through ip_newroute
				 * and come back here without the ipc context.
				 *
				 * Note: we do late binding i.e. we bind to
				 * the interface when the first packet is sent.
				 * For performance reasons we do not rebind on
				 * each packet but keep the binding until the
				 * next IP_MULTICAST_IF option.
				 *
				 * ipc_multicast_{ipif,ill} are shared between
				 * IPv4 and IPv6 and AF_INET6 sockets can
				 * send both IPv4 and IPv6 packets. Hence
				 * we have to check that "isv6" matches above.
				 */
				ipif = ipif_lookup_group(
				    htonl(INADDR_UNSPEC_GROUP));
				if (ipif == NULL) {
					ip1dbg(("ip_wput: No ipif for "
					    "multicast\n"));
					BUMP_MIB(ip_mib.ipOutNoRoutes);
					goto drop_pkt;
				}
				ipc->ipc_multicast_ipif = ipif;
				ipc->ipc_multicast_ill = ipif->ipif_ill;
			}
		}
		ASSERT(!ipif->ipif_isv6);
		/*
		 * As we may lose the ipc by the time we reach ip_wput_ire,
		 * we copy ipc_multicast_loop and ipc_dontroute on to an
		 * ipsec_out. In case if this datagram goes out secure,
		 * we need the ill_index also. Copy that also into the
		 * ipsec_out.
		 */
		if (secure) {
			io = (ipsec_out_t *)first_mp->b_rptr;
			ASSERT(first_mp->b_datap->db_type == M_CTL);
			ASSERT(io->ipsec_out_type == IPSEC_OUT);
		} else {
			ASSERT(mp == first_mp);
			if ((first_mp = allocb(sizeof (ipsec_info_t),
			    BPRI_HI)) == NULL) {
				first_mp = mp;
				goto drop_pkt;
			}
			first_mp->b_datap->db_type = M_CTL;
			first_mp->b_wptr += sizeof (ipsec_info_t);
			/* ipsec_out_secure is B_FALSE now */
			bzero(first_mp->b_rptr, sizeof (ipsec_info_t));
			io = (ipsec_out_t *)first_mp->b_rptr;
			io->ipsec_out_type = IPSEC_OUT;
			io->ipsec_out_len = sizeof (ipsec_out_t);
			io->ipsec_out_use_global_policy = B_TRUE;
			first_mp->b_cont = mp;
			secure = B_TRUE;
		}
		io->ipsec_out_ill_index = ipif->ipif_ill->ill_index;
		if (ipc != NULL) {
			io->ipsec_out_multicast_loop = ipc->ipc_multicast_loop;
			io->ipsec_out_dontroute = ipc->ipc_dontroute;
		}
		/*
		 * Currently we do the ire lookup based on the ipif.
		 * If the application uses IP_MULTICAST_IF with
		 * different logical addresses of the same ILL, we
		 * create multiple IRE_CACHE entries as the lookup
		 * is based on ipif. Thus when we return from IPSEC
		 * processing, we need to use the right ipif to do
		 * the lookup so that the packet gets the right source
		 * address. If we can assign the source address here,
		 * it would be sufficient to match on the ILL when we
		 * come back from IPSEC processing.
		 *
		 * NOTE : We need to do it for non-secure case also as
		 * this might go out secure if there is a global policy
		 * match in ip_wput_ire.
		 */
		if (!ipha->ipha_src) {
			ipha->ipha_src = ipif->ipif_src_addr;
		}
		/*
		 * Find an IRE which matches the destination and the outgoing
		 * queue (i.e. the outgoing interface.)
		 */
		if (ipif->ipif_flags & IFF_POINTOPOINT)
			dst = ipif->ipif_pp_dst_addr;
		ire = ire_ctable_lookup(dst, 0, 0, ipif, NULL, MATCH_IRE_IPIF);
		if (!ire) {
			/*
			 * Multicast loopback and multicast forwarding is
			 * done in ip_wput_ire.
			 *
			 * Mark this packet to make it be delivered to
			 * ip_wput_ire after the new ire has been
			 * created.
			 */
			mp->b_prev = mp->b_next = NULL;
			ip_newroute_ipif(q, first_mp, ipif, dst, ipc);
			TRACE_2(TR_FAC_IP, TR_IP_WPUT_END,
			    "ip_wput_end: q %p (%S)", q, "noire");
			return;
		}
	} else {
		/*
		 * Guard against coming in from arp in which case ipc is
		 * NULL.
		 */
		if (ipc && ipc->ipc_dontroute) {
		    dontroute:
			/*
			 * Set TTL to 1 if SO_DONTROUTE is set to prevent
			 * routing protocols from seeing false direct
			 * connectivity.
			 */
			ipha->ipha_ttl = 1;
		}
		ire = ire_cache_lookup(dst);
		if (!ire) {
noirefound:
			/*
			 * Mark this packet as having originated on
			 * this machine.  This will be noted in
			 * ire_add_then_send, which needs to know
			 * whether to run it back through ip_wput or
			 * ip_rput following successful resolution.
			 */
			mp->b_prev = NULL;
			mp->b_next = NULL;
			ip_newroute(q, first_mp, dst, ipc);
			TRACE_2(TR_FAC_IP, TR_IP_WPUT_END,
			    "ip_wput_end: q %p (%S)", q, "newroute");
			return;
		}
	}

	/* We now know where we are going with it. */

	TRACE_2(TR_FAC_IP, TR_IP_WPUT_END,
	    "ip_wput_end: q %p (%S)", q, "end");
	ip_wput_ire(q, first_mp, ire, ipc);
	return;

drop_pkt:
	ip1dbg(("ip_wput: dropped packet\n"));
	if (ire != NULL)
		ire_refrele(ire);
	freemsg(first_mp);
	TRACE_2(TR_FAC_IP, TR_IP_WPUT_END,
	    "ip_wput_end: q %p (%S)", q, "droppkt");
}

/*
 * This is called if the outbound datagram needs fragmentation.
 *
 * NOTE : This function does not ire_refrele the ire argument passed in.
 */
static void
ip_wput_ire_fragmentit(mblk_t *ipsec_mp, ire_t *ire)
{
	ipha_t		*ipha;
	mblk_t		*mp;
	uint32_t	v_hlen_tos_len;
	uint32_t	max_frag;
	uint32_t	frag_flag;
	boolean_t	dont_use;

	if (ipsec_mp->b_datap->db_type == M_CTL) {
		mp = ipsec_mp->b_cont;
	} else {
		mp = ipsec_mp;
	}

	ipha = (ipha_t *)mp->b_rptr;
	v_hlen_tos_len = ((uint32_t *)ipha)[0];

#ifdef	_BIG_ENDIAN
#define	V_HLEN	(v_hlen_tos_len >> 24)
#define	LENGTH	(v_hlen_tos_len & 0xFFFF)
#else
#define	V_HLEN	(v_hlen_tos_len & 0xFF)
#define	LENGTH	((v_hlen_tos_len >> 24) | ((v_hlen_tos_len >> 8) & 0xFF00))
#endif

#ifndef SPEED_BEFORE_SAFETY
	/*
	 * Check that ipha_length is consistent with
	 * the mblk length
	 */
	if (LENGTH != (mp->b_cont ? msgdsize(mp) : mp->b_wptr - rptr)) {
		ip0dbg(("Packet length mismatch: %d, %ld\n",
		    LENGTH, msgdsize(mp)));
		freemsg(ipsec_mp);
		TRACE_2(TR_FAC_IP, TR_IP_WPUT_IRE_END,
		    "ip_wput_ire_fragmentit: mp %p (%S)", mp,
		    "packet length mismatch");
		return;
	}
#endif
	/*
	 * Don't use frag_flag if source routed or if
	 * multicast (since multicast packets do not solicit
	 * ICMP "packet too big" messages). Get the values of
	 * max_frag and frag_flag atomically by acquiring the
	 * ire_lock.
	 */
	mutex_enter(&ire->ire_lock);
	max_frag = ire->ire_max_frag;
	frag_flag = ire->ire_frag_flag;
	mutex_exit(&ire->ire_lock);

	dont_use = ((V_HLEN != IP_SIMPLE_HDR_VERSION &&
	    ip_source_route_included(ipha)) || CLASSD(ipha->ipha_dst));

	ip_wput_frag(ire, ipsec_mp, &ire->ire_ob_pkt_count, max_frag,
	    (dont_use ? 0 : frag_flag));
}

/*
 * Used for deciding the MSS size for the upper layer. Thus
 * we need to check the outbound policy values in the ipc.
 */
int
ipc_ipsec_length(ipc_t *ipc)
{
	int length = 0;

	if (ipc->ipc_outbound_policy == NULL)
		return (0);

	if (ipc->ipc_out_ah_req & IPSEC_PREF_REQUIRED)
		length += IPSEC_MAX_AH_HDR_SIZE;

	if (ipc->ipc_out_esp_req & IPSEC_PREF_REQUIRED)
		length += IPSEC_MAX_ESP_HDR_SIZE;

	if (ipc->ipc_out_self_encap_req & IPSEC_PREF_REQUIRED)
		length += IP_SIMPLE_HDR_LENGTH;

	return (length);
}

/*
 * Returns an estimate of the IPSEC headers size. This is used if
 * we don't want to call into IPSEC to get the exact size.
 */
static int
ipsec_extra_length(mblk_t *ipsec_mp)
{
	int length = 0;
	ipsec_info_t *in;

	in = (ipsec_info_t *)ipsec_mp->b_rptr;

	if (in->ipsec_info_type == IPSEC_IN) {
		ipsec_in_t *ii = (ipsec_in_t *)in;

		if (ii->ipsec_in_loopback) {

			if (ii->ipsec_in_ah_done & IPSEC_PREF_REQUIRED)
				length += IPSEC_MAX_AH_HDR_SIZE;

			if (ii->ipsec_in_esp_done & IPSEC_PREF_REQUIRED)
				length += IPSEC_MAX_ESP_HDR_SIZE;

			if (ii->ipsec_in_self_encap_done & IPSEC_PREF_REQUIRED)
				length += IP_SIMPLE_HDR_LENGTH;
		} else {
			if (ii->ipsec_in_ah_spi != 0)
				length += IPSEC_MAX_AH_HDR_SIZE;

			if (ii->ipsec_in_esp_spi != 0)
				length += IPSEC_MAX_ESP_HDR_SIZE;

			if (ii->ipsec_in_decaps)
				length += IP_SIMPLE_HDR_LENGTH;
		}
	} else {
		ipsec_out_t *io = (ipsec_out_t *)in;

		ASSERT(io->ipsec_out_type == IPSEC_OUT);

		if (io->ipsec_out_ah_req & IPSEC_PREF_REQUIRED)
			length += IPSEC_MAX_AH_HDR_SIZE;

		if (io->ipsec_out_esp_req & IPSEC_PREF_REQUIRED)
			length += IPSEC_MAX_ESP_HDR_SIZE;

		if (io->ipsec_out_self_encap_req & IPSEC_PREF_REQUIRED)
			length += IP_SIMPLE_HDR_LENGTH;
	}
	return (length);
}

/*
 * If there are any source route options, return the true final
 * destination. Otherwise, return the destination.
 */
ipaddr_t
ip_get_dst(ipha_t *ipha)
{
	uint32_t	option_length;
	uchar_t		*opt;
	uint32_t	optval;
	uint32_t	optlen;
	ipaddr_t	dst;
	uint32_t off;

	dst = ipha->ipha_dst;
	option_length = ipha->ipha_version_and_hdr_length -
		(uint8_t)((IP_VERSION << 4) + IP_SIMPLE_HDR_LENGTH_IN_WORDS);
	opt = (uchar_t *)&ipha[1];
	option_length <<= 2;

	while (option_length != 0) {
		switch (optval = opt[IPOPT_POS_VAL]) {
		case IPOPT_EOL:
			return (dst);
		case IPOPT_NOP:
			optlen = 1;
			break;
		default:
			optlen = opt[IPOPT_POS_LEN];
		}

		if (optlen == 0 || optlen > option_length)
			return (dst);

		switch (optval) {
		case IPOPT_SSRR:
		case IPOPT_LSRR:
			off = opt[IPOPT_POS_OFF];
			/*
			 * If one of the conditions is true, it means
			 * end of options and dst already has the right
			 * value.
			 */
			if (!(optlen < IP_ADDR_LEN || off > optlen - 3)) {
				off = optlen - IP_ADDR_LEN;
				bcopy(&opt[off], &dst, IP_ADDR_LEN);
			}
			return (dst);
		default:
			break;
		}
		option_length -= optlen;
		opt += optlen;
	}
	return (dst);
}

/*
 * Consults global policy to see whether this datagram should
 * go out secure. If so it attaches a ipsec_mp in front and
 * returns.
 */
static mblk_t *
ip_wput_attach_policy(mblk_t *ipsec_mp, ire_t *ire)
{
	ipha_t *ipha;
	mblk_t *mp;
	ipsec_out_t *io = NULL;
	ipsec_selector_t sel;
	uint_t	ill_index;
	boolean_t ipc_dontroute;
	boolean_t ipc_multicast_loop;
	ipsec_req_t isr;
	boolean_t inherited = B_FALSE;
	uint_t req_mask;

	/*
	 * Fast Path to see if there is any policy.
	 */
	if (ipsec_policy_head[IPSEC_TYPE_OUTBOUND].ipsec_policy_next == NULL) {
		if (ipsec_mp->b_datap->db_type == M_CTL) {
			io = (ipsec_out_t *)ipsec_mp->b_rptr;
			if (!io->ipsec_out_secure) {
				/*
				 * If there is no global policy and ip_wput
				 * or ip_wput_multicast has attached this mp
				 * for multicast case, free the ipsec_mp and
				 * return the original mp.
				 */
				mp = ipsec_mp->b_cont;
				freeb(ipsec_mp);
				ipsec_mp = mp;
			}
		}
		return (ipsec_mp);
	}

	ill_index = 0;
	ipc_multicast_loop = ipc_dontroute = B_FALSE;
	mp = ipsec_mp;
	if (ipsec_mp->b_datap->db_type == M_CTL) {
		/*
		 * This is a connection where we have some per-socket
		 * policy or ip_wput has attached an ipsec_mp for
		 * the multicast datagram.
		 */
		io = (ipsec_out_t *)ipsec_mp->b_rptr;
		if (!io->ipsec_out_secure) {
			/*
			 * This ipsec_mp was allocated in ip_wput or
			 * ip_wput_multicast so that we will know the
			 * value of ill_index, ipc_dontroute,
			 * ipc_multicast_loop in the multicast case if
			 * we inherit global policy here.
			 */
			ill_index = io->ipsec_out_ill_index;
			ipc_dontroute = io->ipsec_out_dontroute;
			ipc_multicast_loop = io->ipsec_out_multicast_loop;
			mp = ipsec_mp->b_cont;
			freeb(ipsec_mp);
			ipsec_mp = mp;
			io = NULL;
		} else {
			mp = ipsec_mp->b_cont;
		}
	}

	ipha = (ipha_t *)mp->b_rptr;
	sel.src_addr = (ipha->ipha_src != 0 ?
	    ipha->ipha_src : ire->ire_src_addr);
	sel.dst_addr = ip_get_dst(ipha);
	sel.src_mask = sel.dst_mask = 0;
	sel.outbound = IPSEC_OUTBOUND;
	sel.protocol = (uint8_t)ipha->ipha_protocol;

	ipsec_init_ports(&sel, mp);

	if (io != NULL) {
		/*
		 * We seem to have some local policy. Look at global
		 * policy and see whether we have to inherit or not.
		 */
		isr.ipsr_ah_req = io->ipsec_out_ah_req;
		isr.ipsr_esp_req = io->ipsec_out_esp_req;
		isr.ipsr_self_encap_req = io->ipsec_out_self_encap_req;
		isr.ipsr_auth_alg = io->ipsec_out_ah_alg;
		isr.ipsr_esp_alg = io->ipsec_out_esp_alg;
		isr.ipsr_esp_auth_alg = io->ipsec_out_esp_ah_alg;

		(void) ipsec_inherit_global_policy(NULL, &isr, &sel);

		io->ipsec_out_ah_req = isr.ipsr_ah_req;
		io->ipsec_out_esp_req = isr.ipsr_esp_req;
		io->ipsec_out_self_encap_req = isr.ipsr_self_encap_req;
		io->ipsec_out_ah_alg = isr.ipsr_auth_alg;
		io->ipsec_out_esp_alg = isr.ipsr_esp_alg;
		io->ipsec_out_esp_ah_alg = isr.ipsr_esp_auth_alg;
		req_mask = (IPSEC_PREF_REQUIRED | IPSEC_PREF_NEVER);
		if (!(io->ipsec_out_ah_req & req_mask) &&
		    !(io->ipsec_out_esp_req & req_mask) &&
		    !(io->ipsec_out_self_encap_req & req_mask)) {
			/*
			 * This happens if there is a bypass.
			 */
			freeb(ipsec_mp);
			return (mp);
		}
		return (ipsec_mp);
	} else {
		bzero(&isr, sizeof (ipsec_req_t));
		inherited = ipsec_inherit_global_policy(NULL, &isr, &sel);
	}

	if (inherited) {
		ipsec_mp = ipsec_attach_ipsec_out(mp, NULL, &isr,
		    sel.protocol);
		/*
		 * Copy the right port information.
		 */
		if (ipsec_mp != NULL) {
			ASSERT(ipsec_mp->b_datap->db_type == M_CTL);
			io = (ipsec_out_t *)ipsec_mp->b_rptr;
			io->ipsec_out_src_port = sel.src_port;
			io->ipsec_out_dst_port = sel.dst_port;
			/*
			 * Set ill_index, ipc_dontroute and ipc_multicast_loop
			 * for multicast datagrams.
			 */
			io->ipsec_out_ill_index = ill_index;
			io->ipsec_out_dontroute = ipc_dontroute;
			io->ipsec_out_multicast_loop = ipc_multicast_loop;
		}
	}
	return (ipsec_mp);
}

/*
 * This function does the ire_refrele of the ire passed in as the
 * argument. As this function looks up more ires i.e broadcast ires,
 * it needs to REFRELE them. Currently, for simplicity we don't
 * differentiate the one passed in and looked up here. We always
 * REFRELE.
 */
void
ip_wput_ire(queue_t *q, mblk_t *mp, ire_t *ire, ipc_t *ipc)
{
	ipha_t	*ipha;
#define	rptr	((uchar_t *)ipha)
	mblk_t	*mp1;
	queue_t	*stq;
	uint32_t	v_hlen_tos_len;
	uint32_t	ttl_protocol;
	ipaddr_t	src;
	ipaddr_t	dst;
	uint32_t	cksum;
	ipaddr_t	orig_src;
	ire_t	*ire1;
	mblk_t	*next_mp;
	uint_t	hlen;
	uint16_t	*up;
	uint32_t	max_frag;
	ill_t	*ill = ire_to_ill(ire);
	int	clusterwide;
	int	no_tp_cksum;	/* Perform transport level checksum? */
	int	extra_len;
	boolean_t mctl_present;
	mblk_t *first_mp;
	ipsec_out_t *io;
	boolean_t ipc_dontroute;	/* ipc value for multicast */
	boolean_t ipc_multicast_loop;	/* ipc value for multicast */
	boolean_t multicast_forward;	/* Should we forward ? */

	TRACE_1(TR_FAC_IP, TR_IP_WPUT_IRE_START,
	    "ip_wput_ire_start: q %p", q);

	first_mp = mp;
	extra_len = 0;
	multicast_forward = B_FALSE;
	if (mp->b_datap->db_type == M_CTL) {
		io = (ipsec_out_t *)mp->b_rptr;
		ASSERT(io->ipsec_out_type == IPSEC_OUT);
		ipha = (ipha_t *)mp->b_cont->b_rptr;
		dst = ipha->ipha_dst;
		/*
		 * For the multicast case, ipsec_out carries ipc_dontroute
		 * and ipc_multicast_loop as ipc may not be available here.
		 * We need this for multicast loopback and forwarding which
		 * is done later in the code.
		 */
		if (CLASSD(dst)) {
			ipc_dontroute = io->ipsec_out_dontroute;
			ipc_multicast_loop = io->ipsec_out_multicast_loop;
			/*
			 * If ipc_dontroute is not set or ipc_multicast_loop
			 * is set, we need to do forwarding/loopback.
			 * For datagrams from ip_wput_multicast, ipc_dontroute
			 * is set to B_TRUE and ipc_multicast_loop is set to
			 * B_FALSE so that we neither do forwarding nor
			 * loopback.
			 */
			if (!ipc_dontroute || ipc_multicast_loop)
				multicast_forward = B_TRUE;
		}
		/*
		 * ip_wput attaches an IPSEC_OUT in two cases.
		 *
		 * 1) There is per-socket policy.
		 * 2) There is no per-socket policy, but it is
		 *    a multicast packet that needs to go out
		 *    on a specific interface. This is the case
		 *    where (ip_wput and ip_wput_multicast) attaches
		 *    an IPSEC_OUT and sets ipsec_out_secure B_FALSE.
		 *
		 * In case (2) we check with global policy to
		 * see if there is a match and set the ill_index
		 * appropriately so that we can lookup the ire
		 * properly in ip_wput_ipsec_out.
		 */
		if (!io->ipsec_out_policy_cached) {
			/*
			 * ipsec_out_use_global_policy is set to B_FALSE
			 * in ipsec_in_to_out(). Refer to that function for
			 * details.
			 */
			if (io->ipsec_out_use_global_policy) {
				mp = ip_wput_attach_policy(first_mp, ire);
				if (mp == NULL) {
					ire_refrele(ire);
					return;
				}
			} else if (!io->ipsec_out_secure) {
				/*
				 * If this is not a secure packet, drop
				 * the IPSEC_OUT mp and treat it as a clear
				 * packet. This happens when we are sending
				 * a ICMP reply back to a clear packet. See
				 * ipsec_in_to_out() for details.
				 */
				mp = first_mp->b_cont;
				freeb(first_mp);
			}
		}
		if (mp->b_datap->db_type == M_CTL) {
			io = (ipsec_out_t *)mp->b_rptr;
			ASSERT(io->ipsec_out_type == IPSEC_OUT);
			first_mp = mp;
			mp = mp->b_cont;
			extra_len = ipsec_extra_length(first_mp);
			mctl_present = B_TRUE;
		} else {
			/*
			 * This happens for case(2) above when
			 * there is no global policy match or if there
			 * was bypass in the global policy.
			 */
			first_mp = mp;
			mctl_present = B_FALSE;
		}
	} else {
		/*
		 * See whether we need to attach a global policy
		 * here. We don't depend on the ipc (as it could be null)
		 * for deciding what policy this datagram should go through
		 * because it should have happened in ip_wput
		 * if there was some policy. This normally happens for
		 * connections which are not fully bound preventing
		 * us from caching policies in ip_bind. Packets coming
		 * from the TCP listener/global queue - which are
		 * non-hard_bound - could also be affected by applying
		 * policy here.
		 *
		 * If this packet is coming from tcp global
		 * queue or listener, we will be applying policy here.
		 * This may not be *right* if these packets are coming from
		 * the detached connection as it could have gone in clear
		 * before. This happens only if a TCP connection started
		 * when there is no policy and somebody added policy
		 * before it became detached. Thus packets of the detached
		 * connection could go out secure and the other end would
		 * drop it because it will be expecting in clear. The
		 * converse is not true i.e if somebody starts a TCP
		 * connection and deletes the policy, all the packets will
		 * still go out with the policy that existed before deleting
		 * because ip_unbind sends up policy information which is used
		 * by TCP on subsequent ip_wputs. The right solution is to fix
		 * TCP to attach a dummy IPSEC_OUT and set
		 * ipsec_out_use_global_policy to B_FALSE. As this might
		 * affect performance for normal cases, we are not doing it.
		 * Thus, set policy before starting any TCP connections.
		 *
		 * XXX We might apply policy even for a hard bound connection
		 * - for which we cached policy in ip_bind - if somebody added
		 * global policy after we inherited the policy in ip_bind.
		 * This means that the packets that were going out in clear
		 * previously would start going secure and hence get dropped
		 * on the other side. To fix this, ip_wput needs to attach a
		 * dummy ipsec_out and make sure that we don't apply global
		 * policy.
		 */
		ipsec_policy_t *head;

		head = &ipsec_policy_head[IPSEC_TYPE_OUTBOUND];
		if (head->ipsec_policy_next != NULL) {
			mp = ip_wput_attach_policy(mp, ire);
			if (mp == NULL) {
				ire_refrele(ire);
				return;
			}
			if (mp->b_datap->db_type == M_CTL) {
				first_mp = mp;
				mp = mp->b_cont;
				extra_len = ipsec_extra_length(first_mp);
				mctl_present = B_TRUE;
			} else {
				ASSERT(first_mp == mp);
				mctl_present = B_FALSE;
			}
		} else {
			first_mp = mp;
			mctl_present = B_FALSE;
		}
	}

	/*
	 * Fast path for ip_wput_ire
	 */

	ipha = (ipha_t *)mp->b_rptr;
	v_hlen_tos_len = ((uint32_t *)ipha)[0];
	dst = ipha->ipha_dst;

	/*
	 * ICMP(RAWIP) module should set the ipha_ident to NO_IP_TP_CKSUM
	 * if the socket is a SOCK_RAW type. The transport checksum should
	 * be provided in the pre-built packet, so we don't need to compute it.
	 * Other transport MUST pass down zero.
	 */
	no_tp_cksum = ipha->ipha_ident;
	ASSERT(ipha->ipha_ident == 0 || ipha->ipha_ident == NO_IP_TP_CKSUM);

	if (CLASSD(dst)) {
		ip1dbg(("ip_wput_ire: to 0x%x ire %s addr 0x%x\n",
		    ntohl(dst),
		    ip_nv_lookup(ire_nv_tbl, ire->ire_type),
		    ntohl(ire->ire_addr)));
	}

/* Macros to extract header fields from data already in registers */
#ifdef	_BIG_ENDIAN
#define	V_HLEN	(v_hlen_tos_len >> 24)
#define	LENGTH	(v_hlen_tos_len & 0xFFFF)
#define	PROTO	(ttl_protocol & 0xFF)
#else
#define	V_HLEN	(v_hlen_tos_len & 0xFF)
#define	LENGTH	((v_hlen_tos_len >> 24) | ((v_hlen_tos_len >> 8) & 0xFF00))
#define	PROTO	(ttl_protocol >> 8)
#endif


	orig_src = src = ipha->ipha_src;
	/* (The loop back to "another" is explained down below.) */
another:;
	/*
	 * Assign an ident value for this packet.  We assign idents on
	 * a per destination basis out of the IRE.  There could be
	 * other threads targeting the same destination, so we have to
	 * arrange for a atomic increment.  Note that we use a 32-bit
	 * atomic add because it has better performance than its
	 * 16-bit sibling.
	 *
	 * If running in cluster mode and if the source address
	 * belongs to a replicated service then vector through
	 * cl_inet_ipident vector to allocate ip identifier
	 * NOTE: This is a contract private interface with the
	 * clustering group.
	 */
	clusterwide = 0;
	if (cl_inet_ipident) {
		ASSERT(cl_inet_isclusterwide);
		if ((*cl_inet_isclusterwide)(IPPROTO_IP,
		    AF_INET, (uint8_t *)src)) {
			ipha->ipha_ident = (*cl_inet_ipident)(IPPROTO_IP,
			    AF_INET, (uint8_t *)src, (uint8_t *)dst);
			clusterwide = 1;
		}
	}
	if (!clusterwide) {
		ipha->ipha_ident =
		    (uint16_t)atomic_add_32_nv(&ire->ire_ident, 1);
	}

#ifndef _BIG_ENDIAN
	ipha->ipha_ident = (ipha->ipha_ident << 8) | (ipha->ipha_ident >> 8);
#endif

	/*
	 * Set source address unless sent on an ill or ipc_unspec_src is set.
	 * This is needed to obey ipc_unspec_src when packets go through
	 * ip_newroute + arp.
	 * Assumes ip_newroute{,_ipif} sets the source address as well.
	 */
	if (src == INADDR_ANY && ipc != NULL && !ipc->ipc_unspec_src) {
		/*
		 * Assign the appropriate source address from the IRE
		 * if none was specified.
		 */
		ASSERT(ire->ire_ipversion == IPV4_VERSION);
		src = ire->ire_src_addr;
		ipha->ipha_src = src;
	}
	stq = ire->ire_stq;

	/*
	 * We only allow ire chains for broadcasts since there will
	 * be multiple IRE_CACHE entries for the same multicast
	 * address (one per ipif).
	 */
	next_mp = NULL;

	/* broadcast packet */
	if (ire->ire_type == IRE_BROADCAST)
		goto broadcast;

	/* loopback ? */
	if (stq == NULL)
		goto nullstq;

	BUMP_MIB(ip_mib.ipOutRequests);
	ttl_protocol = ((uint16_t *)ipha)[4];

	/* pseudo checksum (do it in parts for IP header checksum) */
	cksum = (dst >> 16) + (dst & 0xFFFF) + (src >> 16) + (src & 0xFFFF);

#define	IPH_UDPH_CHECKSUMP(ipha, hlen) \
	((uint16_t *)(((uchar_t *)ipha)+(hlen + 6)))
#define	IPH_TCPH_CHECKSUMP(ipha, hlen) \
	    ((uint16_t *)(((uchar_t *)ipha)+(hlen+TCP_CHECKSUM_OFFSET)))

	if (PROTO != IPPROTO_TCP) {
		queue_t *dev_q = stq->q_next;

		/* flow controlled */
		if ((dev_q->q_next || dev_q->q_first) &&
		    !canput(dev_q))
			goto blocked;
		if (PROTO == IPPROTO_UDP && !no_tp_cksum) {
			hlen = (V_HLEN & 0xF) << 2;
			up = IPH_UDPH_CHECKSUMP(ipha, hlen);
			if (*up) {
				uint_t   sum;

				sum = IP_CSUM(mp, hlen,
				    cksum + IP_UDP_CSUM_COMP);
				*up = (uint16_t)(sum ? sum : ~sum);
			}
		}
	} else if (!no_tp_cksum) {
		hlen = (V_HLEN & 0xF) << 2;
		up = IPH_TCPH_CHECKSUMP(ipha, hlen);
		if (ill && ill->ill_ick.ick_magic == ICK_M_CTL_MAGIC &&
		    dohwcksum && ill->ill_ick.ick_xmit == 0 &&
		    ire->ire_fp_mp != NULL &&
		    (rptr - mp->b_datap->db_base) >=
		    (ire->ire_fp_mp->b_wptr - ire->ire_fp_mp->b_rptr) &&
		    !mctl_present) {
			/*
			 * Underlying interface supports inetcksuming and
			 * M_DATA fast path, so postpone the cksum to the
			 * interface driver.
			 * XXX - we only need to set up b_ick_xxx on the
			 * first mblk.
			 */
			uint32_t	sum;
#ifdef ZC_TEST
			zckstat->zc_hwcksum_w.value.ul++;
#endif
			sum = *up + cksum + IP_TCP_CSUM_COMP;
			sum = (sum & 0xFFFF) + (sum >> 16);
			*up = (sum & 0xFFFF) + (sum >> 16);
			mp->b_ick_start = rptr + hlen;
			mp->b_ick_stuff = (uchar_t *)up;
			mp->b_ick_end = mp->b_wptr;
			mp->b_ick_flag = ICK_VALID;
		} else {
#ifdef ZC_TEST
			zckstat->zc_swcksum_w.value.ul++;
#endif
			*up = IP_CSUM(mp, hlen, cksum + IP_TCP_CSUM_COMP);
			mp->b_ick_flag = ICK_NONE;
			mp->b_ick_stuff = NULL;
		}
	}

	/*
	 * If this is a mutlicast packet and originated from ip_wput
	 * we need to do loopback and forwarding checks. If it comes
	 * from ip_wput_multicast, we SHOULD not do this.
	 */
	if (CLASSD(ipha->ipha_dst) && multicast_forward) goto multi_loopback;

	/* checksum */
	cksum += ttl_protocol;

	max_frag = ire->ire_max_frag;
	/* fragment the packet */
	if (max_frag < (unsigned int)(LENGTH + extra_len)) goto fragmentit;

	/* checksum */
	cksum += ipha->ipha_ident;

	if ((V_HLEN == IP_SIMPLE_HDR_VERSION ||
	    !ip_source_route_included(ipha)) &&
	    !CLASSD(ipha->ipha_dst))
		ipha->ipha_fragment_offset_and_flags |=
		    htons(ire->ire_frag_flag);
	/* checksum */
	cksum += (v_hlen_tos_len >> 16)+(v_hlen_tos_len & 0xFFFF);
	cksum += ipha->ipha_fragment_offset_and_flags;

	/* IP options present */
	hlen = (V_HLEN & 0xF) - IP_SIMPLE_HDR_LENGTH_IN_WORDS;
	if (hlen)
		goto checksumoptions;

	/* calculate hdr checksum */
	cksum = ((cksum & 0xFFFF) + (cksum >> 16));
	cksum = ~(cksum + (cksum >> 16));
	ipha->ipha_hdr_checksum = (uint16_t)cksum;

	if (mctl_present) {
		/*
		 * We will do the rest of the processing after
		 * we come back from IPSEC in ip_wput_ipsec_out().
		 */
		ipsec_out_process(q, first_mp);
		ire_refrele(ire);
		return;
	}

	hlen = 0;
	mp1 = ire->ire_fp_mp;
	if (mp1 != NULL) {
		ASSERT(mp1->b_datap->db_type == M_DATA);
		hlen = mp1->b_wptr - mp1->b_rptr;
	} else {
		mp1 = ire->ire_dlureq_mp;
	}

	/* if not MDATA fast path or fp hdr doesn't fit */
	if (!hlen || (rptr - mp->b_datap->db_base) < hlen)
		goto noprepend;

	ipha = (ipha_t *)(rptr - hlen);
	mp->b_rptr = rptr;
	bcopy(mp1->b_rptr, rptr, hlen);
	ire->ire_ob_pkt_count++;

	TRACE_2(TR_FAC_IP, TR_IP_WPUT_IRE_END,
	    "ip_wput_ire_end: q %p (%S)",
	    q, "last copy out");
	putnext(stq, mp);
	IRE_REFRELE(ire);
	return;

	/*
	 * ire->ire_type == IRE_BROADCAST (minimize diffs)
	 */
broadcast:
	{
		/*
		 * Avoid broadcast storms by setting the ttl to 1
		 * for broadcasts
		 */
		ipha->ipha_ttl = ip_broadcast_ttl;

		/*
		 * Note that we are not doing a IRB_REFHOLD here.
		 * Actually we don't care if the list changes i.e
		 * if somebody deletes an IRE from the list while
		 * we drop the lock, the next time we come around
		 * ire_next will be NULL and hence we won't send
		 * out multiple copies which is fine.
		 */
		rw_enter(&ire->ire_bucket->irb_lock, RW_READER);
		ire1 = ire->ire_next;
		/*
		 * Honor ipc_outgoing_ill for broadcast. Useful in selectively
		 * sending to the all-ones broadcast address.
		 */
		if (ipc != NULL && ipc->ipc_outgoing_ill != NULL) {
			while (ire->ire_ipif->ipif_ill !=
			    ipc->ipc_outgoing_ill) {
				ASSERT(ire1 == ire->ire_next);
				if (ire1 != NULL && ire1->ire_addr == dst) {
					ire_refrele(ire);
					ire = ire1;
					IRE_REFHOLD(ire);
					ire1 = ire->ire_next;
					continue;
				}
				rw_exit(&ire->ire_bucket->irb_lock);
				/* Did not find a matching ill */
				ip1dbg(("ip_wput_ire: broadcast with no "
				    "matching IP_BOUND_IF ill\n"));
				freemsg(first_mp);
				if (ire != NULL)
					ire_refrele(ire);
				return;
			}
		}
		if (ire1 && ire1->ire_addr == dst) {
			/*
			 * IRE chain.  The next IRE has the same
			 * address as the current one thus we might want
			 * to send out multiple copies of this packets.
			 * This is used to send a single copy of a broadcast
			 * packet out all physical interfaces that have an
			 * matching IRE_BROADCAST while also looping
			 * back one copy (to ip_wput_local) for each
			 * matching physical interface. However, we avoid
			 * sending packets out different logical that match by
			 * having ipif_up/ipif_down supress duplicate
			 * IRE_BROADCASTS.
			 *
			 * This feature is currently
			 * used to get broadcasts sent to multiple
			 * interfaces, when the broadcast address
			 * being used applies to multiple interfaces.
			 * For example, a whole net broadcast will be
			 * replicated on every connected subnet of
			 * the target net.
			 *
			 * This logic assumes that ire_add() groups the
			 * IRE_BROADCAST entries so that those with the same
			 * ire_addr are kept together.
			 */
			next_mp = copymsg(first_mp);
			if (next_mp != NULL)
				IRE_REFHOLD(ire1);
		}
		rw_exit(&ire->ire_bucket->irb_lock);
	}

	if (stq) {
		/*
		 * A non-NULL send-to queue means this packet is going
		 * out of this machine.
		 */

		BUMP_MIB(ip_mib.ipOutRequests);
		ttl_protocol = ((uint16_t *)ipha)[4];
		/*
		 * We accumulate the pseudo header checksum in cksum.
		 * This is pretty hairy code, so watch close.  One
		 * thing to keep in mind is that UDP and TCP have
		 * stored their respective datagram lengths in their
		 * checksum fields.  This lines things up real nice.
		 */
		cksum = (dst >> 16) + (dst & 0xFFFF) +
		    (src >> 16) + (src & 0xFFFF);
		/*
		 * We assume the udp checksum field contains the
		 * length, so to compute the pseudo header checksum,
		 * all we need is the protocol number and src/dst.
		 */
		/* Provide the checksums for UDP and TCP. */
		if (PROTO == IPPROTO_TCP && !no_tp_cksum) {
			/* hlen gets the number of uchar_ts in the IP header */
			hlen = (V_HLEN & 0xF) << 2;
			up = IPH_TCPH_CHECKSUMP(ipha, hlen);
			*up = IP_CSUM(mp, hlen, cksum + IP_TCP_CSUM_COMP);
		} else {
			queue_t *dev_q = stq->q_next;

			if ((dev_q->q_next || dev_q->q_first) &&
			    !canput(dev_q)) {
			    blocked:
				ire_refrele(ire);
				if (next_mp) {
					ire_refrele(ire1);
					freemsg(next_mp);
				}
				ipha->ipha_ident = (uint16_t)no_tp_cksum;
				if (ip_output_queue && ipc != NULL) {
					if (ipc->ipc_draining) {
						ipc->ipc_did_putbq = 1;
						(void) putbq(ipc->ipc_wq,
						    first_mp);
					} else {
						(void) putq(ipc->ipc_wq,
						    first_mp);
					}
					return;
				}
				BUMP_MIB(ip_mib.ipOutDiscards);
				freemsg(first_mp);
				TRACE_2(TR_FAC_IP, TR_IP_WPUT_IRE_END,
				    "ip_wput_ire_end: q %p (%S)",
				    q, "discard");
				return;
			}
			if (PROTO == IPPROTO_UDP && !no_tp_cksum) {
				/*
				 * hlen gets the number of uchar_ts in the
				 * IP header
				 */
				hlen = (V_HLEN & 0xF) << 2;
				up = IPH_UDPH_CHECKSUMP(ipha, hlen);
				if (*up) {
					uint_t	sum;

					/*
					 * NOTE: watch out for compiler high
					 * bits
					 */
					sum = IP_CSUM(mp, hlen,
					    cksum + IP_UDP_CSUM_COMP);
					*up = (uint16_t)(sum ? sum : ~sum);
				}
			}
		}
		/*
		 * Need to do this even when fragmenting. The local
		 * loopback can be done without computing checksums
		 * but forwarding out other interface must be done
		 * after the IP checksum (and ULP checksums) have been
		 * computed.
		 *
		 * NOTE : multicast_forward is set only if this packet
		 * originated from ip_wput. For packets originating from
		 * ip_wput_multicast, it is not set.
		 */
		if (CLASSD(ipha->ipha_dst) && multicast_forward) {
		    multi_loopback:
			ill = NULL;
			ip2dbg(("ip_wput: multicast, loop %d\n",
			    ipc_multicast_loop));
			/*
			 * Local loopback of multicasts?  Check the
			 * ill.
			 *
			 * Note that the loopback function will not come
			 * in through ip_rput - it will only do the
			 * client fanout thus we need to do an mforward
			 * as well.  The is different from the BSD
			 * logic.
			 */
			if (ipc_multicast_loop &&
			    (ill = ire_to_ill(ire)) &&
			    ilm_lookup_ill(ill, ipha->ipha_dst) != NULL) {
				/* Pass along the virtual output q. */
				ip_multicast_loopback(q, ill, first_mp);
			}
			if (ipha->ipha_ttl == 0) {
				/*
				 * 0 => only to this host i.e. we are
				 * done.
				 */
				freemsg(first_mp);
				TRACE_2(TR_FAC_IP, TR_IP_WPUT_IRE_END,
				    "ip_wput_ire_end: q %p (%S)",
				    q, "loopback");
				ire_refrele(ire);
				return;
			}
			/*
			 * IFF_MULTICAST is checked in ip_newroute
			 * i.e. we don't need to check it here since
			 * all IRE_CACHEs come from ip_newroute.
			 * For multicast traffic, SO_DONTROUTE is interpreted
			 * to mean only send the packet out the interface
			 * (optionally specified with IP_MULTICAST_IF)
			 * and do not forward it out additional interfaces.
			 * RSVP and the rsvp daemon is an example of a
			 * protocol and user level process that
			 * handles it's own routing. Hence, it uses the
			 * SO_DONTROUTE option to accomplish this.
			 */

			if (ip_g_mrouter && !ipc_dontroute &&
			    (ill || (ill = ire_to_ill(ire)))) {
				/* The checksum has not been computed yet */
				ipha->ipha_hdr_checksum = 0;
				ipha->ipha_hdr_checksum = ip_csum_hdr(ipha);

				/*
				 * If this needs to go out secure, we need
				 * to wait till we finish the IPSEC
				 * processing.
				 */
				if (!mctl_present &&
				    ip_mforward(ill, ipha, mp)) {
					freemsg(first_mp);
					ip1dbg(("ip_wput: mforward failed\n"));
					TRACE_2(TR_FAC_IP, TR_IP_WPUT_IRE_END,
					    "ip_wput_ire_end: q %p (%S)",
					    q, "mforward failed");
					ire_refrele(ire);
					return;
				}
			}
		}

		max_frag = ire->ire_max_frag;
		cksum += ttl_protocol;
		if (max_frag >= (unsigned int)(LENGTH + extra_len)) {
			/* No fragmentation required for this one. */
			/* Complete the IP header checksum. */
			cksum += ipha->ipha_ident;
			/*
			 * Don't use frag_flag if source routed or if
			 * multicast (since multicast packets do not solicit
			 * ICMP "packet too big" messages).
			 */
			if ((V_HLEN == IP_SIMPLE_HDR_VERSION ||
			    !ip_source_route_included(ipha)) &&
			    !CLASSD(ipha->ipha_dst))
				ipha->ipha_fragment_offset_and_flags |=
				    htons(ire->ire_frag_flag);

			cksum += (v_hlen_tos_len >> 16)+
			    (v_hlen_tos_len & 0xFFFF);
			cksum += ipha->ipha_fragment_offset_and_flags;
			hlen = (V_HLEN & 0xF) - IP_SIMPLE_HDR_LENGTH_IN_WORDS;
			if (hlen) {
			    checksumoptions:
				/*
				 * Account for the IP Options in the IP
				 * header checksum.
				 */
				up = (uint16_t *)(rptr+IP_SIMPLE_HDR_LENGTH);
				do {
					cksum += up[0];
					cksum += up[1];
					up += 2;
				} while (--hlen);
			}
			cksum = ((cksum & 0xFFFF) + (cksum >> 16));
			cksum = ~(cksum + (cksum >> 16));
			ipha->ipha_hdr_checksum = (uint16_t)cksum;
			if (mctl_present) {
				ipsec_out_process(q, first_mp);
				if (!next_mp) {
					ire_refrele(ire);
					return;
				}
				goto next;
			}

		noprepend:
			ASSERT(!mctl_present);
			mp1 = ip_wput_attach_llhdr(mp, ire);
			if (!mp1) {
				BUMP_MIB(ip_mib.ipOutDiscards);
				if (next_mp) {
					freemsg(next_mp);
					ire_refrele(ire1);
				}
				freemsg(mp);
				ire_refrele(ire);
				TRACE_2(TR_FAC_IP, TR_IP_WPUT_IRE_END,
				    "ip_wput_ire_end: q %p (%S)",
				    q, "discard MDATA");
				return;
			}
			/* TODO: make this atomic */
			ire->ire_ob_pkt_count++;
			if (!next_mp) {
				/*
				 * Last copy going out (the ultra-common
				 * case).  Note that we intentionally replicate
				 * the putnext rather than calling it before
				 * the next_mp check in hopes of a little
				 * tail-call action out of the compiler.
				 */
				TRACE_2(TR_FAC_IP, TR_IP_WPUT_IRE_END,
				    "ip_wput_ire_end: q %p (%S)",
				    q, "last copy out(1)");
				putnext(stq, mp1);
				ire_refrele(ire);
				return;
			}
			/* More copies going out below. */
			putnext(stq, mp1);
		} else {
			int offset;
		    fragmentit:
			offset = ntohs(ipha->ipha_fragment_offset_and_flags);
			/*
			 * If this would generate a icmp_frag_needed message,
			 * we need to handle it before we do the IPSEC
			 * processing. Otherwise, we need to strip the IPSEC
			 * headers before we send up the message to the ULPs
			 * which becomes messy and difficult.
			 */
			if (mctl_present) {
				if ((max_frag < (unsigned int)(LENGTH +
				    extra_len)) && (offset & IPH_DF)) {

					BUMP_MIB(ip_mib.ipFragFails);
					ipha->ipha_hdr_checksum = 0;
					ipha->ipha_hdr_checksum =
					    (uint16_t)ip_csum_hdr(ipha);
					icmp_frag_needed(ire->ire_stq, first_mp,
					    max_frag);
					if (!next_mp) {
						ire_refrele(ire);
						return;
					}
				} else {
					/*
					 * This won't cause a icmp_frag_needed
					 * message. to be gnerated. Send it on
					 * the wire. Note that this could still
					 * cause fragmentation and all we
					 * do is the generation of the message
					 * to the ULP if needed before IPSEC.
					 */
					if (!next_mp) {
						ipsec_out_process(q, first_mp);
						TRACE_2(TR_FAC_IP,
						    TR_IP_WPUT_IRE_END,
						    "ip_wput_ire_end: q %p "
						    "(%S)", q,
						    "last ipsec_out_process");
						ire_refrele(ire);
						return;
					}
					ipsec_out_process(q, first_mp);
				}
			} else {
				if (!next_mp) {
					TRACE_2(TR_FAC_IP, TR_IP_WPUT_IRE_END,
					    "ip_wput_ire_end: q %p (%S)",
					    q, "last fragmentation");
					ip_wput_ire_fragmentit(mp, ire);
					ire_refrele(ire);
					return;
				}
				ip_wput_ire_fragmentit(mp, ire);
			}
		}
	} else {
	    nullstq:
		/* A NULL stq means the destination address is local. */
		/* TODO: make this atomic */
		ire->ire_ob_pkt_count++;
		ASSERT(ire->ire_ipif != NULL);
		if (!next_mp) {
			TRACE_2(TR_FAC_IP, TR_IP_WPUT_IRE_END,
			    "ip_wput_ire_end: q %p (%S)",
			    q, "local address");
			ip_wput_local(q, ire->ire_ipif->ipif_ill, ipha,
			    first_mp, ire->ire_type);
			ire_refrele(ire);
			return;
		}
		ip_wput_local(q, ire->ire_ipif->ipif_ill, ipha, first_mp,
		    ire->ire_type);
	}
next:
	/*
	 * More copies going out to additional interfaces.
	 * ire1 has already been held. We don't need the
	 * "ire" anymore.
	 */
	ire_refrele(ire);
	ire = ire1;
	ASSERT(ire != NULL && ire->ire_refcnt >= 1 && next_mp != NULL);
	mp = next_mp;
	ASSERT(ire->ire_ipversion == IPV4_VERSION);
	ill = ire_to_ill(ire);
	first_mp = mp;
	if (mctl_present) {
		mp = mp->b_cont;
	}
	dst = ire->ire_addr;
	ipha = (ipha_t *)mp->b_rptr;
	/*
	 * Restore src so that we will pick up ire->ire_src_addr if src was 0.
	 * Restore ipha_ident "no checksum" flag.
	 */
	src = orig_src;
	ipha->ipha_ident = (uint16_t)no_tp_cksum;
	goto another;

#undef	rptr
}

/*
 * Outbound IP fragmentation routine.
 *
 * NOTE : This routine does not ire_refrele the ire that is passed in
 * as the argument.
 */
static void
ip_wput_frag(ire_t *ire, mblk_t *mp_orig, uint_t *pkt_count, uint32_t max_frag,
    uint32_t frag_flag)
{
	int	i1;
	mblk_t	*ll_hdr_mp;
	int 	ll_hdr_len;
	int	hdr_len;
	mblk_t	*hdr_mp;
	ipha_t	*ipha;
	int	ip_data_end;
	int	len;
	mblk_t	*mp = mp_orig;
	int	offset;
	queue_t	*q;
	uint32_t	v_hlen_tos_len;
	mblk_t	*first_mp;
	boolean_t mctl_present;
	mblk_t	*xmit_mp;
	mblk_t	*carve_mp;

	TRACE_0(TR_FAC_IP, TR_IP_WPUT_FRAG_START,
	    "ip_wput_frag_start:");

	if (mp->b_datap->db_type == M_CTL) {
		first_mp = mp;
		mp_orig = mp = mp->b_cont;
		mctl_present = B_TRUE;
	} else {
		first_mp = mp;
		mctl_present = B_FALSE;
	}

	ipha = (ipha_t *)mp->b_rptr;

	/*
	 * If the Don't Fragment flag is on, generate an ICMP destination
	 * unreachable, fragmentation needed.
	 */
	offset = ntohs(ipha->ipha_fragment_offset_and_flags);
	if (offset & IPH_DF) {
		BUMP_MIB(ip_mib.ipFragFails);
		/*
		 * Need to compute hdr checksum if called from ip_wput_ire.
		 * Note that ip_rput_forward verifies the checksum before
		 * calling this routine so in that case this is a noop.
		 */
		ipha->ipha_hdr_checksum = 0;
		ipha->ipha_hdr_checksum = ip_csum_hdr(ipha);
		icmp_frag_needed(ire->ire_stq, first_mp, max_frag);
		TRACE_1(TR_FAC_IP, TR_IP_WPUT_FRAG_END,
		    "ip_wput_frag_end:(%S)",
		    "don't fragment");
		return;
	}
	if (mctl_present)
		freeb(first_mp);
	/*
	 * Establish the starting offset.  May not be zero if we are fragging
	 * a fragment that is being forwarded.
	 */
	offset = offset & IPH_OFFSET;

	/* TODO why is this test needed? */
	v_hlen_tos_len = ((uint32_t *)ipha)[0];
	if (((max_frag - LENGTH) & ~7) < 8) {
		/* TODO: notify ulp somehow */
		BUMP_MIB(ip_mib.ipFragFails);
		freemsg(mp);
		TRACE_1(TR_FAC_IP, TR_IP_WPUT_FRAG_END,
		    "ip_wput_frag_end:(%S)",
		    "len < 8");
		return;
	}

	hdr_len = (V_HLEN & 0xF) << 2;
	ipha->ipha_hdr_checksum = 0;

	/* Get a copy of the header for the trailing frags */
	hdr_mp = ip_wput_frag_copyhdr((uchar_t *)ipha, hdr_len, offset);
	if (!hdr_mp) {
		BUMP_MIB(ip_mib.ipOutDiscards);
		freemsg(mp);
		TRACE_1(TR_FAC_IP, TR_IP_WPUT_FRAG_END,
		    "ip_wput_frag_end:(%S)",
		    "couldn't copy hdr");
		return;
	}

	/* Store the starting offset, with the MoreFrags flag. */
	i1 = offset | IPH_MF | frag_flag;
	ipha->ipha_fragment_offset_and_flags = htons((uint16_t)i1);

	/* Establish the ending byte offset, based on the starting offset. */
	offset <<= 3;
	ip_data_end = offset + ntohs(ipha->ipha_length) - hdr_len;

	/*
	 * Establish the number of bytes maximum per frag, after putting
	 * in the header.
	 */
	len = (max_frag - hdr_len) & ~7;

	/* Store the length of the first fragment in the IP header. */
	i1 = len + hdr_len;
	ASSERT(i1 <= IP_MAXPACKET);
	ipha->ipha_length = htons((uint16_t)i1);

	/*
	 * Compute the IP header checksum for the first frag.  We have to
	 * watch out that we stop at the end of the header.
	 */
	ipha->ipha_hdr_checksum = ip_csum_hdr(ipha);

	/*
	 * Turn off hardware checksumming in case it was on.
	 */
	mp_orig->b_ick_start = NULL;
	mp_orig->b_ick_stuff = NULL;
	mp_orig->b_ick_end = NULL;
	mp_orig->b_ick_flag = ICK_NONE;

	/*
	 * Now carve off the first frag.  Note that this will include the
	 * original IP header.
	 */
	if (!(mp = ip_carve_mp(&mp_orig, i1))) {
		BUMP_MIB(ip_mib.ipOutDiscards);
		freeb(hdr_mp);
		freemsg(mp_orig);
		TRACE_1(TR_FAC_IP, TR_IP_WPUT_FRAG_END,
		    "ip_wput_frag_end:(%S)",
		    "couldn't carve first");
		return;
	}
	ll_hdr_len = 0;
	ll_hdr_mp = ire->ire_fp_mp;
	if (ll_hdr_mp != NULL) {
		ASSERT(ll_hdr_mp->b_datap->db_type == M_DATA);
		ll_hdr_len = ll_hdr_mp->b_wptr - ll_hdr_mp->b_rptr;
	} else {
		ll_hdr_mp = ire->ire_dlureq_mp;
	}

	/* If there is a transmit header, get a copy for this frag. */
	/*
	 * TODO: should check db_ref before calling ip_carve_mp since
	 * it might give us a dup.
	 */
	if (!ll_hdr_mp) {
		/* No xmit header. */
		xmit_mp = mp;
	} else if (mp->b_datap->db_ref == 1 &&
	    ll_hdr_len != 0 &&
	    ll_hdr_len <= mp->b_rptr - mp->b_datap->db_base) {
		/* M_DATA fastpath */
		mp->b_rptr -= ll_hdr_len;
		bcopy(ll_hdr_mp->b_rptr, mp->b_rptr, ll_hdr_len);
		xmit_mp = mp;
	} else if (!(xmit_mp = copyb(ll_hdr_mp))) {
		BUMP_MIB(ip_mib.ipOutDiscards);
		freeb(hdr_mp);
		freemsg(mp);
		freemsg(mp_orig);
		TRACE_1(TR_FAC_IP, TR_IP_WPUT_FRAG_END,
		    "ip_wput_frag_end:(%S)",
		    "discard");
		return;
	} else {
		xmit_mp->b_cont = mp;
	}
	q = ire->ire_stq;
	BUMP_MIB(ip_mib.ipFragCreates);
	putnext(q, xmit_mp);
	pkt_count[0]++;

	/* Advance the offset to the second frag starting point. */
	offset += len;
	/*
	 * Update hdr_len from the copied header - there might be less options
	 * in the later fragments.
	 */
	hdr_len = IPH_HDR_LENGTH(hdr_mp->b_rptr);
	/* Loop until done. */
	for (;;) {
		uint16_t	offset_and_flags;
		uint16_t	ip_len;

		if (ip_data_end - offset > len) {
			/*
			 * Carve off the appropriate amount from the original
			 * datagram.
			 */
			if (!(carve_mp = ip_carve_mp(&mp_orig, len))) {
				mp = NULL;
				break;
			}
			/*
			 * More frags after this one.  Get another copy
			 * of the header.
			 */
			if (carve_mp->b_datap->db_ref == 1 &&
			    hdr_mp->b_wptr - hdr_mp->b_rptr <
			    carve_mp->b_rptr - carve_mp->b_datap->db_base) {
				/* Inline IP header */
				carve_mp->b_rptr -= hdr_mp->b_wptr -
				    hdr_mp->b_rptr;
				bcopy(hdr_mp->b_rptr, carve_mp->b_rptr,
				    hdr_mp->b_wptr - hdr_mp->b_rptr);
				mp = carve_mp;
			} else {
				if (!(mp = copyb(hdr_mp))) {
					freemsg(carve_mp);
					break;
				}
				mp->b_cont = carve_mp;
			}
			ipha = (ipha_t *)mp->b_rptr;
			offset_and_flags = IPH_MF;
		} else {
			/*
			 * Last frag.  Consume the header. Set len to
			 * the length of this last piece.
			 */
			len = ip_data_end - offset;

			/*
			 * Carve off the appropriate amount from the original
			 * datagram.
			 */
			if (!(carve_mp = ip_carve_mp(&mp_orig, len))) {
				mp = NULL;
				break;
			}
			if (carve_mp->b_datap->db_ref == 1 &&
			    hdr_mp->b_wptr - hdr_mp->b_rptr <
			    carve_mp->b_rptr - carve_mp->b_datap->db_base) {
				/* Inline IP header */
				carve_mp->b_rptr -= hdr_mp->b_wptr -
				    hdr_mp->b_rptr;
				bcopy(hdr_mp->b_rptr, carve_mp->b_rptr,
				    hdr_mp->b_wptr - hdr_mp->b_rptr);
				mp = carve_mp;
				freeb(hdr_mp);
				hdr_mp = mp;
			} else {
				mp = hdr_mp;
				mp->b_cont = carve_mp;
			}
			ipha = (ipha_t *)mp->b_rptr;
			/* A frag of a frag might have IPH_MF non-zero */
			offset_and_flags =
			    ntohs(ipha->ipha_fragment_offset_and_flags) &
			    IPH_MF;
		}
		offset_and_flags |= (uint16_t)(offset >> 3);
		offset_and_flags |= (uint16_t)frag_flag;
		/* Store the offset and flags in the IP header. */
		ipha->ipha_fragment_offset_and_flags = htons(offset_and_flags);

		/* Store the length in the IP header. */
		ip_len = (uint16_t)(len + hdr_len);
		ASSERT(ip_len <= IP_MAXPACKET);
		ipha->ipha_length = htons(ip_len);

		/*
		 * Set the IP header checksum.	Note that mp is just
		 * the header, so this is easy to pass to ip_csum.
		 */
		ipha->ipha_hdr_checksum = ip_csum_hdr(ipha);

		/* Attach a transmit header, if any, and ship it. */
		pkt_count[0]++;
		if (!ll_hdr_mp) {
			xmit_mp = mp;
		} else if (mp->b_datap->db_ref == 1 &&
		    ll_hdr_len != 0 &&
		    ll_hdr_len <= mp->b_rptr - mp->b_datap->db_base) {
			/* M_DATA fastpath */
			mp->b_rptr -= ll_hdr_len;
			bcopy(ll_hdr_mp->b_rptr, mp->b_rptr, ll_hdr_len);
			xmit_mp = mp;
		} else if ((xmit_mp = copyb(ll_hdr_mp)) != NULL) {
			xmit_mp->b_cont = mp;
		} else {
			break;
		}
		BUMP_MIB(ip_mib.ipFragCreates);
		putnext(q, xmit_mp);

		/* All done if we just consumed the hdr_mp. */
		if (mp == hdr_mp) {
			BUMP_MIB(ip_mib.ipFragOKs);
			TRACE_1(TR_FAC_IP, TR_IP_WPUT_FRAG_END,
			    "ip_wput_frag_end:(%S)",
			    "consumed hdr_mp");
			return;
		}
		/* Otherwise, advance and loop. */
		offset += len;
	}

	/* Clean up following allocation failure. */
	BUMP_MIB(ip_mib.ipOutDiscards);
	freemsg(mp);
	if (mp != hdr_mp)
		freeb(hdr_mp);
	if (mp != mp_orig)
		freemsg(mp_orig);
	TRACE_1(TR_FAC_IP, TR_IP_WPUT_FRAG_END,
	    "ip_wput_frag_end:(%S)",
	    "end--alloc failure");
}

/*
 * Copy the header plus those options which have the copy bit set
 */
static mblk_t *
ip_wput_frag_copyhdr(uchar_t *rptr, int hdr_len, int offset)
{
	mblk_t	*mp;
	uchar_t	*up;

	/*
	 * Quick check if we need to look for options without the copy bit
	 * set
	 */
	mp = allocb(ip_wroff_extra + hdr_len, BPRI_HI);
	if (!mp)
		return (mp);
	mp->b_rptr += ip_wroff_extra;
	if (hdr_len == IP_SIMPLE_HDR_LENGTH || offset != 0) {
		bcopy(rptr, mp->b_rptr, hdr_len);
		mp->b_wptr += hdr_len + ip_wroff_extra;
		return (mp);
	}
	up  = mp->b_rptr;
	bcopy(rptr, up, IP_SIMPLE_HDR_LENGTH);
	up += IP_SIMPLE_HDR_LENGTH;
	rptr += IP_SIMPLE_HDR_LENGTH;
	hdr_len -= IP_SIMPLE_HDR_LENGTH;
	while (hdr_len > 0) {
		uint32_t optval;
		uint32_t optlen;

		optval = *rptr;
		if (optval == IPOPT_EOL)
			break;
		if (optval == IPOPT_NOP)
			optlen = 1;
		else
			optlen = rptr[1];
		if (optval & IPOPT_COPY) {
			bcopy(rptr, up, optlen);
			up += optlen;
		}
		rptr += optlen;
		hdr_len -= optlen;
	}
	/*
	 * Make sure that we drop an even number of words by filling
	 * with EOL to the next word boundary.
	 */
	for (hdr_len = up - (mp->b_rptr + IP_SIMPLE_HDR_LENGTH);
	    hdr_len & 0x3; hdr_len++)
		*up++ = IPOPT_EOL;
	mp->b_wptr = up;
	/* Update header length */
	mp->b_rptr[0] = (uint8_t)((IP_VERSION << 4) | ((up - mp->b_rptr) >> 2));
	return (mp);
}

/*
 * Delivery to local recipients including fanout to multiple recipients.
 * Does not do checksumming of UDP/TCP.
 * Note: q should be the read side queue for either the ill or ipc.
 * Note: rq should be the read side q for the lower (ill) stream.
 */
void
ip_wput_local(queue_t *q, ill_t *ill, ipha_t *ipha, mblk_t *mp, int ire_type)
{
	uint32_t	protocol;
	mblk_t		*first_mp;
	boolean_t	mctl_present;
#define	rptr	((uchar_t *)ipha)

	TRACE_1(TR_FAC_IP, TR_IP_WPUT_LOCAL_START,
	    "ip_wput_local_start: q %p", q);

	first_mp = mp;
	if (first_mp->b_datap->db_type == M_CTL) {
		ipsec_out_t *io = (ipsec_out_t *)first_mp->b_rptr;
		if (!io->ipsec_out_secure) {
			/*
			 * This ipsec_out_t was allocated in ip_wput
			 * for multicast packets to store the ill_index.
			 * As this is being delivered locally, we don't
			 * need this anymore.
			 */
			mp = first_mp->b_cont;
			freeb(first_mp);
			first_mp = mp;
			mctl_present = B_FALSE;
		} else {
			mctl_present = B_TRUE;
			mp = first_mp->b_cont;
			ASSERT(mp != NULL);
			ipsec_out_to_in(first_mp);
		}
	} else {
		mctl_present = B_FALSE;
	}

	loopback_packets++;

	ip2dbg(("ip_wput_local: from 0x%x to 0x%x\n",
	    ntohl(ipha->ipha_src), ntohl(ipha->ipha_dst)));
	if (!IS_SIMPLE_IPH(ipha)) {
		ip_wput_local_options(ipha);
	}
	protocol = ipha->ipha_protocol;
	switch (protocol) {
	case IPPROTO_ICMP:
		icmp_inbound(q, first_mp, ire_type, ill, 0, 0, mctl_present);
		TRACE_2(TR_FAC_IP, TR_IP_WPUT_LOCAL_END,
		    "ip_wput_local_end: q %p (%S)",
		    q, "icmp");
		return;
	case IPPROTO_IGMP:
		if (igmp_input(q, mp, ill)) {
			/* Bad packet - discarded by igmp_input */
			TRACE_2(TR_FAC_IP, TR_IP_WPUT_LOCAL_END,
			    "ip_wput_local_end: q %p (%S)",
			    q, "igmp_input--bad packet");
			if (mctl_present)
				freeb(first_mp);
			return;
		}
		/*
		 * igmp_input() may have pulled up the message so ipha needs to
		 * be reinitialized.
		 */
		ipha = (ipha_t *)mp->b_rptr;
		/* deliver to local raw users */
		break;
	case IPPROTO_ENCAP:
		/*
		 * This case is covered by either ip_fanout_proto, or by
		 * the above security processing for self-tunneled packets.
		 */
		break;
	case IPPROTO_UDP: {
		uint16_t	*up;
		uint32_t	ports;

		up = (uint16_t *)(rptr + IPH_HDR_LENGTH(ipha) +
		    UDP_PORTS_OFFSET);
		/* Force a 'valid' checksum. */
		up[3] = 0;

		ports = *(uint32_t *)up;
		ip_fanout_udp(q, first_mp, ill, ipha, ports,
		    (ushort_t)ire_type, IP_FF_SEND_ICMP | IP_FF_HDR_COMPLETE,
		    mctl_present);
		TRACE_2(TR_FAC_IP, TR_IP_WPUT_LOCAL_END,
		    "ip_wput_local_end: q %p (%S)", q, "ip_fanout_udp");
		return;
	}
	case IPPROTO_TCP: {
		uint32_t	ports;

		if (mp->b_datap->db_type == M_DATA) {
			/*
			 * M_DATA mblk, so init mblk (chain) for no struio().
			 */
			mblk_t	*mp1 = mp;

			do
				mp1->b_datap->db_struioflag = 0;
			while ((mp1 = mp1->b_cont) != NULL);
		}
		ports = *(uint32_t *)(rptr + IPH_HDR_LENGTH(ipha) +
		    TCP_PORTS_OFFSET);
		ip_fanout_tcp(q, first_mp, ipha, ports,
		    IP_FF_SEND_ICMP | IP_FF_HDR_COMPLETE |
		    IP_FF_SYN_ADDIRE, mctl_present);
		TRACE_2(TR_FAC_IP, TR_IP_WPUT_LOCAL_END,
		    "ip_wput_local_end: q %p (%S)", q, "ip_fanout_tcp");
		return;
	}
	default:
		break;
	}
	/*
	 * Find a client for some other protocol.  We give
	 * copies to multiple clients, if more than one is
	 * bound.
	 */
	ip_fanout_proto(q, first_mp, ill, ipha,
	    IP_FF_SEND_ICMP | IP_FF_HDR_COMPLETE | IP_FF_RAWIP, mctl_present);
	TRACE_2(TR_FAC_IP, TR_IP_WPUT_LOCAL_END,
	    "ip_wput_local_end: q %p (%S)", q, "ip_fanout_proto");
#undef	rptr
}

/* Update any source route, record route or timestamp options */
/* Check that we are at end of strict source route */
static void
ip_wput_local_options(ipha_t *ipha)
{
	uint32_t	totallen;
	uchar_t		*opt;
	uint32_t	optval;
	uint32_t	optlen;
	ipaddr_t	dst;
	uint32_t	ts;
	ire_t		*ire;

	ip2dbg(("ip_wput_local_options\n"));
	totallen = ipha->ipha_version_and_hdr_length -
	    (uint8_t)((IP_VERSION << 4) + IP_SIMPLE_HDR_LENGTH_IN_WORDS);
	opt = (uchar_t *)&ipha[1];
	totallen <<= 2;
	while (totallen != 0) {
		switch (optval = opt[IPOPT_POS_VAL]) {
		case IPOPT_EOL:
			return;
		case IPOPT_NOP:
			optlen = 1;
			break;
		default:
			optlen = opt[IPOPT_POS_LEN];
		}
		if (optlen == 0 || optlen > totallen)
			break;

		switch (optval) {
			uint32_t off;
		case IPOPT_SSRR:
		case IPOPT_LSRR:
			off = opt[IPOPT_POS_OFF];
			off--;
			if (optlen < IP_ADDR_LEN ||
			    off > optlen - IP_ADDR_LEN) {
				/* End of source route */
				break;
			}
			/*
			 * This will only happen if two consecutive entries
			 * in the source route contains our address or if
			 * it is a packet with a loose source route which
			 * reaches us before consuming the whole source route
			 */
			ip1dbg(("ip_wput_local_options: not end of SR\n"));
			if (optval == IPOPT_SSRR) {
				return;
			}
			/*
			 * Hack: instead of dropping the packet truncate the
			 * source route to what has been used.
			 */
			bzero((char *)opt + off, optlen - off);
			opt[IPOPT_POS_LEN] = (uint8_t)off;
			break;
		case IPOPT_RR:
			off = opt[IPOPT_POS_OFF];
			off--;
			if (optlen < IP_ADDR_LEN ||
			    off > optlen - IP_ADDR_LEN) {
				/* No more room - ignore */
				ip1dbg((
				    "ip_wput_forward_options: end of RR\n"));
				break;
			}
			dst = htonl(INADDR_LOOPBACK);
			bcopy(&dst, (char *)opt + off, IP_ADDR_LEN);
			opt[IPOPT_POS_OFF] += IP_ADDR_LEN;
			break;
		case IPOPT_IT:
			/* Insert timestamp if there is romm */
			switch (opt[IPOPT_POS_OV_FLG] & 0x0F) {
			case IPOPT_IT_TIME:
				off = IPOPT_IT_TIMELEN;
				break;
			case IPOPT_IT_SPEC:
#ifdef IPOPT_IT_SPEC_BSD
			case IPOPT_IT_SPEC_BSD:
#endif
				/* Verify that the address matched */
				off = opt[IPOPT_POS_OFF] - 1;
				bcopy((char *)opt + off, &dst, IP_ADDR_LEN);
				ire = ire_ctable_lookup(dst, 0, IRE_LOCAL,
				    NULL, NULL, MATCH_IRE_TYPE);
				if (ire == NULL) {
					/* Not for us */
					break;
				}
				ire_refrele(ire);
				/* FALLTHRU */
			case IPOPT_IT_TIME_ADDR:
				off = IP_ADDR_LEN + IPOPT_IT_TIMELEN;
				break;
			default:
				/*
				 * ip_*put_options should have already
				 * dropped this packet.
				 */
				cmn_err(CE_PANIC, "ip_wput_local_options: "
				    "unknown IT - bug in ip_wput_options?\n");
				return;	/* Keep "lint" happy */
			}
			if (opt[IPOPT_POS_OFF] - 1 + off > optlen) {
				/* Increase overflow counter */
				off = (opt[IPOPT_POS_OV_FLG] >> 4) + 1;
				opt[IPOPT_POS_OV_FLG] = (uint8_t)
				    (opt[IPOPT_POS_OV_FLG] & 0x0F) |
				    (off << 4);
				break;
			}
			off = opt[IPOPT_POS_OFF] - 1;
			switch (opt[IPOPT_POS_OV_FLG] & 0x0F) {
			case IPOPT_IT_SPEC:
#ifdef IPOPT_IT_SPEC_BSD
			case IPOPT_IT_SPEC_BSD:
#endif
			case IPOPT_IT_TIME_ADDR:
				dst = htonl(INADDR_LOOPBACK);
				bcopy(&dst, (char *)opt + off, IP_ADDR_LEN);
				opt[IPOPT_POS_OFF] += IP_ADDR_LEN;
				/* FALLTHRU */
			case IPOPT_IT_TIME:
				off = opt[IPOPT_POS_OFF] - 1;
				/* Compute # of milliseconds since midnight */
				ts = (hrestime.tv_sec % (24 * 60 * 60)) * 1000 +
					hrestime.tv_nsec / (NANOSEC / MILLISEC);
				bcopy(&ts, (char *)opt + off, IPOPT_IT_TIMELEN);
				opt[IPOPT_POS_OFF] += IPOPT_IT_TIMELEN;
				break;
			}
			break;
		}
		totallen -= optlen;
		opt += optlen;
	}
}

/*
 * Send out a multicast packet on interface ipif.
 * The sender does not have an ipc.
 */
void
ip_wput_multicast(queue_t *q, mblk_t *mp, ipif_t *ipif)
{
	ipha_t	*ipha;
#define	rptr	((uchar_t *)ipha)
	ire_t	*ire;
	uint32_t	v_hlen_tos_len;
	ipaddr_t	dst;
	mblk_t		*first_mp;
	ipsec_out_t	*io;

	ASSERT(!ipif->ipif_isv6);
	ipha = (ipha_t *)mp->b_rptr;

#ifdef	_BIG_ENDIAN
#define	V_HLEN	(v_hlen_tos_len >> 24)
#else
#define	V_HLEN	(v_hlen_tos_len & 0xFF)
#endif

#ifndef SPEED_BEFORE_SAFETY
	/*
	 * Make sure we have a full-word aligned message and that at least
	 * a simple IP header is accessible in the first message.  If not,
	 * try a pullup.
	 */
	if (!OK_32PTR(rptr) || (mp->b_wptr - rptr) < IP_SIMPLE_HDR_LENGTH) {
		if (!pullupmsg(mp, IP_SIMPLE_HDR_LENGTH)) {
			BUMP_MIB(ip_mib.ipOutDiscards);
			freemsg(mp);
			return;
		}
		ipha = (ipha_t *)mp->b_rptr;
	}
#endif

	/* Most of the code below is written for speed, not readability */
	v_hlen_tos_len = ((uint32_t *)ipha)[0];

#ifndef SPEED_BEFORE_SAFETY
	/*
	 * If ip_newroute() fails, we're going to need a full
	 * header for the icmp wraparound.
	 */
	if (V_HLEN != IP_SIMPLE_HDR_VERSION) {
		uint_t	v_hlen = V_HLEN;
		if ((v_hlen >> 4) != IP_VERSION) {
			BUMP_MIB(ip_mib.ipOutDiscards);
			freemsg(mp);
			return;
		}
		/*
		 * Are there enough bytes accessible in the header?  If
		 * not, try a pullup.
		 */
		v_hlen &= 0xF;
		v_hlen <<= 2;
		if (v_hlen > (mp->b_wptr - rptr)) {
			if (!pullupmsg(mp, v_hlen)) {
				BUMP_MIB(ip_mib.ipOutDiscards);
				freemsg(mp);
				return;
			}
			ipha = (ipha_t *)mp->b_rptr;
		}
	}
#endif
	/*
	 * We need to make sure this packet goes out on ipif. If
	 * there is some global policy match in ip_wput_ire, we need
	 * to get to the right interface after IPSEC processing.
	 * To make sure this multicast packet goes out on the right
	 * interface, we attach an ipsec_out and initialize ill_index
	 * like we did in ip_wput. To make sure that this packet does
	 * not get forwarded on other interfaces or looped back, we
	 * set ipsec_out_dontroute to B_TRUE and ipsec_out_multicast_loop
	 * to B_FALSE.
	 */

	if ((first_mp = allocb(sizeof (ipsec_info_t), BPRI_HI)) == NULL) {
		freemsg(mp);
		return;
	}

	first_mp->b_datap->db_type = M_CTL;
	first_mp->b_wptr += sizeof (ipsec_info_t);
	/* ipsec_out_secure is B_FALSE now */
	bzero(first_mp->b_rptr, sizeof (ipsec_info_t));
	io = (ipsec_out_t *)first_mp->b_rptr;
	io->ipsec_out_type = IPSEC_OUT;
	io->ipsec_out_len = sizeof (ipsec_out_t);
	io->ipsec_out_use_global_policy = B_TRUE;
	first_mp->b_cont = mp;
	io->ipsec_out_ill_index = ipif->ipif_ill->ill_index;
	io->ipsec_out_multicast_loop = B_FALSE;
	io->ipsec_out_dontroute = B_TRUE;

	/*
	 * Find an IRE which matches the destination and the outgoing
	 * queue (i.e. the outgoing interface.)
	 */
	if (ipif->ipif_flags & IFF_POINTOPOINT)
		dst = ipif->ipif_pp_dst_addr;
	else
		dst = ipha->ipha_dst;
	ire = ire_ctable_lookup(dst, 0, 0, ipif, NULL, MATCH_IRE_IPIF);
	if (!ire) {
		/*
		 * Mark this packet to make it be delivered to
		 * ip_wput_ire after the new ire has been
		 * created.
		 */
		mp->b_prev = NULL;
		mp->b_next = NULL;
		ip_newroute_ipif(q, first_mp, ipif, dst, NULL);
		return;
	}
	ip_wput_ire(q, first_mp, ire, NULL);
#undef rptr
}

/*
 * NOTE : This function does not ire_refrele the ire argument passed in.
 */
static mblk_t *
ip_wput_attach_llhdr(mblk_t *mp, ire_t *ire)
{
	uint_t	hlen;
	ipha_t *ipha;
	mblk_t *mp1;
#define	rptr	((uchar_t *)ipha)

	ipha = (ipha_t *)mp->b_rptr;

	hlen = 0;
	mp1 = ire->ire_fp_mp;
	if (mp1 != NULL) {
		ASSERT(mp1->b_datap->db_type == M_DATA);
		hlen = mp1->b_wptr - mp1->b_rptr;
	} else {
		mp1 = ire->ire_dlureq_mp;
	}
	/*
	 * If the driver accepts M_DATA prepends
	 * and we have enough room to lay it in ...
	 */
	if (hlen && (rptr - mp->b_datap->db_base) >= hlen) {
		/* XXX ipha is not aligned here */
		ipha = (ipha_t *)(rptr - hlen);
		mp->b_rptr = rptr;
		/* TODO: inline this small copy */
		bcopy(mp1->b_rptr, rptr, hlen);
		mp1 = mp;
	} else {
		mp1 = copyb(mp1);
		if (!mp1) {
			return (NULL);
		}
		mp1->b_cont = mp;
	}
	return (mp1);
#undef rptr
}

void
ip_wput_ipsec_out(queue_t *q, mblk_t *ipsec_mp)
{
	uint32_t v_hlen_tos_len;
	ipha_t *ipha;
#define	rptr	((uchar_t *)ipha)
	ipaddr_t	dst;
	ipif_t	*ipif;
	ire_t *ire;
	mblk_t *mp;
	mblk_t *mp1;

#ifdef	_BIG_ENDIAN
#define	LENGTH	(v_hlen_tos_len & 0xFFFF)
#else
#define	LENGTH	((v_hlen_tos_len >> 24) | ((v_hlen_tos_len >> 8) & 0xFF00))
#endif

	mp = ipsec_mp->b_cont;
	ASSERT(mp != NULL);

	ipha = (ipha_t *)mp->b_rptr;
	v_hlen_tos_len = ((uint32_t *)ipha)[0];
	dst = ipha->ipha_dst;

	if (CLASSD(dst)) {
		uint_t	ill_index;
		ipsec_out_t *io;
		boolean_t ipc_dontroute;
		ill_t *ill;
		/*
		 * Use the ill_index to get the right ipif.
		 */
		io = (ipsec_out_t *)ipsec_mp->b_rptr;
		ill_index = io->ipsec_out_ill_index;
		ipc_dontroute = io->ipsec_out_dontroute;
		if (ill_index == 0) {
			ipif = ipif_lookup_group(htonl(INADDR_UNSPEC_GROUP));
			if (ipif == NULL) {
				ip1dbg(("ip_wput_ipsec_out: No ipif for"
				    " multicast\n"));
				BUMP_MIB(ip_mib.ipOutNoRoutes);
				freemsg(ipsec_mp);
				return;
			}
			ire = ire_ctable_lookup(dst, 0, 0, ipif, NULL,
			    MATCH_IRE_ILL);
		} else {
			for (ill = ill_g_head; ill; ill = ill->ill_next) {
				if (ill->ill_index == ill_index)
					break;
			}
			if (ill == NULL) {
				ip0dbg(("ip_wput_ipsec_out: interface"
				    " disappeared\n"));
				freemsg(ipsec_mp);
				return;
			}
			ipif = ill->ill_ipif;
			/*
			 * ipha_src has already been intialized with the
			 * value of the ipif in ip_wput. All we need now is
			 * an ire to send this downstream.
			 */
			ire = ire_ctable_lookup(dst, 0, 0, ipif, NULL,
			    MATCH_IRE_ILL);
		}

		if (ire != NULL) {
			/*
			 * Do the multicast forwarding now, as the IPSEC
			 * processing has been done.
			 */
			if (ip_g_mrouter && !ipc_dontroute &&
			    (ill = ire_to_ill(ire))) {
				if (ip_mforward(ill, ipha, mp)) {
					freemsg(ipsec_mp);
					ip1dbg(("ip_wput_ipsec_out: mforward "
					    "failed\n"));
					ire_refrele(ire);
					return;
				}
			}
			goto send;
		}

		ip0dbg(("ip_wput_ipsec_out: multicast: IRE disappeared\n"));
		mp->b_prev = NULL;
		mp->b_next = NULL;

		/*
		 * We may be using a wrong ipif to create the ire.
		 * But it is okay as the source address is assigned
		 * for the packet already. Next outbound packet would
		 * create the IRE with the right IPIF in ip_wput.
		 */
		ip_newroute_ipif(q, ipsec_mp, ipif, dst, NULL);
	} else {

		ire = ire_cache_lookup(dst);
		if (ire != NULL)
			goto send;


		/*
		 * ire disappeared underneath.
		 *
		 * What we need to do here is the ip_newroute
		 * logic to get the ire without doing the IPSEC
		 * processing. Follow the same old path. But this
		 * time, ip_wput or ire_add_then_put will call us
		 * directly as all the IPSEC operations are done.
		 */
		ip1dbg(("ip_wput_ipsec_out: IRE disappeared\n"));
		mp->b_prev = NULL;
		mp->b_next = NULL;

		ip_newroute(q, ipsec_mp, dst, NULL);
	}
	return;
send:
	if (ire->ire_stq == NULL) {
		/*
		 * Loopbacks go through ip_wput_local except for one case.
		 * We come here if we generate a icmp_frag_needed message
		 * after IPSEC processing is over. When this function calls
		 * ip_wput_ire_fragmentit, ip_wput_frag might end up calling
		 * icmp_frag_needed. The message generated comes back here
		 * through icmp_frag_needed -> icmp_pkt -> ip_wput ->
		 * ipsec_out_process -> ip_wput_ipsec_out. We need to set the
		 * source address as it is usually set in ip_wput_ire. As
		 * ipsec_out_proc_begin is set, ip_wput calls ipsec_out_process
		 * and we end up here. We can't enter ip_wput_ire once the
		 * IPSEC processing is over and hence we need to do it here.
		 */
		ire->ire_ob_pkt_count++;
		if (ipha->ipha_src == 0)
			ipha->ipha_src = ire->ire_src_addr;
		ip_wput_local(RD(q), ire->ire_ipif->ipif_ill, ipha, ipsec_mp,
		    ire->ire_type);
		ire_refrele(ire);
		return;
	}

	/* Everything is done. Send it out on the wire */
	if (ire->ire_max_frag >= (unsigned int)LENGTH) {

		mp1 = ip_wput_attach_llhdr(mp, ire);
		if (!mp1) {
			BUMP_MIB(ip_mib.ipOutDiscards);
			freemsg(ipsec_mp);
			ire_refrele(ire);
			return;
		}
		ire->ire_ob_pkt_count++;
		putnext(ire->ire_stq, mp1);
		freeb(ipsec_mp);
	} else {
		/*
		 * We are through with IPSEC processing.
		 * Fragment this and send it on the wire.
		 */
		ip_wput_ire_fragmentit(ipsec_mp, ire);
	}
#undef rptr
	ire_refrele(ire);
}

/*
 * Process an IPSEC_OUT message and see what you can
 * do with it.
 */

static void
ipsec_out_process(queue_t *q, mblk_t *ipsec_mp)
{
	ipsec_out_t *io;
	uint32_t ah_spi;
	uint32_t esp_spi;
	boolean_t encaps;
	int	ah_req;
	int	esp_req;
	int	se_req;
	mblk_t	*outer_mp;
	mblk_t *inner_mp;

	io = (ipsec_out_t *)ipsec_mp->b_rptr;

	/*
	 * IPSEC processing has started.
	 */
	io->ipsec_out_proc_begin = B_TRUE;

	ah_spi = io->ipsec_out_ah_spi;
	esp_spi = io->ipsec_out_esp_spi;
	encaps = io->ipsec_out_encaps;

	/* Following are the requests that we need to satisfy. */

	ah_req = io->ipsec_out_ah_req;
	esp_req = io->ipsec_out_esp_req;
	se_req = io->ipsec_out_self_encap_req;

	/*
	 * The order of processing is first insert a IP header if needed.
	 * Then insert the ESP header and then the AH header.
	 */
	if ((se_req & IPSEC_PREF_REQUIRED) &&
	    !(se_req & IPSEC_REQ_DONE)) {

		/*
		 * First get the outer IP header before sending
		 * it to ESP.
		 */
		if (!(se_req & IPSEC_REQ_FAILED) && !encaps) {
			ipha_t *oipha, *iipha;
			if ((outer_mp = allocb(sizeof (ipha_t), BPRI_HI))
			    == NULL) {
				(void) mi_strlog(q, 0, SL_ERROR|SL_TRACE|
				    SL_CONSOLE, "ipsec_out_process: "
				    "Self-Encapsualtion failed : Out of "
				    "memory\n");
				freemsg(ipsec_mp);
				BUMP_MIB(ip_mib.ipOutDiscards);
				return;
			}
			inner_mp = ipsec_mp->b_cont;
			ASSERT(inner_mp->b_datap->db_type == M_DATA);
			oipha = (ipha_t *)outer_mp->b_rptr;
			iipha = (ipha_t *)inner_mp->b_rptr;
			*oipha = *iipha;
			outer_mp->b_wptr += sizeof (ipha_t);
			oipha->ipha_length = htons(ntohs(
			    iipha->ipha_length) + sizeof (ipha_t));
			oipha->ipha_protocol = IPPROTO_ENCAP;
			oipha->ipha_version_and_hdr_length =
			    IP_SIMPLE_HDR_VERSION;
			oipha->ipha_hdr_checksum = 0;
			oipha->ipha_hdr_checksum = ip_csum_hdr(oipha);
			outer_mp->b_cont = inner_mp;
			ipsec_mp->b_cont = outer_mp;
		}
		io->ipsec_out_self_encap_req |= IPSEC_REQ_DONE;
		io->ipsec_out_encaps = B_TRUE;
	}

	if ((esp_req & IPSEC_PREF_REQUIRED) && !(esp_req & IPSEC_REQ_DONE)) {

		if (esp_req & IPSEC_REQ_FAILED) {
			/*
			 * Some failure in ESP. See whether it is okay
			 * for this request. As of now, we would always
			 * drop the datagram.
			 */
			if (!ipsec_check_policy(ipsec_mp, NULL, IPSEC_OUTBOUND,
			    B_TRUE)) {
				/*
				 * Policy requires to drop this datagram.
				 * As ESP transport is always done first
				 * we should not have anything here to
				 * release. But if we support nested ESP
				 * it may come here.
				 */
				freemsg(ipsec_mp);
				BUMP_MIB(ip_mib.ipOutDiscards);
				return;
			}
			/*
			 * IPSEC_REQ_FAILED is used to communicate between IP
			 * and IPSEC. This value is not used within IP itself
			 * to infer about an operation. Instead, IPSEC_REQ_DONE
			 * is used. So, just leave IPSEC_REQ_FAILED unmarked.
			 */
		}
		if (!(esp_req & IPSEC_REQ_FAILED) && esp_spi == 0) {
			/* Nothing has been done. */
			ip_fanout_sec_proto(q, ipsec_mp, IPPROTO_ESP, 0);
			return;
		}
		/*
		 * Either policy granted us to transmit this without
		 * ESP or ESP processing is done.
		 */
		io->ipsec_out_esp_req |= IPSEC_REQ_DONE;
	}

	if ((ah_req & IPSEC_PREF_REQUIRED) && !(ah_req & IPSEC_REQ_DONE)) {

		if (ah_req & IPSEC_REQ_FAILED) {
			/*
			 * Some failure in AH. See whether it is okay
			 * for this request. As of now, we always drop the
			 * datagram.
			 */
			if (!ipsec_check_policy(ipsec_mp, NULL, IPSEC_OUTBOUND,
			    B_TRUE)) {
				/*
				 * Policy requires to drop this datagram.
				 */
				freemsg(ipsec_mp);
				BUMP_MIB(ip_mib.ipOutDiscards);
				return;
			}
		}
		if (!(ah_req & IPSEC_REQ_FAILED) && ah_spi == 0) {
			/* Nothing has been done. */
			ip_fanout_sec_proto(q, ipsec_mp, IPPROTO_AH, 0);
			return;
		}
		/*
		 * Either policy granted us to transmit this without
		 * AH or AH processing is done.
		 */
		io->ipsec_out_ah_req |= IPSEC_REQ_DONE;
	}

	/*
	 * We are done with IPSEC processing. Send it over
	 * the wire.
	 */
	ip_wput_ipsec_out(q, ipsec_mp);
}

/* Called from ip_wput for all non data messages */
void
ip_wput_nondata(queue_t *q, mblk_t *mp)
{
	mblk_t	*mp1;

	switch (mp->b_datap->db_type) {
	case M_IOCTL:
		/*
		 * IOCTL processing begins in ip_sioctl_copyin_setup which
		 * will arrange to copy in associated control structures.
		 */
		ip_sioctl_copyin_setup(q, mp);
		return;
	case M_IOCDATA:
		/* IOCTL continuation following copyin or copyout. */
		if (mi_copy_state(q, mp, NULL) == -1) {
			/*
			 * The copy operation failed.  mi_copy_state already
			 * cleaned up, so we're out of here.
			 */
			return;
		}
		/*
		 * If we just completed a copy in, we become writer and
		 * continue processing in ip_sioctl_copyin_done.  If it
		 * was a copy out, we call mi_copyout again.  If there is
		 * nothing more to copy out, it will complete the IOCTL.
		 */
		if (MI_COPY_DIRECTION(mp) == MI_COPY_IN) {
			if (ip_sioctl_copyin_writer(mp))
				become_exclusive(q, mp, ip_sioctl_copyin_done);
			else
				ip_sioctl_copyin_done(q, mp);
		} else
			mi_copyout(q, mp);
		return;
	case M_IOCNAK:
		/*
		 * The only way we could get here is if a resolver didn't like
		 * an IOCTL we sent it.	 This shouldn't happen.
		 */
		(void) mi_strlog(q, 1, SL_ERROR|SL_TRACE,
		    "ip_wput: unexpected M_IOCNAK, ioc_cmd 0x%x",
		    ((struct iocblk *)mp->b_rptr)->ioc_cmd);
		freemsg(mp);
		return;
	case M_IOCACK:
		/* Finish socket ioctls passed through to ARP. */
		ip_sioctl_iocack(q, mp);
		return;
	case M_FLUSH:
		if (*mp->b_rptr & FLUSHW)
			flushq(q, FLUSHALL);
		if (q->q_next) {
			putnext(q, mp);
			return;
		}
		if (*mp->b_rptr & FLUSHR) {
			*mp->b_rptr &= ~FLUSHW;
			qreply(q, mp);
			return;
		}
		freemsg(mp);
		return;
	case IRE_DB_REQ_TYPE:
		/* An Upper Level Protocol wants a copy of an IRE. */
		ip_ire_req(q, mp);
		return;
	case M_CTL:
		/* M_CTL messages are used by ARP to tell us things. */
		if ((mp->b_wptr - mp->b_rptr) < sizeof (arc_t))
			break;
		switch (((arc_t *)mp->b_rptr)->arc_cmd) {
		case AR_ENTRY_SQUERY:
			ip_wput_ctl(q, mp);
			return;
		case AR_CLIENT_NOTIFY:
			ip_arp_news(q, mp);
			return;
		case AR_DLPIOP_DONE:
			become_exclusive(q, mp, (pfi_t)ip_arp_done);
			return;
		default:
			break;
		}
		break;
	case M_PROTO:
	case M_PCPROTO:
		/*
		 * The only PROTO messages we expect are ULP binds and
		 * copies of option negotiation acknowledgements.
		 */
		switch (((union T_primitives *)mp->b_rptr)->type) {
		case O_T_BIND_REQ:
		case T_BIND_REQ: {
			ipc_t *ipc = (ipc_t *)q->q_ptr;

			if (ipc->ipc_af_isv6)
				ip_bind_v6(q, mp);
			else
				ip_bind_v4(q, mp);
			return;
		}
		case T_SVR4_OPTMGMT_REQ:
			ip2dbg(("ip_wput: T_SVR4_OPTMGMT_REQ\n"));

			if (ip_optmgmt_writer(mp))
				become_exclusive(q, mp, (pfi_t)ip_optcom_req);
			else {
				if (!snmpcom_req(q, mp, ip_snmp_set,
				    ip_snmp_get,
				    IS_PRIVILEGED_QUEUE(q)))
					/*
					 * Call svr4_optcom_req so that it can
					 * generate the ack.
					 */
					svr4_optcom_req(q, mp,
					    IS_PRIVILEGED_QUEUE(q),
					    &ip_opt_obj);
			}
			return;
		case T_OPTMGMT_REQ:
			ip2dbg(("ip_wput: T_OPTMGMT_REQ\n"));
			/*
			 * Note: No snmpcom_req support through new
			 * T_OPTMGMT_REQ.
			 */
			if (ip_optmgmt_writer(mp))
				become_exclusive(q, mp, (pfi_t)ip_optcom_req);
			else {
				/*
				 * Call tpi_optcom_req so that it can
				 * generate the ack.
				 */
				tpi_optcom_req(q, mp, IS_PRIVILEGED_QUEUE(q),
				    &ip_opt_obj);
			}
			return;
		case T_UNBIND_REQ:
			ip_unbind(q, mp);
			return;
		default:
			/*
			 * Have to drop any DLPI messages coming down from
			 * arp (such as an info_req which would cause ip
			 * to receive an extra info_ack if it was passed
			 * through.
			 */
			ip1dbg(("ip_wput_nondata: dropping M_PROTO %d\n",
			    (int)*(uint_t *)mp->b_rptr));
			freemsg(mp);
			return;
		}
		/* NOTREACHED */
	case IRE_DB_TYPE:
		/*
		 * This is a response back from a resolver.  It
		 * consists of a message chain containing:
		 *	IRE_MBLK-->LL_HDR_MBLK->pkt
		 * The IRE_MBLK is the one we allocated in ip_newroute.
		 * The LL_HDR_MBLK is the DLPI header to use to get
		 * the attached packet, and subsequent ones for the
		 * same destination, transmitted.
		 *
		 * Rearrange the message into:
		 *	IRE_MBLK-->pkt
		 * and point the ire_dlureq_mp field of the IRE
		 * to LL_HDR_MBLK. Then in ire_add_then_send
		 * the IRE will be added, and then the packet will be
		 * run back through ip_wput. This time it will make
		 * it to the wire.
		 */
		if ((mp->b_wptr - mp->b_rptr) != sizeof (ire_t))
			break;
		mp1 = mp->b_cont;
		((ire_t *)mp->b_rptr)->ire_dlureq_mp = mp1;
		mp->b_cont = mp1->b_cont;
		mp1->b_cont = NULL;
		ire_add_then_send(q, mp);
		return;
	default:
		break;
	}
	if (q->q_next) {
		putnext(q, mp);
	} else
		freemsg(mp);
}

/*
 * Process IP options in an outbound packet.  Modify the destination if there
 * is a source route option.
 * Returns non-zero if something fails in which case an ICMP error has been
 * sent and mp freed.
 */
static int
ip_wput_options(queue_t *q, mblk_t *ipsec_mp, ipha_t *ipha,
    boolean_t mctl_present)
{
	uint32_t	totallen;
	uchar_t		*opt;
	uint32_t	optval;
	uint32_t	optlen;
	ipaddr_t	dst;
	intptr_t	code = 0;
	mblk_t		*mp;
	ire_t		*ire = NULL;

	ip2dbg(("ip_wput_options\n"));
	mp = ipsec_mp;
	if (mctl_present) {
		mp = ipsec_mp->b_cont;
	}

	dst = ipha->ipha_dst;
	totallen = ipha->ipha_version_and_hdr_length -
	    (uint8_t)((IP_VERSION << 4) + IP_SIMPLE_HDR_LENGTH_IN_WORDS);
	opt = (uchar_t *)&ipha[1];
	totallen <<= 2;
	while (totallen != 0) {
		switch (optval = opt[IPOPT_POS_VAL]) {
		case IPOPT_EOL:
			return (0);
		case IPOPT_NOP:
			optlen = 1;
			break;
		default:
			optlen = opt[IPOPT_POS_LEN];
		}
		if (optlen == 0 || optlen > totallen) {
			ip1dbg(("ip_wput_options: bad option len %d, %d\n",
			    optlen, totallen));
			code = (char *)&opt[IPOPT_POS_LEN] - (char *)ipha;
			goto param_prob;
		}
		ip2dbg(("ip_wput_options: opt %d, len %d\n",
		    optval, optlen));

		switch (optval) {
			uint32_t off;
		case IPOPT_SSRR:
		case IPOPT_LSRR:
			off = opt[IPOPT_POS_OFF];
			if (off < IPOPT_MINOFF_SR) {
				ip1dbg((
				    "ip_wput_options: bad option offset %d\n",
				    off));
				code = (char *)&opt[IPOPT_POS_OFF] -
				    (char *)ipha;
				goto param_prob;
			}
			ip1dbg(("ip_wput_options: next hop 0x%x\n",
			    ntohl(dst)));
			/*
			 * For strict: verify that dst is directly
			 * reachable.
			 */
			if (optval == IPOPT_SSRR) {
				ire = ire_ftable_lookup(dst, 0, 0,
				    IRE_INTERFACE, NULL, NULL, NULL, 0,
				    MATCH_IRE_TYPE);
				if (ire == NULL) {
					ip1dbg(("ip_wput_options: SSRR not"
					    " directly reachable: 0x%x\n",
					    ntohl(dst)));
					goto bad_src_route;
				}
				ire_refrele(ire);
			}
			break;
		case IPOPT_RR:
			off = opt[IPOPT_POS_OFF];
			if (off < IPOPT_MINOFF_SR) {
				ip1dbg((
				    "ip_wput_options: bad option offset %d\n",
				    off));
				code = (char *)&opt[IPOPT_POS_OFF] -
				    (char *)ipha;
				goto param_prob;
			}
			break;
		case IPOPT_IT:
			/*
			 * Verify that length >=5 and that there is either
			 * room for another timestamp or that the overflow
			 * counter is not maxed out.
			 */
			code = (char *)&opt[IPOPT_POS_LEN] - (char *)ipha;
			if (optlen < IPOPT_MINLEN_IT) {
				goto param_prob;
			}
			switch (opt[IPOPT_POS_OV_FLG] & 0x0F) {
			case IPOPT_IT_TIME:
				off = IPOPT_IT_TIMELEN;
				break;
			case IPOPT_IT_TIME_ADDR:
			case IPOPT_IT_SPEC:
#ifdef IPOPT_IT_SPEC_BSD
			case IPOPT_IT_SPEC_BSD:
#endif
				off = IP_ADDR_LEN + IPOPT_IT_TIMELEN;
				break;
			default:
				code = (char *)&opt[IPOPT_POS_OV_FLG] -
				    (char *)ipha;
				goto param_prob;
			}
			if (opt[IPOPT_POS_OFF] - 1 + off > optlen &&
			    (opt[IPOPT_POS_OV_FLG] & 0xF0) == 0xF0) {
				/*
				 * No room and the overflow counter is 15
				 * already.
				 */
				goto param_prob;
			}
			off = opt[IPOPT_POS_OFF];
			if (off < IPOPT_MINOFF_IT) {
				code = (char *)&opt[IPOPT_POS_OFF] -
				    (char *)ipha;
				goto param_prob;
			}
			break;
		}
		totallen -= optlen;
		opt += optlen;
	}
	return (0);

param_prob:
	/*
	 * Since ip_wput() isn't close to finished, we fill
	 * in enough of the header for credible error reporting.
	 */
	if (ip_hdr_complete((ipha_t *)mp->b_rptr)) {
		/* Failed */
		freemsg(ipsec_mp);
		return (-1);
	}
	icmp_param_problem(q, ipsec_mp, (uint8_t)code);
	return (-1);

bad_src_route:
	/*
	 * Since ip_wput() isn't close to finished, we fill
	 * in enough of the header for credible error reporting.
	 */
	if (ip_hdr_complete((ipha_t *)mp->b_rptr)) {
		/* Failed */
		freemsg(ipsec_mp);
		return (-1);
	}
	icmp_unreachable(q, ipsec_mp, ICMP_SOURCE_ROUTE_FAILED);
	return (-1);
}

/*
 * Hash list insertion routine for IP client structures.
 * Used when fully connected. Insert at the front of the list so that
 * IP_UDP_MATCH will give preference to connected sockets.
 */
void
ipc_hash_insert_connected(icf_t *icf, ipc_t *ipc)
{
	ipc_t	**ipcp;
	ipc_t	*ipcnext;

	if (ipc->ipc_ptphn)
		ipc_hash_remove(ipc);
	ipcp = &icf->icf_ipc;
	mutex_enter(&icf->icf_lock);
	ipcnext = ipcp[0];
	if (ipcnext)
		ipcnext->ipc_ptphn = &ipc->ipc_hash_next;
	ipc->ipc_hash_next = ipcnext;
	ipc->ipc_ptphn = ipcp;
	ipcp[0] = ipc;
	ipc->ipc_fanout_lock = &icf->icf_lock;	/* For ipc_hash_remove */
	mutex_exit(&icf->icf_lock);
}

/*
 * Hash list insertion routine for IP client structures.
 * Used when bound to a specified interface address.
 */
void
ipc_hash_insert_bound(icf_t *icf, ipc_t *ipc)
{
	ipc_t	**ipcp;
	ipc_t	*ipcnext;

	if (ipc->ipc_ptphn)
		ipc_hash_remove(ipc);
	ipcp = &icf->icf_ipc;
	mutex_enter(&icf->icf_lock);
	/*
	 * Skip past all connected ipcs.
	 * Separate hashes for v4 and v6 imply that we can check
	 * ipc_af_isv6 once for the whole chain.
	 */
	if (ipc->ipc_af_isv6) {
		while (ipcp[0] &&
		    !IN6_IS_ADDR_UNSPECIFIED(&(ipcp[0]->ipc_v6faddr)))
			ipcp = &(ipcp[0]->ipc_hash_next);
	} else {
		while (ipcp[0] &&
		    ipcp[0]->ipc_faddr != INADDR_ANY)
			ipcp = &(ipcp[0]->ipc_hash_next);
	}
	ipcnext = ipcp[0];
	if (ipcnext)
		ipcnext->ipc_ptphn = &ipc->ipc_hash_next;
	ipc->ipc_hash_next = ipcnext;
	ipc->ipc_ptphn = ipcp;
	ipcp[0] = ipc;
	ipc->ipc_fanout_lock = &icf->icf_lock;	/* For ipc_hash_remove */
	mutex_exit(&icf->icf_lock);
}

/*
 * Hash list insertion routine for IP client structures.
 * Used when bound to INADDR_ANY to make streams bound to a specified interface
 * address take precedence.
 */
void
ipc_hash_insert_wildcard(icf_t *icf, ipc_t *ipc)
{
	ipc_t	**ipcp;
	ipc_t	*ipcnext;

	if (ipc->ipc_ptphn)
		ipc_hash_remove(ipc);
	ipcp = &icf->icf_ipc;
	mutex_enter(&icf->icf_lock);
	/* Skip to end of list */
	while (ipcp[0])
		ipcp = &(ipcp[0]->ipc_hash_next);
	ipcnext = ipcp[0];
	if (ipcnext)
		ipcnext->ipc_ptphn = &ipc->ipc_hash_next;
	ipc->ipc_hash_next = ipcnext;
	ipc->ipc_ptphn = ipcp;
	ipcp[0] = ipc;
	ipc->ipc_fanout_lock = &icf->icf_lock;	/* For ipc_hash_remove */
	mutex_exit(&icf->icf_lock);
}

/*
 * Write service routine.
 * Handles backenabling when a device as been flow controlled by
 * backenabling all non-tcp upper layers.
 * Also handles operations that ip_open/ip_close needs help with;
 * those routines are hot and certain operations require exclusive access
 * in ip. These operations are:
 *	ip_open ill initialization
 *	ip_close ill removal
 *	ip_close ipc removal in the uncommon cases:
 *		The ipc has multicast group membership (ipc_ilg_inuse)
 *		There is a pending interface op on the ipc (ipc_pending_ill)
 *		The ipc is that of the multicast routing daemon (ip_g_mrouter)
 */
void
ip_wsrv(queue_t *q)
{
	ipc_t	*ipc;
	ill_t	*ill;
	mblk_t	*mp;

	if (q->q_next) {
		ill = (ill_t *)q->q_ptr;

		/*
		 * The device flow control has opened up.
		 * Walk through all upper (ipc) streams and qenable
		 * those that have queued data.
		 * In the case of close synchronization this needs to
		 * be done to ensure that all upper layers blocked
		 * due to flow control to the closing device
		 * get unblocked.
		 */
		ip1dbg(("ip_wsrv: walking\n"));
		ipc_walk_nontcp(ipc_qenable, NULL);

		if (ill->ill_close_flags & IPCF_OPENING &&
		    !(ill->ill_close_flags & IPCF_OPEN_DONE)) {
			/* Initialize the new ILL. */
			ill->ill_error = ill_init(RD(q), ill);
			ill->ill_close_flags |= IPCF_OPEN_DONE;
			return;
		}
		if (ill->ill_close_flags & IPCF_CLOSING &&
		    !(ill->ill_close_flags & IPCF_CLOSE_DONE)) {
			if (ill->ill_pending_q != NULL) {
				ipc = (ipc_t *)(ill->ill_pending_q->q_ptr);
				ASSERT(ipc->ipc_pending_ill == ill);
				ipc->ipc_pending_ill = NULL;
				if (ill->ill_pending_mp != NULL) {
					freemsg(ill->ill_pending_mp);
					ill->ill_pending_mp = NULL;
					/*
					 * Since ill_pending_mp and
					 * ill_pending_ipif go together,
					 * clear the pending_ipif.
					 */
					ill->ill_pending_ipif = NULL;
				}
			}
			ill->ill_pending_q = NULL;
			ill_delete(ill);
			ill->ill_close_flags |= IPCF_CLOSE_DONE;
			return;
		}
		return;
	}
	ipc = (ipc_t *)q->q_ptr;
	ip1dbg(("ip_wsrv: %p %p\n", (void *)q, (void *)ipc));

	if (ipc->ipc_close_flags & IPCF_CLOSING) {
		if (ipc->ipc_pending_ill != NULL) {
			ASSERT(ipc->ipc_pending_ill->ill_pending_q == q);
			ipc->ipc_pending_ill->ill_pending_q = NULL;
			if (ipc->ipc_pending_ill->ill_pending_mp != NULL) {
				freemsg(ipc->ipc_pending_ill->ill_pending_mp);
				ipc->ipc_pending_ill->ill_pending_mp = NULL;
				/*
				 * Since ill_pending_mp and ill_pending_ipif
				 * always go together, clear the pending_ipif.
				 */
				ipc->ipc_pending_ill->ill_pending_ipif = NULL;
			}
			ipc->ipc_pending_ill = NULL;
		}
		if (ipc->ipc_rq == ip_g_mrouter || ipc->ipc_wq == ip_g_mrouter)
			(void) ip_mrouter_done();

		mutex_enter(&ipsec_loader_lock);
		while (ipsec_loader_q == ipc->ipc_wq)
			cv_wait(&ipsec_loader_cv, &ipsec_loader_lock);
		mutex_exit(&ipsec_loader_lock);

		ilg_delete_all(ipc);
		ipc->ipc_close_flags |= IPCF_CLOSE_DONE;
		return;
	}
	if (q->q_first == NULL)
		return;

	/*
	 * Set ipc_draining flag to make ip_wput pass messages through and
	 * do a putbq instead of a putq if flow controlled from the driver.
	 * noenable the queue so that a putbq from ip_wsrv does not reenable
	 * (causing an infinite loop).
	 * Note: this assumes that ip is configured such that no
	 * other thread can execute in ip_wput while ip_wsrv is running.
	 */
	ipc->ipc_draining = 1;
	noenable(q);
	while ((mp = getq(q)) != NULL) {
		ip_wput(q, mp);
		if (ipc->ipc_did_putbq) {
			/* ip_wput did a putbq */
			ipc->ipc_did_putbq = 0;
			break;
		}
	}
	enableok(q);
	ipc->ipc_draining = 0;
}

/*
 * Hash list removal routine for IP client structures.
 */
static void
ipc_hash_remove(ipc_t *ipc)
{
	ipc_t	*ipcnext;
	kmutex_t *lockp;

	/*
	 * Extract the lock pointer in case there are concurrent
	 * hash_remove's for this instance.
	 */
	lockp = ipc->ipc_fanout_lock;
	if (ipc->ipc_ptphn) {
		ASSERT(lockp != NULL);
		mutex_enter(lockp);
		if (ipc->ipc_ptphn) {
			ipcnext = ipc->ipc_hash_next;
			if (ipcnext) {
				ipcnext->ipc_ptphn = ipc->ipc_ptphn;
				ipc->ipc_hash_next = NULL;
			}
			*ipc->ipc_ptphn = ipcnext;
			ipc->ipc_ptphn = NULL;
		}
		mutex_exit(lockp);
		ipc->ipc_fanout_lock = NULL;
	}
}

/*
 * qenable the write side queue.
 * Used when flow control is opened up on the device stream.
 * Note: it is not possible to restrict the enabling to those
 * queues that have data queued since that would introduce a race
 * condition.
 * Used as an ipc_walk_nontcp function (tcp can not be subject to
 * outbound flow control in ip).
 */
/* ARGSUSED */
static void
ipc_qenable(ipc_t *ipc, void *arg)
{
	qenable(ipc->ipc_wq);
}

/*
 * Walk the list of all IPC's calling the function provided with the
 * specified argument for each.	 Note that this only walks IPC's that
 * have been bound.
 * Applies to both IPv4 and IPv6.
 */
void
ipc_walk(pfv_t func, void *arg)
{
	int	i;
	icf_t	*icf;
	ipc_t	*ipc;
	ipc_t	*ipc1;

	for (i = 0; i < A_CNT(ipc_udp_fanout); i++) {
		icf = &ipc_udp_fanout[i];
		mutex_enter(&icf->icf_lock);
		for (ipc = icf->icf_ipc; ipc; ipc = ipc1) {
			IPC_REFHOLD(ipc);
			mutex_exit(&icf->icf_lock);
			(*func)(ipc, arg);
			mutex_enter(&icf->icf_lock);
			ipc1 = ipc->ipc_hash_next;
			IPC_REFRELE(ipc);
		}
		mutex_exit(&icf->icf_lock);
	}
	for (i = 0; i < ipc_tcp_conn_hash_size; i++) {
		icf = &ipc_tcp_conn_fanout[i];
		mutex_enter(&icf->icf_lock);
		for (ipc = icf->icf_ipc; ipc; ipc = ipc1) {
			IPC_REFHOLD(ipc);
			mutex_exit(&icf->icf_lock);
			(*func)(ipc, arg);
			mutex_enter(&icf->icf_lock);
			ipc1 = ipc->ipc_hash_next;
			IPC_REFRELE(ipc);
		}
		mutex_exit(&icf->icf_lock);
	}
	for (i = 0; i < A_CNT(ipc_tcp_listen_fanout); i++) {
		icf = &ipc_tcp_listen_fanout[i];
		mutex_enter(&icf->icf_lock);
		for (ipc = icf->icf_ipc; ipc; ipc = ipc1) {
			IPC_REFHOLD(ipc);
			mutex_exit(&icf->icf_lock);
			(*func)(ipc, arg);
			mutex_enter(&icf->icf_lock);
			ipc1 = ipc->ipc_hash_next;
			IPC_REFRELE(ipc);
		}
		mutex_exit(&icf->icf_lock);
	}
	for (i = 0; i < A_CNT(ipc_proto_fanout); i++) {
		icf = &ipc_proto_fanout[i];
		mutex_enter(&icf->icf_lock);
		for (ipc = icf->icf_ipc; ipc; ipc = ipc1) {
			IPC_REFHOLD(ipc);
			mutex_exit(&icf->icf_lock);
			(*func)(ipc, arg);
			mutex_enter(&icf->icf_lock);
			ipc1 = ipc->ipc_hash_next;
			IPC_REFRELE(ipc);
		}
		mutex_exit(&icf->icf_lock);
	}

	/* IPv6 */
	for (i = 0; i < A_CNT(ipc_udp_fanout_v6); i++) {
		icf = &ipc_udp_fanout_v6[i];
		mutex_enter(&icf->icf_lock);
		for (ipc = icf->icf_ipc; ipc; ipc = ipc1) {
			IPC_REFHOLD(ipc);
			mutex_exit(&icf->icf_lock);
			(*func)(ipc, arg);
			mutex_enter(&icf->icf_lock);
			ipc1 = ipc->ipc_hash_next;
			IPC_REFRELE(ipc);
		}
		mutex_exit(&icf->icf_lock);
	}
	for (i = 0; i < ipc_tcp_conn_hash_size; i++) {
		icf = &ipc_tcp_conn_fanout_v6[i];
		mutex_enter(&icf->icf_lock);
		for (ipc = icf->icf_ipc; ipc; ipc = ipc1) {
			IPC_REFHOLD(ipc);
			mutex_exit(&icf->icf_lock);
			(*func)(ipc, arg);
			mutex_enter(&icf->icf_lock);
			ipc1 = ipc->ipc_hash_next;
			IPC_REFRELE(ipc);
		}
		mutex_exit(&icf->icf_lock);
	}
	for (i = 0; i < A_CNT(ipc_tcp_listen_fanout_v6); i++) {
		icf = &ipc_tcp_listen_fanout_v6[i];
		mutex_enter(&icf->icf_lock);
		for (ipc = icf->icf_ipc; ipc; ipc = ipc1) {
			IPC_REFHOLD(ipc);
			mutex_exit(&icf->icf_lock);
			(*func)(ipc, arg);
			mutex_enter(&icf->icf_lock);
			ipc1 = ipc->ipc_hash_next;
			IPC_REFRELE(ipc);
		}
		mutex_exit(&icf->icf_lock);
	}
	for (i = 0; i < A_CNT(ipc_proto_fanout_v6); i++) {
		icf = &ipc_proto_fanout_v6[i];
		mutex_enter(&icf->icf_lock);
		for (ipc = icf->icf_ipc; ipc; ipc = ipc1) {
			IPC_REFHOLD(ipc);
			mutex_exit(&icf->icf_lock);
			(*func)(ipc, arg);
			mutex_enter(&icf->icf_lock);
			ipc1 = ipc->ipc_hash_next;
			IPC_REFRELE(ipc);
		}
		mutex_exit(&icf->icf_lock);
	}
}

/*
 * Walk the list of all IPC's except TCP IPC's calling the function
 * provided with the specified argument for each.
 * Note that this only walks IPC's that have been bound.
 * Applies to both IPv4 and IPv6.
 */
void
ipc_walk_nontcp(pfv_t func, void *arg)
{
	int	i;
	icf_t	*icf;
	ipc_t	*ipc;
	ipc_t	*ipc1;

	for (i = 0; i < A_CNT(ipc_udp_fanout); i++) {
		icf = &ipc_udp_fanout[i];
		mutex_enter(&icf->icf_lock);
		for (ipc = icf->icf_ipc; ipc; ipc = ipc1) {
			IPC_REFHOLD(ipc);
			mutex_exit(&icf->icf_lock);
			(*func)(ipc, arg);
			mutex_enter(&icf->icf_lock);
			ipc1 = ipc->ipc_hash_next;
			IPC_REFRELE(ipc);
		}
		mutex_exit(&icf->icf_lock);
	}
	for (i = 0; i < A_CNT(ipc_proto_fanout); i++) {
		icf = &ipc_proto_fanout[i];
		mutex_enter(&icf->icf_lock);
		for (ipc = icf->icf_ipc; ipc; ipc = ipc1) {
			IPC_REFHOLD(ipc);
			mutex_exit(&icf->icf_lock);
			(*func)(ipc, arg);
			mutex_enter(&icf->icf_lock);
			ipc1 = ipc->ipc_hash_next;
			IPC_REFRELE(ipc);
		}
		mutex_exit(&icf->icf_lock);
	}
	/* IPv6 */
	for (i = 0; i < A_CNT(ipc_udp_fanout_v6); i++) {
		icf = &ipc_udp_fanout_v6[i];
		mutex_enter(&icf->icf_lock);
		for (ipc = icf->icf_ipc; ipc; ipc = ipc1) {
			IPC_REFHOLD(ipc);
			mutex_exit(&icf->icf_lock);
			(*func)(ipc, arg);
			mutex_enter(&icf->icf_lock);
			ipc1 = ipc->ipc_hash_next;
			IPC_REFRELE(ipc);
		}
		mutex_exit(&icf->icf_lock);
	}
	for (i = 0; i < A_CNT(ipc_proto_fanout_v6); i++) {
		icf = &ipc_proto_fanout_v6[i];
		mutex_enter(&icf->icf_lock);
		for (ipc = icf->icf_ipc; ipc; ipc = ipc1) {
			IPC_REFHOLD(ipc);
			mutex_exit(&icf->icf_lock);
			(*func)(ipc, arg);
			mutex_enter(&icf->icf_lock);
			ipc1 = ipc->ipc_hash_next;
			IPC_REFRELE(ipc);
		}
		mutex_exit(&icf->icf_lock);
	}
}

/* ipc_walk routine invoked for ip_ipc_report for each ipc. */
static void
ipc_report1(ipc_t *ipc, void *mp)
{
	char	buf1[INET6_ADDRSTRLEN];
	char	buf2[INET6_ADDRSTRLEN];

	(void) inet_ntop(AF_INET6, &ipc->ipc_v6laddr, buf1, sizeof (buf1)),
	(void) inet_ntop(AF_INET6, &ipc->ipc_v6faddr, buf2, sizeof (buf2)),
	(void) mi_mpprintf((mblk_t *)mp,
	    MI_COL_PTRFMT_STR MI_COL_PTRFMT_STR MI_COL_PTRFMT_STR
	    "%s/%05d %s/%05d",
	    (void *)ipc, (void *)ipc->ipc_rq, (void *)ipc->ipc_wq,
	    buf1, ipc->ipc_lport, buf2, ipc->ipc_fport);
}

/*
 * Named Dispatch routine to produce a formatted report on all IPCs
 * that are listed in one of the fanout tables.
 * This report is accessed by using the ndd utility to "get" ND variable
 * "ip_ipc_status".
 */
/* ARGSUSED */
int
ip_ipc_report(queue_t *q, mblk_t *mp, void *arg)
{
	(void) mi_mpprintf(mp,
	    "IPC      " MI_COL_HDRPAD_STR
	    "rfq      " MI_COL_HDRPAD_STR
	    "stq      " MI_COL_HDRPAD_STR
	    "local                 remote");
	ipc_walk(ipc_report1, mp);
	return (0);
}

/*
 * Report all the proxy listeners, their port number/proxy addresses,
 * and total inbound packets/fragments intercepted.
 */
/* ARGSUSED */
static int
ip_proxy_addr_report(queue_t *q, mblk_t *mp, void *arg)
{
	ipc_t	*ipc;
	char	buf[16];
	proxy_addr_t *pa;

	(void) mi_mpprintf(mp,
	    "IPC      " MI_COL_HDRPAD_STR
	    "rq       " MI_COL_HDRPAD_STR
	    "lport in_pkts       "
	    "proxy_addr/mask      total in_pkts/frags= %d/%d",
	    ip_proxy_ib_pkt_count, ip_proxy_ib_frag_count);
	rw_enter(&ip_palist_lock, RW_READER);
	ipc = ip_proxy_listeners;
	if (ipc == NULL)
		goto out;
	do {
		(void) mi_mpprintf(mp,
		    MI_COL_PTRFMT_STR MI_COL_PTRFMT_STR
		    "%05d %d      ",
		    (void *)ipc,
		    (void *)ipc->ipc_rq, ntohs(ipc->ipc_lport),
		    ipc->ipc_proxy_ib_pkt_count);
		pa = ipc->ipc_palist;
		do {
			(void) mi_mpprintf_nr(mp, "%s/%x ",
			    ip_dot_addr(pa->pa_addr, buf), ntohl(pa->pa_mask));
			pa = pa->pa_next;
		} while (pa != NULL);
		ipc = ipc->ipc_hash_next;
	} while (ipc != ip_proxy_listeners);
out:
	rw_exit(&ip_palist_lock);
	return (0);
}

/*
 * Determine if the ill and multicast aspects of that packets
 * "matches" the ipc.
 */
boolean_t
ipc_wantpacket(ipc_t *ipc, ill_t *ill, ipaddr_t dst)
{
	if (ipc->ipc_incoming_ill != NULL &&
	    ipc->ipc_incoming_ill != ill)
		return (B_FALSE);

	if (!CLASSD(dst) || ipc->ipc_multi_router)
		return (B_TRUE);
	return (ilg_lookup_ill(ipc, dst, ill) != NULL);
}

/*
 * Finish processing of "arp_up" when AR_DLPIOP_DONE is received from arp.
 */
static void
ip_arp_done(queue_t *q, mblk_t *mp)
{
	ill_t *ill = (ill_t *)q->q_ptr;
	mblk_t	*mp1;
	ipif_t  *ipif;
	int err = 0;

	ip1dbg(("ip_arp_done(%s)\n", ill->ill_name));

	ASSERT((mp->b_wptr - mp->b_rptr) >= sizeof (arc_t));
	ASSERT(((arc_t *)mp->b_rptr)->arc_cmd == AR_DLPIOP_DONE);

	/* We should have an IOCTL waiting on this. */
	mp1 = ill->ill_pending_mp;
	if (mp1) {
		mblk_t	*mp2;

		ASSERT(ill->ill_pending_ipif->ipif_ill == ill);
		ipif = ill->ill_pending_ipif;
		ill->ill_pending_ipif = NULL;
		ill->ill_pending_mp = NULL;
		q = ill->ill_pending_q;
		/*
		 * If the client stream is no longer around,
		 * then do not try and send a reply.
		 */
		if (q == NULL) {
			mblk_t	*bp = mp1;

			for (; bp; bp = bp->b_cont) {
				bp->b_prev = NULL;
				bp->b_next = NULL;
			}
			freemsg(mp1);
			freemsg(mp);
			return;
		}
		/*
		 * Clear fields here. Since we are holding
		 * the writers lock we can do it here even
		 * though we do the qreply below.
		 */
		ASSERT(((ipc_t *)q->q_ptr)->ipc_pending_ill == ill);
		ill->ill_pending_q = NULL;
		((ipc_t *)q->q_ptr)->ipc_pending_ill = NULL;

		/*
		 * If the DL_BIND_REQ fails, it is noted
		 * in arc_name_offset.
		 */
		mp2 = mp->b_cont;
		err = *((int *)mp2->b_rptr);
		if (err == 0) {
			if ((err = ipif_up_done(ipif)) != 0)
				ip0dbg(("ip_arp_done: init failed\n"));
		} else {
			ip0dbg(("ip_arp_done: DL_BIND_REQ failed\n"));
		}
		mi_copy_done(q, mp1, err);
		freemsg(mp);
	}
}

/*
 * See if a given module appears next. Used to check if the resolver
 * is named "arp" or something else (like an atmarp resolver).
 */
boolean_t
ismod_next(queue_t *q, char *name)
{
	struct qinit *qi = q->q_next->q_qinfo;
	struct module_info *mi;

	if (!qi)
		return (B_FALSE);

	mi = qi->qi_minfo;
	if (mi && mi->mi_idname &&
		(strcmp(mi->mi_idname, name) == 0)) {
		return (B_TRUE);
	}
	return (B_FALSE);
}

/*
 * Return true if the dst IP addr in the IP header is not the final one.
 * This can happen only if source route option is present and we haven't
 * exhausted the list.
 */
static boolean_t
ip_source_route_more_hops(ipha_t *ipha, uint32_t ipoptlen)
{
	uchar_t		*opt;
	uint32_t	optval;
	uint32_t	optlen;

	opt = (uchar_t *)&ipha[1];
	ipoptlen <<= 2;
	while (ipoptlen != 0) {
		optval = opt[IPOPT_POS_VAL];
		if (optval == IPOPT_EOL)
			return (B_FALSE);
		if (optval == IPOPT_NOP)
			optlen = 1;
		else
			optlen = opt[IPOPT_POS_LEN];
		if (optlen == 0 || optlen > ipoptlen)
			return (B_FALSE);
		if ((optval == IPOPT_SSRR) ||
		    (optval == IPOPT_LSRR)) {
			uint32_t off;

			off = opt[IPOPT_POS_OFF];
			off--;
			/*
			 * Return false if we're at the end of the list.
			 * That means no more hops and the dst in the IP
			 * header is the addr of the final node.
			 */
			if (optlen < IP_ADDR_LEN ||
			    off > optlen - IP_ADDR_LEN)
				return (B_FALSE);
			else
				return (B_TRUE);
		}
		ipoptlen -= optlen;
		opt += optlen;
	}
	return (B_FALSE);
}

/*
 * Compare two sets of addr/mask.
 * Note that this routine doesn't assume mask bits are contiguous and thus
 * is more general.
 *
 * 0 - no overlap
 * 1 - pa1 is subset of pa2
 * 2 - equal or partial overlap
 * 3 - pa1 is superset of pa2
 */
static int
ip_proxy_addr_compare(proxy_addr_t *pa1, proxy_addr_t *pa2)
{
	ipaddr_t	common = pa1->pa_mask & pa2->pa_mask;
	if ((pa1->pa_addr & common) != (pa2->pa_addr & common))
		return (0);
	if (pa1->pa_mask == pa2->pa_mask) /* the two are identical */
		return (2);
	if (common == pa2->pa_mask)
		return (1);
	if (common == pa1->pa_mask)
		return (3);
	else
		return (2);
}

/*
 * For !sameport, return 0 if no overlap, 1 if any overlap.
 *
 * Otherwise:
 * 0 - no overlap
 * 1 - list1 is a subset of list2. To satisfy this condition, each pa in list1
 *     has to be fully contained (but not identical) by some pa from list2.
 * 2 - partial overlap (none of the rest)
 * 3 - is superset. This condition is determined by recursively calling
 *     ip_proxy_check_palist() with arguments palist1 and palist2 reversed.
 */
static int
ip_proxy_check_palist(proxy_addr_t *palist1, proxy_addr_t *palist2,
    boolean_t sameport, boolean_t first_call)
{
	proxy_addr_t *pa1, *pa2;
	boolean_t subset, overlap, is_subset, first;
	int	result;

	overlap = is_subset = B_FALSE;
	first = B_TRUE;
	for (pa1 = palist1; pa1 != NULL; pa1 = pa1->pa_next) {
		subset = B_FALSE;
		for (pa2 = palist2; pa2 != NULL; pa2 = pa2->pa_next) {
			result = ip_proxy_addr_compare(pa1, pa2);
			if (result > 0) { /* Some form of overlap exists. */
				if (!sameport)
					return (1);
				if (result == 1) {
					subset = B_TRUE;
					break;
				}
				/* equal or partial overlap */
				if (result == 2)
					return (2);
				overlap = B_TRUE;
				break;
			}
			/* otherwise no overlap */
		}
		if (first) {
			is_subset = subset;
			first = B_FALSE;
		} else if (is_subset != subset) {
			/*
			 * Some pa are subset while others aren't. This is
			 * considered "partial overlap" and is not allowed.
			 */
			return (2);
		}
	}
	if (is_subset)
		return (1);
	if (!overlap)
		return (0);
	/*
	 * This is a recursive call and the two are not in a subset/superset
	 * relation. Since we know there has been overlap, it must be
	 * a partial overlap case so return 2 now.
	 */
	if (!first_call)
		return (2);
	/*
	 * We know now that there is some overlap between the two, but
	 * palist1 is not a subset of palist2. Check for whether palist2
	 * is a subset of palist1 by calling ip_proxy_check_palist() again
	 * with two args reversed.
	 */
	if (ip_proxy_check_palist(palist2, palist1, sameport, B_FALSE) == 1)
		return (3);
	else
		return (2); /* Neither subset nor superset */
}

/*
 * The rules governing proxy listeners are:
 *
 * 1. Any new proxy listener must have SO_REUSEADDR set if it overlaps
 * (i.e., there exist an addr/port pair that match both listeners) with any
 * existing proxy listener.
 *
 * 2. Partially overlapped proxy address sets are forbidden and EADDRINUSE
 * will be returned if the port numbers are the same (or both 0). In other
 * words, listeners to the same port (inlcuding 0) must either have disjoint
 * proxy address sets, or one being a proper subset of the other.
 *
 * 3. Specific-port listeners take higher precedence over any all-port
 * listener.
 *
 * The proxy listener list is a doubly-linked list with ip_proxy_listeners
 * points to the head of the list (or null if the list is empty). It is
 * sorted by the matching precedence, with all-port listeners behind all
 * the listeners to specific ports.
 *
 * When inserting a new proxy listener to the list, one not only needs to
 * check addr/port overlapping, but also finds the right place to insert.
 * I.e. a proxy listener must precede any listener to the same port whose
 * proxy address set is a proper superset.
 *
 * If no overlap is found, always insert a specific-port listener to the
 * front, and an all-port listener from the tail
 * (ip_proxy_listeners->ipc_ptphn).
 */
static int
ip_proxy_add_listener(ipc_t *ipc)
{
	ipc_t	*proxy, *insert_before_here = NULL;
	int	result, error = 0;
	boolean_t	move_head = B_FALSE;
	uint16_t	port;

	ASSERT(ipc->ipc_hash_next == NULL);
	ASSERT(ipc->ipc_ptphn == NULL);
	rw_enter(&ip_palist_lock, RW_WRITER);
	if (ip_proxy_listeners == NULL) {
		ip_proxy_listeners = ipc;
		ipc->ipc_hash_next = ipc;
		ipc->ipc_ptphn = &ipc->ipc_hash_next;
		ipc->ipc_proxy_listen = B_TRUE;
		goto out;
	}
	port = ipc->ipc_lport;
	proxy = ip_proxy_listeners;
	/*
	 * Now we go down the list, checking for illegal overlap, ipc_reuseaddr
	 * flag, remembering the right location for insertion...
	 */
	do {
		/*
		 * For an overlap to be possible, ports have to either be the
		 * same or one is wild-card.
		 */
		if ((port != proxy->ipc_lport) && (port != 0) &&
		    (proxy->ipc_lport != 0))
			continue;
		result = ip_proxy_check_palist(ipc->ipc_palist,
		    proxy->ipc_palist,
		    port == proxy->ipc_lport ? B_TRUE : B_FALSE, B_TRUE);
		if (result == 0) continue; /* addrs don't overlap */

		/* Addrs partially overlap, or SO_REUSEADDR not on */
		if ((result == 2) || (!ipc->ipc_reuseaddr)) {
			error = EADDRINUSE;
			goto out;
		}
		if (port == proxy->ipc_lport) {
			if (result == 1) {
				/* Superset found. Must insert before it. */
				insert_before_here = proxy;
				/* move the head if inserting to the front */
				if (insert_before_here == ip_proxy_listeners)
					move_head = B_TRUE;
				/* we can break now. */
				break;
			} else {
				/* Subset found. Must insert after it. */
				insert_before_here = proxy->ipc_hash_next;
			}
		}
	} while ((proxy = proxy->ipc_hash_next) != ip_proxy_listeners);
	if (insert_before_here != NULL) {
		ipc->ipc_hash_next = insert_before_here;
		ipc->ipc_ptphn = insert_before_here->ipc_ptphn;
		insert_before_here->ipc_ptphn[0] = ipc;
		insert_before_here->ipc_ptphn = &ipc->ipc_hash_next;
		if (move_head)
			ip_proxy_listeners = ipc;
	} else {
		/*
		 * Otherwise we insert to the front and only move the head to
		 * point to the new proxy listener if it's a specific port
		 * listener. This effectively makes all-port listeners go to
		 * tail and specific port listeners go to front.
		 */
		ipc->ipc_hash_next = ip_proxy_listeners;
		ipc->ipc_ptphn = ip_proxy_listeners->ipc_ptphn;
		ip_proxy_listeners->ipc_ptphn[0] = ipc;
		ip_proxy_listeners->ipc_ptphn = &ipc->ipc_hash_next;
		if (port != 0)
			ip_proxy_listeners = ipc;
	}
	ipc->ipc_proxy_listen = B_TRUE;
	ipc->ipc_proxy_ib_pkt_count = 0;
out:
	rw_exit(&ip_palist_lock);
	return (error);
}

/*
 * If the proxy user is a listener, remove it from the proxy listeners' list.
 * Otherwise, remove it from the regular conn fanout table.
 * In both cases, deallocate the proxy addrs structures.
 */
static void
ip_proxy_remove_listener(ipc_t *ipc)
{
	proxy_addr_t *pa;

	if (ipc->ipc_proxy_listen) {
		rw_enter(&ip_palist_lock, RW_WRITER);
		ipc->ipc_hash_next->ipc_ptphn = ipc->ipc_ptphn;
		ipc->ipc_ptphn[0] = ipc->ipc_hash_next;
		/*
		 * We have to update the head if the listener it points to
		 * is what we're removing.
		 */
		if (ip_proxy_listeners == ipc) {
			ip_proxy_listeners = ipc->ipc_hash_next;
			/*
			 * If this is the last proxy listener, set
			 * ip_proxy_listeners to null
			 */
			if (ip_proxy_listeners == ipc)
				ip_proxy_listeners = NULL;
		}
		rw_exit(&ip_palist_lock);
		ipc->ipc_hash_next = NULL;
		ipc->ipc_ptphn = NULL;
		ipc->ipc_proxy_listen = B_FALSE;
	} else if (ipc->ipc_laddr != INADDR_ANY) {
		ipc_hash_remove(ipc);
	}
	pa = ipc->ipc_palist;
	do {
		ipc->ipc_palist = pa->pa_next;
		kmem_free((void *)pa, sizeof (proxy_addr_t));
	} while ((pa = ipc->ipc_palist) != NULL);
}

/* Set the following to non-zero to skip the dstport match for fragments */
static int ip_proxy_take_all_frags = 0;

/*
 * Find a match in the proxy listener's list against both IP address and port
 * number. Since the list has been sorted according to the matching priority,
 * One can stop at the first match.
 *
 * Fragments pose a problem for our matching logic since we need the ULP
 * header to match against proxy listeners but fragments other than the
 * first one don't contain an ULP header.
 *
 * A complete solution requires reassembling all the fragments right away,
 * which is probably an overkill. For now we'll correctly handle fragments
 * if there is an all-port listener to pick up all the frags with matching
 * IP addresses. Otherwise, we'll let the frags (except the first one) go.
 * Alternatively, one can set "ip_proxy_take_all_frags" to non-zero, which
 * will direct us to pick up frags based on dst IP address only, skipping
 * the dstport match.
 */
static ipc_t *
ip_proxy_match_listener(ipaddr_t dst, uint16_t dstport, boolean_t is_frag)
{
	ipc_t	*ipc;
	proxy_addr_t *pa;

	rw_enter(&ip_palist_lock, RW_READER);
	ipc = ip_proxy_listeners;
	if (ipc == NULL)
		goto out;
	do {
		if ((ipc->ipc_lport != dstport) &&
		    (ipc->ipc_lport != 0) &&
		    (!is_frag || ip_proxy_take_all_frags == 0))
			continue;
		pa = ipc->ipc_palist;
		do {
			if ((dst & pa->pa_mask) == pa->pa_addr)
				goto out;
			pa = pa->pa_next;
		} while (pa != NULL);
	} while ((ipc = ipc->ipc_hash_next) != ip_proxy_listeners);
	ipc = NULL;
out:
	/*
	 * We can't use IPC_REFHOLD macro because it contains assertions that
	 * don't apply to us.
	 */
	if (ipc) {
		mutex_enter(&(ipc)->ipc_reflock);
		(ipc)->ipc_refcnt++;
		mutex_exit(&(ipc)->ipc_reflock);
	}
	rw_exit(&ip_palist_lock);
	return (ipc);
}
